[{"content":"Convolutional networks (LeCun, 1989), also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data that has a known, grid-like topology. Examples include time-series data, which can be thought of as a 1D grid taking samples at regular time intervals, and image data, which can be thought of as a 2D grid of pixels. Convolutional networks have been tremendously successful in practical applications. The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution. Convolution is a specialized kind of linear operation. Convolutional networks are simply neural networks that use convolution in place of general matrix multiplication in at least one of their layers.\n 卷积神经网络（Convolutional Neural Network, CNN 是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。它受到人类视觉神经系统的启发。卷积神经网络能够有效的将大数据量的图片降维成小数据量，同时能够有效的保留图片特征，符合图片处理的原则。\n A typical layer of a convolutional network consists of three stages. In the first stage, the layer performs several convolutions in parallel to produce a set of linear activations.\nIn the second stage, each linear activation is run through a nonlinear activation function, such as the rectified linear activation function. This stage is sometimes called the detector stage.\nIn the third stage, we use a pooling function to modify the output of the layer further.\n The Convolution Operation In its most general form, convolution is an operation on two functions of a real-valued argument.\n$$ s(t) = \\int x(a)w(t-a)da \\\\ $$\nThis operation is called convolution. The convolution operation is typically denoted with an asterisk:\n$$ s(t) = (x \\ast w)(t) \\\\ $$\n 对卷积这个名词的理解：所谓两个函数的卷积，本质上就是先将一个函数翻转，然后进行滑动叠加。先对$w$函数进行翻转，相当于在数轴上把$w$函数从右边褶到左边去，也就是卷积的 “卷” 的由来。然后再把$w$函数平移到 $t$，在这个位置对两个函数的对应点相乘，然后相加，这个过程是卷积的 “积” 的过程。\n In convolutional network terminology, the first argument (in this example, the function $x$) to the convolution is often referred to as the input and the second argument (in this example, the function $w$) as the kernel. The output is sometimes referred to as the feature map.\nIf we now assume that $x$ and $w$ are defined only on integer $t$, we can define the discrete convolution:\n$$ s[t] = (x \\ast w)(t) = \\sum_{a=-\\infty}^{\\infty} x[a]w[t-a] \\\\ $$\n Example: 信号分析\n输入信号是 $f(t)$ ，是随时间变化的。系统响应函数是 $g(t)$ ，图中的响应函数是随时间指数下降的，它的物理意义是说：如果在 $t=0$ 的时刻有一个输入，那么随着时间的流逝，这个输入将不断衰减。换言之，到了 $t=T$ 时刻，原来在 $t=0$ 时刻的输入$f(0)$的值将衰减为$f(0)g(T)$。\n 考虑到信号是连续输入的，也就是说，每个时刻都有新的信号进来，所以，最终输出的是所有之前输入信号的累积效果。如下图所示，在$T=10$时刻，输出结果跟图中带标记的区域整体有关。其中，$f(10)$因为是刚输入的，所以其输出结果应该是$f(10)g(0)$，而时刻$t=9$的输入$f(9)$，只经过了1个时间单位的衰减，所以产生的输出应该是 $f(9)g(1)$，如此类推，即图中虚线所描述的关系。这些对应点相乘然后累加，就是$T=10$时刻的输出信号值，这个结果也是$f$和$g$两个函数在$T=10$时刻的卷积值。\n ​​显然，上面的对应关系看上去比较难看，是拧着的，所以，我们把$g$函数对折一下，变成了$g(-t)$，这样就好看一些了。看到了吗？这就是为什么卷积要“卷”，要翻转的原因，这是从它的物理意义中给出的。\n ​上图虽然没有拧着，已经顺过来了，但看上去还有点错位，所以再进一步平移$T$个单位，就是下图。它就是本文开始给出的卷积定义的一种图形的表述：\n ​​所以，在以上计算$T$时刻的卷积时，要维持的约束就是： $t+(T-t)=T$ 。\n We often use convolutions over more than one axis at a time. For example, if we use a two-dimensional image $I$ as our input, we probably also want to use a two-dimensional kernel $K$:\n$$ s[i,j] = (I * K)[i,j] = \\sum_{m}\\sum_{n}I[m,n]K[i-m,j-n] \\\\ $$\nConvolution is commutative, meaning we can equivalently write:\n$$ s[i,j] = (K * I)[i,j] = \\sum_{m}\\sum_{n}I[i-m,j-n]K[m,n] \\\\ $$\nThe commutative property of convolution arises because we have flipped the kernel relative to the input, in the sense that as m increases, the index into the input increases, but the index into the kernel decreases. The only reason to flip the kernel is to obtain the commutative property. While the commutative property is useful for writing proofs, it is not usually an important property of a neural network implementation. Instead, many neural network libraries implement a related function called the cross-correlation, which is the same as convolution but without flipping the kernel:\n$$ s[i,j] = (I * K)[i,j] = \\sum_{m}\\sum_{n}I[i-m,j-n]K[m,n] \\\\ $$\nDiscrete convolution can be viewed as multiplication by a matrix. However, the matrix has several entries constrained to be equal to other entries. For example, for univariate discrete convolution, each row of the matrix is constrained to be equal to the row above shifted by one element. This is known as a Toeplitz matrix.\n  这个过程我们可以理解为我们使用一个 过滤器（卷积核) 来过滤图像的各个小区域，从而得到这些小区域的特征值。\n   在具体应用中，往往有多个卷积核，可以认为，每个卷积核代表了一种 图像模式 ，如果某个图像块与此卷积核卷积出的值大，则认为此图像块十分接近于此卷积核。如果我们设计了6个卷积核，可以理解：我们认为这个图像上有6种底层纹理模式，也就是我们用6中基础模式就能描绘出一副图像。以下就是25种不同的卷积核的示例：\n  Motivation Convolution leverages three important ideas that can help improve a machine learning system: sparse interactions, parameter sharing and equivariant representations. Moreover, convolution provides a means for working with inputs of variable size.\nTraditional neural network layers use matrix multiplication by a matrix of parameters with a separate parameter describing the interaction between each input unit and each output unit. This means every output unit interacts with every input unit. Convolutional networks, however, typically have sparse interactions (also referred to as sparse connectivity or sparse weights). This is accomplished by making the kernel smaller than the input.\n 卷积网络具有 稀疏交互(sparse interactions) (也叫做 稀疏连接(sparse connectivity) 或者 稀疏权重(sparse weights)) 的特征。这是使核的大小远小于输入的大小来达到的。当处理一张图像时，输入的图像可能包含成千上万个像素点，但是我们可以通过只 占用几十到上百个像素点的核来检测一些小的有意义的特征，例如图像的边缘。这意味着我们需要存储的参数更少，不仅减少了模型的存储需求，而且提高了它的统计效率。这也意味着为了得到输出我们只需要更少的计算量。这些效率上的提高往往是很显著的。\n Parameter sharing refers to using the same parameter for more than one function in a model. In a traditional neural net, each element of the weight matrix is used exactly once when computing the output of a layer. It is multiplied by one element of the input and then never revisited. In a convolutional neural net, each member of the kernel is used at every position of the input (except perhaps some of the boundary pixels, depending on the design decisions regarding the boundary). The parameter sharing used by the convolution operation means that rather than learning a separate set of parameters for every location, we learn only one set. This does not affect the runtime of forward propagation, but it does further reduce the storage requirements of the model to $k$ parameters.\n 参数共享(parameter sharing) 是指在一个模型的多个函数中使用相同的参数。在卷积神经网络中，核的每一个元素都作用在输入的每一位置上。卷积运算中的参数共享保证了我们只需要学习一个参数集合，而不是对于每一位置都需要学习一个单独的参数集合。\n In the case of convolution, the particular form of parameter sharing causes the layer to have a property called equivariance to translation. To say a function is equivariant means that if the input changes, the output changes in the same way. Specifically, a function $f(x)$ is equivariant to a function $g$ if\n$$ f(g(x)) = g(f(x)) \\\\ $$\n 对于卷积，参数共享的特殊形式使得神经网络层具有对平移 等变(equivariance) 的性质。如果一个函数满足输入改变，输出也以同样的方式改变这一性质，我们就说它是等变 (equivariant) 的。\n A convolutional layer have equivariance to translation. For example\n$$ g(x)[i] = x[i-1] \\\\ $$\nIf we apply this transformation to $x$, then apply convolution, the result will be the same as if we applied convolution to $x$, then applied the transformation to the output.\nFor images, convolution creates a 2-D map of where certain features appear in the input. Note that convolution is not equivariant to some other transformations, such as changes in the scale or rotation of an image.\nPooling A pooling function replaces the output of the net at a certain location with a summary statistic of the nearby outputs. For example, the max pooling (Zhou and Chellappa, 1988) operation reports the maximum output within a rectangular neighborhood. Other popular pooling functions include the average of a rectangular neighborhood, the $L^{2}$ norm of a rectangular neighborhood, or a weighted average based on the distance from the central pixel.\n In all cases, pooling helps to make the representation become approximately invariant to small translations of the input. Invariance to translation means that if we translate the input by a small amount, the values of most of the pooled outputs do not change. Invariance to local translation can be a very useful property if we care more about whether some feature is present than exactly where it is.\n 池化层 是一个利用 池化函数 (pooling function) 对网络输出进行进一步调整的网络层。池化函数使用某一位置的相邻输出的总体统计特征来代替网络在该位置的输出。常用的池化函数包括最大池化 (max pooling) 函数 (即给出邻域内的最大值) 和平均池化 (average pooling) 函数 (即给出邻域内的平均值) 等。但无论选择何种池化函数，当对输入做出少量平移时，池化对输入的表示都近似 不变 (invariant)。局部平移不变性 是一个很重要的性质，尤其是当我们关心某个特征是否出现而不关心它出现的位置时。\n The use of pooling can be viewed as adding an infinitely strong prior that the function the layer learns must be invariant to small translations. When this assumption is correct, it can greatly improve the statistical efficiency of the network.\nPooling over spatial regions produces invariance to translation, but if we pool over the outputs of separately parametrized convolutions, the features can learn which transformations to become invariant to.\nIt is also possible to dynamically pool features together, for example, by running a clustering algorithm on the locations of interesting features (Boureau et al., 2011). This approach yields a different set of pooling regions for each image. Another approach is to learn a single pooling structure that is then applied to all images (Jia et al., 2012).\nReference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/convnets.html.\n","permalink":"https://followb1ind1y.github.io/posts/deep_learning/05_convolutional_neural_network/","summary":"Convolutional networks (LeCun, 1989), also known as convolutional neural networks or CNNs, are a specialized kind of neural network for processing data that has a known, grid-like topology. Examples include time-series data, which can be thought of as a 1D grid taking samples at regular time intervals, and image data, which can be thought of as a 2D grid of pixels. Convolutional networks have been tremendously successful in practical applications. The name “convolutional neural network” indicates that the network employs a mathematical operation called convolution.","title":"Convolutional Neural Network"},{"content":"Gradient Descent Optimization Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function $f(x)$ by altering $x$. We usually phrase most optimization problems in terms of minimizing $f(x)$. Maximization may be accomplished via a minimization algorithm by minimizing $-f(x)$.\nThe function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n 大多数深度学习算法都涉及某种形式的优化. 优化指的是改变 $x$ 以最小化或最大化某个函数 $f(x)$ 的任务. 我们通常以最小化 $f(x)$ 指代大多数最优化问题. 我们把要最小化或最大化的函数称为 目标函数(objective function) 或准则 (criterion).当我们对其进行最小化时,我们也把它称为 代价函数(cost function)、损失函数(loss function) 或 误差函数(error function).\n We often denote the value that minimizes or maximizes a function with a superscript $\\ast$ . For example, we might say $x^{*}=\\arg \\min f(x)$.\nSuppose we have a function $y = f(x)$, where both $x$ and $y$ are real numbers. The derivative of this function is denoted as $f'(x)$ or as $\\frac{dy}{dx}$ . The derivative $f'(x)$ gives the slope of $f(x)$ at the point $x$. In other words, it specifies how to scale a small change in the input in order to obtain the corresponding change in the output: $f(x+\\epsilon) \\approx f(x) \\ +$ $ \\epsilon f'(x)$.\nThe derivative is therefore useful for minimizing a function because it tells us how to change $x$ in order to make a small improvement in $y$. For example, we know that $f(x-\\epsilon \\mathrm{sign}(f'(x)))$ is less than $f(x)$ for small enough $\\epsilon$. We can thus reduce $f(x)$ by moving $x$ in small steps with opposite sign of the derivative. This technique is called gradient descent.\n When $f'(x)=0$, the derivative provides no information about which direction to move. Points where $f'(x)=0$ are known as critical points or stationary points. A local minimum is a point where $f(x)$ is lower than at all neighboring points, so it is no longer possible to decrease $f(x)$ by making infinitesimal steps. A local maximum is a point where $f(x)$ is higher than at all neighboring points, so it is not possible to increase $f(x)$ by making infinitesimal steps. Some critical points are neither maxima nor minima. These are known as saddle points.\n 当 $f'(x)=0$,导数无法提供往哪个方向移动的信息.$f'(x)=0$ 的点称为 临界点(critical point) 或 驻点(stationary point).一个 局部极小点(local minimum) 意味着这个点的 $f(x)$ 小于所有邻近点,因此不可能通过移动无穷小的步长来减小 $f(x)$.一个 局部极大点(local maximum) 意味着这个点的 $f(x)$ 大于所有邻近点,因此不可能通过移动无穷小的步长来增大 $f(x)$.有些临界点既不是最小点也不是最大点.这些点被称为 鞍点(saddle point).\n  A point that obtains the absolute lowest value of $f(x)$ is a global minimum. It is possible for there to be only one global minimum or multiple global minima of the function. It is also possible for there to be local minima that are not globally optimal. In the context of deep learning, we optimize functions that may have many local minima that are not optimal, and many saddle points surrounded by very flat regions. All of this makes optimization very difficult, especially when the input to the function is multidimensional. We therefore usually settle for finding a value of f that is very low, but not necessarily minimal in any formal sense.\n 使 $f(x)$ 取得绝对的最小值(相对所有其他值)的点是 全局最小点(global minimum).函数可能只有一个全局最小点或存在多个全局最小点,还可能存在不是全局最优的局部极小点.\n  We often minimize functions that have multiple inputs: $f: \\mathbb{R}^{n} \\to \\mathbb{R}$. For the concept of “minimization” to make sense, there must still be only one (scalar) output.\nFor functions with multiple inputs, we must make use of the concept of partial derivatives. The partial derivative $\\frac{\\partial}{\\partial x_{i}}$ measures how $f$ changes as only the variable $x_{i}$ increases at point $x$. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $\\nabla_{x} \\ f(x)$. Element $i$ of the gradient is the partial derivative of $f$ with respect to $x_{i}$. In multiple dimensions, critical points are points where every element of the gradient is equal to zero.\n 梯度(gradient) 是相对一个向量求导的导数: $f$ 的导数是包含所有偏导数的向量，记为 $\\nabla_{x} \\ f(x)$。在多维情况下，临界点是梯度中所有元素都为零的点。\n The directional derivative in direction $u$ (a unit vector) is the slope of the function $f$ in direction $u$. In other words, the directional derivative is the derivative of the function $f(x+\\alpha u)$ with respect to $\\alpha$, evaluated at $\\alpha=0$. Using the chain rule, we can see that $\\frac{\\partial}{\\partial \\alpha}f(x+\\alpha u)$ evaluates to $u^{T}\\nabla_{x} \\ f(x)$ when $\\alpha=0$.\nTo minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative:\n$$ \\min_{u,u^{T}u=1} u^{T}\\nabla_{x} \\ f(x) \\\\ = \\min_{u,u^{T}u=1} \\lVert u \\rVert_{2} \\lVert \\nabla_{x} \\ f(x) \\rVert_{2} \\cos \\theta \\\\ $$\nwhere $\\theta$ is the angle between $u$ and the gradient. Substituting in $\\lVert u \\rVert_{2}=1$ and ignoring factors that do not depend on $u$, this simplifies to $\\min_{u} \\cos \\theta$. This is minimized when $u$ points in the opposite direction as the gradient. In other words, the gradient points directly uphill, and the negative gradient points directly downhill. We can decrease $f$ by moving in the direction of the negative gradient. This is known as the method of steepest descent or gradient descent.\nSteepest descent proposes a new point\n$$ x' = x - \\epsilon \\nabla_{x} \\ f(x) \\\\ $$\nwhere $\\epsilon$ is the learning rate, a positive scalar determining the size of the step. We can choose $\\epsilon$ in several different ways. A popular approach is to set $\\epsilon$ to a small constant. Sometimes, we can solve for the step size that makes the directional derivative vanish. Another approach is to evaluate $f(x - \\epsilon \\nabla_{x} \\ f(x))$ for several values of $\\epsilon$ and choose the one that results in the smallest objective function value. This last strategy is called a line search.\nSteepest descent converges when every element of the gradient is zero (or, in practice, very close to zero). In some cases, we may be able to avoid running this iterative algorithm, and just jump directly to the critical point by solving the equation $\\nabla_{x} \\ f(x)=0$ for $x$.\nAlthough gradient descent is limited to optimization in continuous spaces, the general concept of repeatedly making a small move (that is approximately the best small move) towards better configurations can be generalized to discrete spaces. Ascending an objective function of discrete parameters is called hill climbing.\nGradient Descent Variants There are three variants of gradient descent, which differ in how much data we use to compute the gradient of the objective function. Depending on the amount of data, we make a trade-off between the accuracy of the parameter update and the time it takes to perform an update.\nBatch gradient descent Vanilla gradient descent, aka batch gradient descent, computes the gradient of the cost function w.r.t. to the parameters $\\theta$ for the entire training dataset:\n$$ \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta) \\\\ $$\nAs we need to calculate the gradients for the whole dataset to perform just one update, batch gradient descent can be very slow and is intractable for datasets that don\u0026rsquo;t fit in memory. Batch gradient descent also doesn\u0026rsquo;t allow us to update our model online, i.e. with new examples on-the-fly.\n BGD 为梯度下降算法中最基础的一个算法，其损失函数定义如下： $$ J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) \\\\ $$ 针对任意参数$\\theta_{j}$我们可以求得其梯度为： $$ \\nabla_{\\theta}J(\\theta) = \\frac{\\partial J(\\theta)}{\\partial \\theta_{j}} = -\\frac{1}{m}\\sum_{i=1}^{m}\\left(y^{(i)}-h_{\\theta}\\left(x^{(i)}\\right)\\right)x_{j}^{(i)} \\\\ $$ 之后，对于任意参数$\\theta_{j}$我们按照其负梯度方向进行更新： $$ \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta) \\\\ $$ 从上述算法流程中我们可以看到，BGD 算法每次计算梯度都使用了整个训练集，也就是说对于给定的一个初始点，其每一步的更新都是沿着全局梯度最大的负方向。但这同样是其问题，当$m$太大时，整个算法的计算开销就很高了\n Stochastic gradient descent Stochastic gradient descent (SGD) in contrast performs a parameter update for each training example $x^{(i)}$ and label $y^{(i)}$:\n$$ \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta;x^{(i)};y^{(i)}) \\\\ $$\nBatch gradient descent performs redundant computations for large datasets, as it recomputes gradients for similar examples before each parameter update. SGD does away with this redundancy by performing one update at a time. It is therefore usually much faster and can also be used to learn online. SGD performs frequent updates with a high variance that cause the objective function to fluctuate heavily.\nWhile batch gradient descent converges to the minimum of the basin the parameters are placed in, SGD\u0026rsquo;s fluctuation, on the one hand, enables it to jump to new and potentially better local minima. On the other hand, this ultimately complicates convergence to the exact minimum, as SGD will keep overshooting. However, it has been shown that when we slowly decrease the learning rate, SGD shows the same convergence behaviour as batch gradient descent, almost certainly converging to a local or the global minimum for non-convex and convex optimization respectively.\n SGD 相比于 BGD，其最主要的区别就在于计算梯度时不再利用整个数据集，而是针对 单个样本 计算梯度并更新权重，因此，其损失函数定义如下： $$ J(\\theta;x^{(i)};y^{(i)}) = \\frac{1}{2}\\left(h_{\\theta}\\left(x^{(i)}\\right)-y^{(i)}\\right) \\\\ $$ 之后，我们按照其负梯度方向进行更新： $$ \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta;x^{(i)};y^{(i)}) \\\\ $$ SGD 相比于 BGD 具有训练速度快的优势，但同时由于权重改变的方向并不是全局梯度最大的负方向，甚至相反，因此不能够保证每次损失函数都会减小。\n Mini-batch gradient descent Mini-batch gradient descent finally takes the best of both worlds and performs an update for every mini-batch of $b$ training examples:\n$$ \\theta = \\theta - \\eta \\cdot \\nabla_{\\theta}J(\\theta;x^{(i:i+b)};y^{(i:i+b)}) \\\\ $$\nThis way, it reduces the variance of the parameter updates, which can lead to more stable convergence; and can make use of highly optimized matrix optimizations common to state-of-the-art deep learning libraries that make computing the gradient w.r.t. a mini-batch very efficient. Common mini-batch sizes range between 50 and 256, but can vary for different applications. Mini-batch gradient descent is typically the algorithm of choice when training a neural network and the term SGD usually is employed also when mini-batches are used.\n 针对 BGD 和 SGD 的问题，MBGD 则是一个折中的方案，在每次更新参数时，MBGD会选取 $b$个样本计算梯度并更新权重。常见的小批量大小范围在 50 到 256 之间，但可能因不同的应用程序而异。MBGD通常是训练神经网络时选择的算法。\n Challenges in Neural Network Optimization Optimization in general is an extremely difficult task. Traditionally, machine learning has avoided the difficulty of general optimization by carefully designing the objective function and constraints to ensure that the optimization problem is convex. When training neural networks, we must confront the general non-convex case. Even convex optimization is not without its complications.\nMini-batch gradient descent, however, does not guarantee good convergence, but offers a few challenges that need to be addressed:\n  Choosing a proper learning rate can be difficult. A learning rate that is too small leads to painfully slow convergence, while a learning rate that is too large can hinder convergence and cause the loss function to fluctuate around the minimum or even to diverge.\n  Additionally, the same learning rate applies to all parameter updates. If our data is sparse and our features have very different frequencies, we might not want to update all of them to the same extent, but perform a larger update for rarely occurring features.\n  Another key challenge of minimizing highly non-convex error functions common for neural networks is avoiding getting trapped in their numerous suboptimal local minima. Dauphin et al. argue that the difficulty arises in fact not from local minima but from saddle points, i.e. points where one dimension slopes up and another slopes down. These saddle points are usually surrounded by a plateau of the same error, which makes it notoriously hard for SGD to escape, as the gradient is close to zero in all dimensions.\n   梯度下降可能会遇到的问题和挑战：\n 选择合适的学习率可能很困难。学习率太小会导致收敛速度很慢，而学习率太大会阻碍收敛并导致损失函数在最小值附近波动甚至发散。 相同的学习率适用于所有参数更新。如果我们的数据是稀疏的并且我们的特征具有非常不同的频率，我们可能不想将它们全部更新到相同的程度。 最小化神经网络常见的另一个关键挑战是避免陷入其众多次优局部最小值和鞍点。   Gradient Descent Optimization Algorithms Momentum While stochastic gradient descent remains a very popular optimization strategy, learning with it can sometimes be slow. The method of Momentum (Polyak, 1964) is designed to accelerate learning, especially in the face of high curvature, small but consistent gradients, or noisy gradients. The momentum algorithm accumulates an exponentially decaying moving average of past gradients and continues to move in their direction.\n Formally, the momentum algorithm introduces a variable $v$ that plays the role of velocity - it is the direction and speed at which the parameters move through parameter space. The velocity is set to an exponentially decaying average of the negative gradient. Momentum helps accelerate SGD in the relevant direction. It does this by adding a fraction $\\gamma$ of the update vector of the past time step to the current update vector:\n$$ \\begin{align*} v_{t} \u0026amp;= - \\eta\\nabla_{\\theta}J(\\theta_{t}) + \\gamma v_{t-1} \\\\ \\theta_{t} \u0026amp;= \\theta_{t-1} + v_{t} \\end{align*} $$\nEssentially, when using momentum, we push a ball down a hill. The ball accumulates momentum as it rolls downhill, becoming faster and faster on the way. The same thing happens to our parameter updates: The momentum term increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. As a result, we gain faster convergence and reduced oscillation.\n 当梯度沿着一个方向要明显比其他方向陡峭，我们可以形象的称之为峡谷形梯度，这种情况多位于局部最优点附近。在这种情况下，SGD 通常会摇摆着通过峡谷的斜坡，这就导致了其到达局部最优值的速度过慢。因此，针对这种情况，Momentum(动量) 方法提供了一种解决方案。针对原始的 SGD 算法，参数每 $t$ 步的变化量可以表示为 $$ v_{t} = - \\eta\\nabla_{\\theta}J(\\theta_{t}) \\\\ $$ Momentum 算法则在其变化量中添加了一个动量分量，即 $$ \\begin{align*} v_{t} \u0026amp;= - \\eta\\nabla_{\\theta}J(\\theta_{t}) + \\gamma v_{t-1} \\\\ \\theta_{t} \u0026amp;= \\theta_{t-1} + v_{t} \\end{align*} $$ 对于添加的动量项，当第 $t$ 步和第 $t-1$ 步的梯度方向 相同 时， $\\theta$则以更快的速度更新；当第 $t$ 步和第 $t-1$ 步的梯度方向 相反 时， $\\theta$则以较慢的速度更新。\n Nesterov Momentum Sutskever et al. (2013) introduced a variant of the momentum algorithm that was inspired by Nesterov’s accelerated gradient method (Nesterov, 1983, 2004). The update rules in this case are given by:\n$$ \\begin{align*} v_{t} \u0026amp;= - \\eta\\nabla_{\\theta}J(\\theta_{t}+\\gamma v_{t-1}) + \\gamma v_{t-1} \\\\ \\theta_{t} \u0026amp;= \\theta_{t-1} + v_{t} \\end{align*} $$\nwhere the parameters $\\gamma$ and $\\eta$ play a similar role as in the standard momentum method. The difference between Nesterov momentum and standard momentum is where the gradient is evaluated. With Nesterov momentum the gradient is evaluated after the current velocity is applied. Thus one can interpret Nesterov momentum as attempting to add a correction factor to the standard method of momentum.\nWe usually set the momentum term $\\gamma$ to a value of around 0.9. While Momentum first computes the current gradient (small blue vector) and then takes a big jump in the direction of the updated accumulated gradient (big blue vector), NAG first makes a big jump in the direction of the previous accumulated gradient (brown vector), measures the gradient and then makes a correction (red vector), which results in the complete NAG update (green vector). This anticipatory update prevents us from going too fast and results in increased responsiveness, which has significantly increased the performance of RNNs on a number of tasks.\n  NAG (Nesterov Accelerated Gradient) 是一种 Momentum 算法的变种，其核心思想会利用“下一步的梯度”确定“这一步的梯度”，当然这里“下一步的梯度”并非真正的下一步的梯度，而是指仅根据动量项更新后位置的梯度。 $$ \\begin{align*} v_{t} \u0026amp;= - \\eta\\nabla_{\\theta}J(\\theta_{t}+\\gamma v_{t-1}) + \\gamma v_{t-1} \\\\ \\theta_{t} \u0026amp;= \\theta_{t-1} + v_{t} \\end{align*} $$ 针对 Momentum 和 NAG 两种不同的方法，其更新权重的差异如下图所示：\n  AdaGrad AdaGrad is an algorithm for gradient-based optimization that does just this: It adapts the learning rate to the parameters, performing smaller updates (i.e. low learning rates) for parameters associated with frequently occurring features, and larger updates (i.e. high learning rates) for parameters associated with infrequent features. For this reason, it is well-suited for dealing with sparse data.\nPreviously, we performed an update for all parameters $\\theta$ at once as every parameter $\\theta_{i}$ used the same learning rate $\\eta$. As AdaGrad uses a different learning rate for every parameter $\\theta_{i}$ at every time step $t$, we first show AdaGrad\u0026rsquo;s per-parameter update, which we then vectorize. For brevity, we use $g_{t}$ to denote the gradient at time step $t$. $g_{t,i}$ is then the partial derivative of the objective function w.r.t. to the parameter $\\theta_{i}$ at time step $t$:\n$$ g_{t,i} = \\nabla_{\\theta}J(\\theta_{t,i}) \\\\ $$\nThe SGD update for every parameter $\\theta_{i}$ at each time step $t$ then becomes:\n$$ \\theta_{t+1,i} = \\theta_{t,i} - \\eta \\cdot g_{t,i} \\\\ $$\nIn its update rule, AdaGrad modifies the general learning rate $\\eta$ at each time step $t$ for every parameter $\\theta_{i}$ based on the past gradients that have been computed for $\\theta_{i}$:\n$$ \\theta_{t+1,i} = \\theta_{t,i} - \\frac{\\eta}{\\sqrt{G_{t,ii}+\\epsilon}} \\cdot g_{t,i} \\\\ $$\n$G_{t} \\in \\mathbb{R}^{d \\times d}$ here is a diagonal matrix where each diagonal element $i$, $i$ is the sum of the squares of the gradients w.r.t. $\\theta_{i}$ up to time step $t$, while $\\epsilon$ is a smoothing term that avoids division by zero (usually on the order of 1e-8). Interestingly, without the square root operation, the algorithm performs much worse.\n$$ G_{t+1} = G_{t} + g \\odot g \\\\ $$\nAs $G_{t}$ contains the sum of the squares of the past gradients w.r.t. to all parameters $\\theta$ along its diagonal, we can now vectorize our implementation by performing a matrix-vector product $\\odot$ between $G_{t}$ and $g_{t}$:\n$$ \\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t}+\\epsilon}} \\odot g_{t} \\\\ $$\nOne of AdaGrad\u0026rsquo;s main benefits is that it eliminates the need to manually tune the learning rate. Most implementations use a default value of 0.01 and leave it at that.\nAdaGrad\u0026rsquo;s main weakness is its accumulation of the squared gradients in the denominator: Since every added term is positive, the accumulated sum keeps growing during training. This in turn causes the learning rate to shrink and eventually become infinitesimally small, at which point the algorithm is no longer able to acquire additional knowledge.\n AdaGrad 是一种具有自适应学习率的的方法，其对于低频特征的参数选择更大的更新量，对于高频特征的参数选择更小的更新量。因此，AdaGrad算法更加适用于处理稀疏数据。\n RMSProp The RMSProp algorithm (Hinton, 2012) modifies AdaGrad to perform better in the non-convex setting by changing the gradient accumulation into an exponentially weighted moving average. AdaGrad is designed to converge rapidly when applied to a convex function. When applied to a non-convex function to train a neural network, the learning trajectory may pass through many different structures and eventually arrive at a region that is a locally convex bowl. AdaGrad shrinks the learning rate according to the entire history of the squared gradient and may have made the learning rate too small before arriving at such a convex structure. RMSProp uses an exponentially decaying average to discard history from the extreme past so that it can converge rapidly after finding a convex bowl, as if it were an instance of the AdaGrad algorithm initialized within that bowl.\nCompared to AdaGrad, the use of the moving average introduces a new hyperparameter, $\\rho$, that controls the length scale of the moving average. Hinton suggests $\\rho$ to be set to 0.9, while a good default value for the learning rate $\\eta$ is 0.001.\n$$ \\begin{align*} G_{t} \u0026amp;= \\rho G_{t-1} + (1-\\rho) g \\odot g \\\\ \\theta_{t+1} \u0026amp;= \\theta_{t} - \\frac{\\eta}{\\sqrt{G_{t}+\\epsilon}} \\odot g_{t} \\\\ \\end{align*} $$\nEmpirically, RMSProp has been shown to be an effective and practical optimization algorithm for deep neural networks. It is currently one of the go-to optimization methods being employed routinely by deep learning practitioners.\nAdam Adaptive Moment Estimation (Adam) is another method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients $v_{t}$ like RMSprop, Adam also keeps an exponentially decaying average of past gradients $m_{t}$, similar to momentum. Whereas momentum can be seen as a ball running down a slope, Adam behaves like a heavy ball with friction, which thus prefers flat minima in the error surface. We compute the decaying averages of past and past squared gradients $m_{t}$ and $v_{t}$ respectively as follows:\n$$ \\begin{align*} m_{t} \u0026amp;= \\beta_{1}m_{t-1} + (1-\\beta_{1})g_{t} \\\\ v_{t} \u0026amp;= \\beta_{2}v_{t-1} + (1-\\beta_{2})g_{t}^{2} \\\\ \\end{align*} $$\n$m_{t}$ and $v_{t}$ are estimates of the first moment (the mean) and the second moment (the uncentered variance) of the gradients respectively, hence the name of the method. As $m_{t}$ and $v_{t}$ are initialized as vectors of 0\u0026rsquo;s, the authors of Adam observe that they are biased towards zero, especially during the initial time steps, and especially when the decay rates are small (i.e. $\\beta_{1}$ and $\\beta_{2}$ are close to 1).\n$$ \\theta_{t+1} = \\theta_{t} - \\frac{\\eta}{\\sqrt{\\hat{v}_{t}}+\\epsilon}\\hat{m}_{t} \\\\ $$\nThe authors propose default values of 0.9 for $\\beta_{1}$, 0.999 for $\\beta_{2}$, and $10^{-8}$for $\\epsilon$. They show empirically that Adam works well in practice and compares favorably to other adaptive learning-method algorithms.\nIn summary, RMSprop is an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the numinator update rule. Adam, finally, adds bias-correction and momentum to RMSprop. Insofar, RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar circumstances. Kingma et al. show that its bias-correction helps Adam slightly outperform RMSprop towards the end of optimization as gradients become sparser. Insofar, Adam might be the best overall choice.\nReference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/optimization.html.\n[2] Ruder, S. (2020, March 20). An overview of gradient descent optimization algorithms. https://ruder.io/optimizing-gradient-descent/index.html#gradientdescentvariants.\n","permalink":"https://followb1ind1y.github.io/posts/deep_learning/04_optimization_methods_for_deep_learning/","summary":"Gradient Descent Optimization Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function $f(x)$ by altering $x$. We usually phrase most optimization problems in terms of minimizing $f(x)$. Maximization may be accomplished via a minimization algorithm by minimizing $-f(x)$.\nThe function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.","title":"Optimization Methods for Deep Learning"},{"content":"Overfitting, Underfitting and Capacity The central challenge in machine learning is that we must perform well on new, previously unseen inputs—not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.\n 机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好, 而不只是在训练集上表现良好.在先前未观测到的输入上表现良好的能力被称为 泛化(generalization) .\n Typically, when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training error, and we reduce this training error. So far, what we have described is simply an optimization problem. What separates machine learning from optimization is that we want the generalization error, also called the test error, to be low as well. The generalization error is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice.\n 通常情况下, 当我们训练机器学习模型时, 我们可以使用某个训练集, 在训练集上计算一些被称为 训练误差(training error) 的度量误差, 目标是降低训练误差.机器学习和优化不同的地方在于, 我们也希望 泛化误差(generalization error) (也被称为 测试误差(test error) )很低.\n We typically estimate the generalization error of a machine learning model by measuring its performance on a test set of examples that were collected separately from the training set.\nThe train and test data are generated by a probability distribution over datasets called the data generating process. We typically make a set of assumptions known collectively as the i.i.d. assumptions. These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data generating distribution, denoted $p_{data}$. This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error.\nOne immediate connection we can observe between the training and test error is that the expected training error of a randomly selected model is equal to the expected test error of that model. Suppose we have a probability distribution $p(x,y)$ and we sample from it repeatedly to generate the train set and the test set. For some fixed value $w$, the expected training set error is exactly the same as the expected test set error, because both expectations are formed using the same dataset sampling process. The only difference between the two conditions is the name we assign to the dataset we sample.\nOf course, when we use a machine learning algorithm, we do not fix the parameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set. Under this process, the expected test error is greater than or equal to the expected value of training error. The factors determining how well a machine learning algorithm will perform are its ability to:\n Make the training error small. Make the gap between training and test error small.  These two factors correspond to the two central challenges in machine learning: underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.\n 欠拟合(underfitting) 是指模型不能在训练集上获得足够低的误差. 而 过拟合 (overfitting) 是指训练误差和和测试误差之间的差距太大.\n  We can control whether a model is more likely to overfit or underfit by altering its capacity. Informally, a model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set.\nOne way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model’s capacity.\nWe must remember that while simpler functions are more likely to generalize (to have a small gap between training and test error) we must still choose a sufficiently complex hypothesis to achieve low training error. Typically, training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value). Typically, generalization error has a U-shaped curve as a function of model capacity.\n The No Free Lunch Theorem Learning theory claims that a machine learning algorithm can generalize well from a finite training set of examples. This seems to contradict some basic principles of logic. Inductive reasoning, or inferring general rules from a limited set of examples, is not logically valid. To logically infer a rule describing every member of a set, one must have information about every member of that set.\nIn part, machine learning avoids this problem by offering only probabilistic rules, rather than the entirely certain rules used in purely logical reasoning. Machine learning promises to find rules that are probably correct about most members of the set they concern.\nUnfortunately, even this does not resolve the entire problem. The no free lunch theorem for machine learning (Wolpert, 1996) states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points. In other words, in some sense, no machine learning algorithm is universally any better than any other. The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class.\n 机器学习的 没有 免费午餐定理(no free lunch theorem) 表明 (Wolpert, 1996), 在所有可能的数据生成分布上平均之后, 每一个分类算法在未事先观测的点上都有相同的错误率. 换言之, 在某种意义上, 没有一个机器学习算法总是比其他的要好.\n Fortunately, these results hold only when we average over all possible data generating distributions. If we make assumptions about the kinds of probability distributions we encounter in real-world applications, then we can design learning algorithms that perform well on these distributions.\nThis means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the \u0026ldquo;real world\u0026rdquo; that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.\nThe no free lunch theorem implies that we must design our machine learning algorithms to perform well on a specific task. We do so by building a set of preferences into the learning algorithm. When these preferences are aligned with the learning problems we ask the algorithm to solve, it performs better.\nWe can regularize a model that learns a function f(x; θ) by adding a penalty called a regularizer to the cost function.\nExpressing preferences for one function over another is a more general way of controlling a model’s capacity than including or excluding members from the hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.\nA central problem in machine learning is how to make an algorithm that will perform well not just on the training data, but also on new inputs. Many strategies used in machine learning are explicitly designed to reduce the test error, possibly at the expense of increased training error. These strategies are known collectively as regularization. Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization.\n 正则化(regularization) 是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。\n The no free lunch theorem has made it clear that there is no best machine learning algorithm, and, in particular, no best form of regularization. Instead we must choose a form of regularization that is well-suited to the particular task we want to solve.\nParameter Norm Penalties Many regularization approaches are based on limiting the capacity of models, such as neural networks, linear regression, or logistic regression, by adding a parameter norm penalty $\\Omega(\\theta)$ to the objective function $J$. We denote the regularized objective function by $\\tilde{J}$:\n$$ \\tilde{J}(\\theta;X,y) = J(\\theta;X,y) + \\alpha\\Omega(\\theta) \\\\ $$\nwhere $\\alpha \\in [0,\\infty)$ is a hyperparameter that weights the relative contribution of the norm penalty term, $\\Omega$, relative to the standard objective function $J$. Setting $\\alpha$ to 0 results in no regularization. Larger values of $\\alpha$ correspond to more regularization.\nWhen our training algorithm minimizes the regularized objective function $\\tilde{J}$ it will decrease both the original objective $J$ on the training data and some measure of the size of the parameters $\\theta$ (or some subset of the parameters). Different choices for the parameter norm $\\Omega$ can result in different solutions being preferred.\n$L^{2}$ Parameter Regularization $L^{2}$ parameter norm penalty commonly known as weight decay, which is one of the simplest and most common kinds of parameter norm penalty. This regularization strategy drives the weights closer to the origin1 by adding a regularization term:\n$$ \\Omega(\\theta)=\\frac{1}{2}\\lVert w \\rVert_{2}^{2}=\\frac{1}{2}\\sum_{i}w_{i}^{2} $$\nto the objective function. In other academic communities, L2 regularization is also known as ridge regression or Tikhonov regularization.\nWe can gain some insight into the behavior of weight decay regularization by studying the gradient of the regularized objective function. To simplify the presentation, we assume no bias parameter, so $\\theta$ is just $w$. Such a model has the following total objective function:\n$$ \\tilde{J}(\\theta;X,y) = \\frac{\\alpha}{2}w^{\\mathrm{T}}w + J(\\theta;X,y) \\\\ $$\nwith the corresponding parameter gradient\n$$ \\nabla_{w}\\tilde{J}(\\theta;X,y) = \\alpha w + \\nabla_{w}J(\\theta;X,y) \\\\ $$\nTo take a single gradient step to update the weights, we perform this update:\n$$ w \\leftarrow w - \\epsilon(\\alpha w + \\nabla_{w}J(\\theta;X,y)) \\\\ $$\nWritten another way, the update is:\n$$ w \\leftarrow (1 - \\epsilon\\alpha)w - \\epsilon\\nabla_{w}J(\\theta;X,y) \\\\ $$\nWe can see that the addition of the weight decay term has modified the learning rule to multiplicatively shrink the weight vector by a constant factor on each step, just before performing the usual gradient update.\nWe will further simplify the analysis by making a quadratic approximation to the objective function in the neighborhood of the value of the weights that obtains minimal unregularized training cost, $w^{*}=\\arg\\max_{w}J(w)$. If the objective function is truly quadratic, as in the case of fitting a linear regression model with mean squared error, then the approximation is perfect. The approximation $\\hat{J}$ is given by\n$$ \\hat{J}(\\theta) = J(w^{*}) \\frac{1}{2}(w-w^{*})^{\\mathrm{T}}H(w-w^{*}) \\\\ $$\nwhere $H$ is the Hessian matrix of $J$ with respect to $w$ evaluated at $w^{*}$. The minimum of $\\hat{J}$ occurs where its gradient\n$$ \\nabla_{w}\\hat{J}(w) = H(w-w^{*}) $$ is equal to 0.\nTo study the effect of weight decay, we could add the weight decay gradient to the equation. We can now solve for the minimum of the regularized version of $\\hat{J}$. We use the variable $\\tilde{w}$ to represent the location of the minimum.\n$$ \\begin{align*} \\alpha\\tilde{w} + H(\\tilde{w}-w^{*})=0 \\\\ (H+\\alpha I)\\tilde{w} = H w^{*} \\\\ \\tilde{w}=(H+\\alpha I)^{-1} H w^{*} \\\\ \\end{align*} $$\nAs $\\alpha$ approaches 0, the regularized solution $\\tilde{w}$ approaches $w^{*}$. But what happens as $\\alpha$ grows? Because $H$ is real and symmetric, we can decompose it into a diagonal matrix $Lambda$ and an orthonormal basis of eigenvectors, $Q$, such that $H=Q\\Lambda Q^{\\mathrm{T}}$. Applying the decomposition to equation, we obtain:\n$$ \\begin{align*} \\tilde{w}\u0026amp;=(Q\\Lambda Q^{\\mathrm{T}}+\\alpha I)^{-1} Q\\Lambda Q^{\\mathrm{T}} w^{*} \\\\ \u0026amp;=[Q(\\Lambda +\\alpha I)Q^{\\mathrm{T}}]^{-1} Q\\Lambda Q^{\\mathrm{T}} w^{*} \\\\ \u0026amp;=Q(\\Lambda +\\alpha I)^{-1}\\Lambda Q^{\\mathrm{T}} w^{*} \\\\ \\end{align*} $$\nWe see that the effect of weight decay is to rescale $w^{*}$ along the axes defined by the eigenvectors of $H$. Specifically, the component of $w^{*}$ that is aligned with the $i$-th eigenvector of $H$ is rescaled by a factor of\n$$ \\frac{\\lambda_{i}}{\\lambda_{i}+\\alpha} \\\\ $$\nAlong the directions where the eigenvalues of $H$ are relatively large, for example, where $\\lambda_{i} \\gg \\alpha$, the effect of regularization is relatively small. However, components with $\\lambda_{i} \\ll \\alpha$ will be shrunk to have nearly zero magnitude.\n Only directions along which the parameters contribute significantly to reducing the objective function are preserved relatively intact. In directions that do not contribute to reducing the objective function, a small eigenvalue of the Hessian tells us that movement in this direction will not significantly increase the gradient. Components of the weight vector corresponding to such unimportant directions are decayed away through the use of the regularization throughout training.\n$L^{1}$ Parameter Regularization While $L^{2}$ weight decay is the most common form of weight decay, there are other ways to penalize the size of the model parameters. Another option is to use $L^{1}$ regularization.\nFormally, $L^{1}$ regularization on the model parameter $w$ is defined as:\n$$ \\Omega(\\theta) = \\lVert w \\rVert_{1} = \\sum_{i}|w_{i}| \\\\ $$\nthat is, as the sum of absolute values of the individual parameters. We will now discuss the effect of $L^{1}$ regularization on the simple linear regression model, with no bias parameter, that we studied in our analysis of $L^{2}$ regularization. As with $L^{2}$ weight decay, $L^{1}$ weight decay controls the strength of the regularization by scaling the penalty $\\Omega$ using a positive hyperparameter $\\alpha$. Thus, the regularized objective function $\\tilde{J}(\\theta;X,y)$ is given by\n$$ \\tilde{J}(\\theta;X,y) = \\alpha\\lVert w \\rVert_{1} + J(\\theta;X,y) \\\\ $$\nwith the corresponding gradient (actually, sub-gradient):\n$$ \\nabla_{w}\\tilde{J}(\\theta;X,y) = \\alpha \\mathrm{sign}(w) + \\nabla_{w}J(\\theta;X,y) \\\\ $$\nwhere $\\mathrm{sign}(w)$ is simply the sign of $w$ applied element-wise.\n   L0范数：向量中非0元素的个数。 L1范数(Lasso Regularization)：向量中各个元素绝对值的和。 L2范数(Ridge Regression)：向量中各元素平方和求平方根。  L0范数和L1范数都能够达到使 参数稀疏 的目的，但L0范数更难优化求解，L1范数是L1的最优凸相似且更易求解，故得到广泛的应用。 因为 L1 天然的输出稀疏性，把不重要的特征都置为 0，所以它也是一个天然的特征选择器。L2范数主要作用是防止模型过拟合，提高模型的泛化能力。L2 计算起来更方便，而 L1 在特别是非稀疏向量上的计算效率就很低。\n Dataset Augmentation The best way to make a machine learning model generalize better is to train it on more data. Of course, in practice, the amount of data we have is limited. One way to get around this problem is to create fake data and add it to the training set. For some machine learning tasks, it is reasonably straightforward to create new fake data.\nData augmentation could involve increasing the size of the available data set by augmenting them with more input created by random cropping, dilating, rotating, adding a small amount of noise, etc. The idea is to artificially create more data in the hopes that the augmented dataset will be a better representation of the underlying hidden distribution. Since we are limited by the available dataset only, this method generally doesn’t work very well as a regularizer.\n Noise Injection The use of noise applied to the inputs can be considered as a dataset augmentation strategy. For some models, the addition of noise with infinitesimal variance at the input of the model is equivalent to imposing a penalty on the norm of the weights (Bishop, 1995a,b). In the general case, it is important to remember that noise injection can be much more powerful than simply shrinking the parameters, especially when the noise is added to the hidden units.\nAnother way that noise has been used in the service of regularizing models is by adding it to the weights. This technique has been used primarily in the context of recurrent neural networks (Jim et al., 1996; Graves, 2011). This can be interpreted as a stochastic implementation of Bayesian inference over the weights. The Bayesian treatment of learning would consider the model weights to be uncertain and representable via a probability distribution that reflects this uncertainty. Adding noise to the weights is a practical, stochastic way to reflect this uncertainty.\nEarly Stopping When training large models with sufficient representational capacity to overfit the task, we often observe that training error decreases steadily over time, but validation set error begins to rise again. This behavior occurs very reliably.\nThis means we can obtain a model with better validation set error (and thus, hopefully better test set error) by returning to the parameter setting at the point in time with the lowest validation set error. Every time the error on the validation set improves, we store a copy of the model parameters. When the training algorithm terminates, we return these parameters, rather than the latest parameters. The algorithm terminates when no parameters have improved over the best recorded validation error for some pre-specified number of iterations.\nThis strategy is known as early stopping. It is probably the most commonly used form of regularization in deep learning. Its popularity is due both to its effectiveness and its simplicity.\nEarly stopping is a very unobtrusive form of regularization, in that it requires almost no change in the underlying training procedure, the objective function, or the set of allowable parameter values. This means that it is easy to use early stopping without damaging the learning dynamics. This is in contrast to weight decay, where one must be careful not to use too much weight decay and trap the network in a bad local minimum corresponding to a solution with pathologically small weights.\nEarly stopping may be used either alone or in conjunction with other regularization strategies. Even when using regularization strategies that modify the objective function to encourage better generalization, it is rare for the best generalization to occur at a local minimum of the training objective.\nEarly stopping is also useful because it reduces the computational cost of the training procedure. Besides the obvious reduction in cost due to limiting the number of training iterations, it also has the benefit of providing regularization without requiring the addition of penalty terms to the cost function or the computation of the gradients of such additional terms.\n (Left) The solid contour lines indicate the contours of the negative log-likelihood. The dashed line indicates the trajectory taken by SGD beginning from the origin. Rather than stopping at the point $w^{*}$ that minimizes the cost, early stopping results in the trajectory stopping at an earlier point $\\tilde{w}$.\n(Right) An illustration of the effect of $L^{2}$ regularization for comparison. The dashed circles indicate the contours of the $L^{2}$ penalty, which causes the minimum of the total cost to lie nearer the origin than the minimum of the unregularized cost.\nDropout Dropout is used when the training model is a neural network. A neural network consists of multiple hidden layers, where the output of one layer is used as input to the subsequent layer. The subsequent layer modifies the input through learnable parameters (usually by multiplying it by a matrix and adding a bias followed by an activation function). The input flows through the neural network layers until it reaches the final output layer, which is used for prediction.\nEach layer in the neural network consists of various nodes. Nodes from the previous layer are connected to nodes of the subsequent layer. In the dropout method, connections between the nodes of consecutive layers are randomly dropped based on a dropout-ratio (%age of the total connection dropped) and the remaining network is trained in the current iteration. In the next iteration, another set of random connections are dropped.\n The dropout method ensures that the neural network learns a more robust set of features that perform equally well with random subsets of the node selected. By randomly dropping connections, the network is able to learn a better-generalized mapping from input to output hence reducing the over-fitting. The dropout ratio needs to be carefully selected and has a significant impact on the learned model. A good value of the dropout ratio is between 0.25 to 0.4.\nReference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/regularization.html.\n[2] Anwar, A. (2021, April 7). Types of regularization in machine learning. Medium. https://towardsdatascience.com/types-of-regularization-in-machine-learning-eb5ce5f9bf50.\n","permalink":"https://followb1ind1y.github.io/posts/deep_learning/03_regularization_for_deep_learning/","summary":"Overfitting, Underfitting and Capacity The central challenge in machine learning is that we must perform well on new, previously unseen inputs—not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.\n 机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好, 而不只是在训练集上表现良好.在先前未观测到的输入上表现良好的能力被称为 泛化(generalization) .\n Typically, when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training error, and we reduce this training error.","title":"Regularization for Deep Learning"},{"content":"In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).\n RBNN is composed of input, hidden, and output layer. RBNN is strictly limited to have exactly one hidden layer. We call this hidden layer as feature vector. We apply non-linear transfer function to the feature vector before we go for classification problem. When we increase the dimension of the feature vector, the linear separability of feature vector increases.\nNetwork Structure  1. Input : $$ \\{(x_{i},y_{i})\\}_{i=1}^{n} \\ , \\ \\mathrm{where} \\ x_{i} \\subset \\mathbb{R}^{d} \\\\ $$\n$$ X = \\begin{bmatrix} x_{11} \\ \\cdots \\ x_{1n} \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ x_{d1} \\ \\cdots \\ x_{dn} \\\\ \\end{bmatrix}_{\\ d\\times n} = \\begin{bmatrix} \\vdots \\ \\ \\ \\vdots \\\\ x_{1} \\ \\cdots \\ x_{n} \\\\ \\vdots \\ \\ \\ \\vdots \\\\ \\end{bmatrix}_{\\ d\\times n} \\ , \\ Y = \\begin{bmatrix} y_{11} \\ \\cdots \\ y_{1n} \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ y_{k1} \\ \\cdots \\ y_{kn} \\\\ \\end{bmatrix}_{\\ k\\times n} $$\n2. Radial Basis Function  We define a $\\mathrm{receptor} = t$. We draw confrontal maps around the $\\mathrm{receptor}$. Gaussian Functions are generally used for Radian Basis Function(confrontal mapping). So we define the radial distance $r = \\parallel x- t \\parallel.$ There are many choices for the basis function. The commonly used is:  $$ \\phi_{j}(x_{i}) = e^{-|x_{i}\\ - \\ \\mu_{j}|^{2}} $$\n 3. Output: $$ y_{k}(x)=\\sum_{j=1}^{m}W_{jk}\\phi_{j}(x) $$\n$$ W = \\begin{bmatrix} w_{11} \\ \\cdots \\ w_{1k} \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ w_{m1} \\ \\cdots \\ w_{mk} \\\\ \\end{bmatrix}_{\\ m\\times k} \\ , \\ \\phi = \\begin{bmatrix} \\phi_{1}(x_{1}) \\ \\cdots \\ \\phi_{1}(x_{n}) \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ \\phi_{m}(x_{1}) \\ \\cdots \\ \\phi_{m}(x_{n}) \\\\ \\end{bmatrix}_{\\ m\\times n} $$\nThe output will be:\n$$ Y = W^{T}\\phi $$\nwhere $Y$ and $\\phi$ are known while $W$ is unknown.\nOptimization $$ \\psi = \\parallel Y - W^{T}\\phi \\ \\parallel^{2} $$\n$W$ can be computed by minimizing our objective function w.r.t $w$.\n$$ \\min_{W}\\parallel Y - W^{T}\\phi \\ \\parallel^{2} $$\nThis optimization problem can be solved in close form:\n$$ \\begin{align*} \\frac{\\partial}{\\partial W}\\parallel Y - W^{T}\\phi \\ \\parallel^{2} \u0026amp;= \\frac{\\partial}{\\partial W}Tr[(Y - W^{T}\\phi)^{T}(Y - W^{T}\\phi)] \\\\ \u0026amp;= \\frac{\\partial}{\\partial W}Tr[Y^{T}Y+\\phi^{T}WW^{T}\\phi-Y^{T}W^{T}\\phi - \\phi^{T}WY] \\\\ \u0026amp;= 0 + 2\\phi\\phi^{T}W - 2\\phi Y^{T} = 0 \\\\ \u0026amp;\\Rightarrow \\phi\\phi^{T}W = \\phi Y^{T} \\\\ \u0026amp;\\Rightarrow W = (\\phi\\phi^{T})^{-1}\\phi Y^{T} \\end{align*} $$\nIn RBF network the estimated function is:\n$$ \\begin{align*} \\hat{Y} \u0026amp;= W^{T}\\phi \\\\ \u0026amp;= ((\\phi\\phi^{T})^{-1}\\phi Y^{T})^{T}\\phi \\\\ \u0026amp;= Y\\phi^{T}((\\phi\\phi^{T})^{-1})^{T}\\phi \\\\ \\Rightarrow \\hat{Y}^{T} \u0026amp;= \\underbrace{\\phi^{T}(\\phi\\phi^{T})^{-1}\\phi}_{H} Y^{T} \\\\ \\Rightarrow \\hat{Y}^{T} \u0026amp;= HY^{T} \\end{align*} $$\nStein’s unbiased risk estimator (SURE) Assume $ T = \\{(x_{i},y_{i})\\}_{i=1}^{n} $ be the training set.\n $f(\\cdot)$ $\\to$ True model $\\hat{f}(\\cdot)$ $\\to$ Estimated model err $\\to$ Empirical error: $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}$ Err $\\to$ True error: $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{f}_{i}-f_{i})^{2}$ $y$ $\\to$ Observations  Also assume\n$$ y_{i} = f(x_{i}) + \\varepsilon_{i} \\ , \\ \\ \\ \\ \\mathrm{Where} \\ \\varepsilon_{i} \\sim N(0,\\sigma^{2}) $$\nFor point $(x_{0},y_{0})$ we are interested in\n$$ \\begin{align*} E[(\\hat{y}_{0}-y_{0})^{2}] \u0026amp;= E[(\\hat{f}_{0}-f_{0}-\\varepsilon_{0})^{2}] \\\\ \u0026amp;= E[((\\hat{f}_{0}-f_{0})-\\varepsilon_{0})^{2}] \\\\ \u0026amp;= E[(\\hat{f}_{0}-f_{0})^{2}+\\varepsilon_{0}^{2}-2\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\ \u0026amp;= E[(\\hat{f}_{0}-f_{0})^{2}]+E[\\varepsilon_{0}^{2}]-2E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\ \u0026amp;= E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2}-2E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\ \\end{align*} $$\nCase 1 Assume $(x_{0},y_{0})\\notin T$.\nIn this case, since $\\hat{f}$ is estimated only based on points in training set, therefore it is completely independent from $(x_{0},y_{0})$\n$$ E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] = 0 \\\\ \\Rightarrow E[(\\hat{y}_{0}-y_{0})^{2}] = E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2} $$\nIf summing up all $m$ points that are not in $T$.\n$$ \\underbrace{\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}_{\\mathrm{err}}= \\underbrace{\\sum_{i=1}^{m}(\\hat{f}_{i}-f_{i})^{2}}_{\\mathrm{Err}}+ m\\sigma^{2} \\\\ \\mathrm{err} = \\mathrm{Err} + m\\sigma^{2} \\\\ \\mathrm{Err} = \\mathrm{err} - m\\sigma^{2} $$\nEmpirical error ($\\mathrm{err}$) is a good estimator of true error ($\\mathrm{Err}$) if the point $(x_{0},y_{0})$ is not in the training set.\nCase 2 Assume $(x_{0},y_{0})\\in T$. Then $E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\neq 0$\nStein\u0026rsquo;s Lemma: If $x \\sim N(\\theta,\\sigma^{2})$ and $g(x)$ differentiable. Then,\n$$ E[g(x)(x-\\theta)]=\\sigma^{2}E[\\frac{\\partial g(x)}{\\partial x}] $$\n$$ \\begin{align*} E[\\underbrace{\\varepsilon_{0}}_{(x-\\theta)}\\underbrace{(\\hat{f}_{0}-f_{0})}_{g(x)}] \u0026amp;=\\sigma^{2}E[\\frac{\\partial (\\hat{f}_{0}-f_{0})}{\\partial \\varepsilon_{0}}] \\\\ \u0026amp;= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial \\varepsilon_{0}}-\\underbrace{\\frac{\\partial f_{0}}{\\partial \\varepsilon_{0}}}_{0}] \\\\ \u0026amp;= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial \\varepsilon_{0}}] \\\\ \u0026amp;= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial y_{0}}\\cdot\\underbrace{\\frac{\\partial y_{0}}{\\partial \\varepsilon_{0}}}_{1}] \\\\ \u0026amp;= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial y_{0}}] \\\\ \u0026amp;= \\sigma^{2}E[D_{0}] \\end{align*} $$\nThus,\n$$ E[(\\hat{y}_{0}-y_{0})^{2}] = E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2} - 2\\sigma^{2}E[D_{0}] $$\nSum over all $n$ data points:\n$$ \\underbrace{\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}}_{\\mathrm{err}}= \\underbrace{\\sum_{i=1}^{n}(\\hat{f}_{i}-f_{i})^{2}}_{\\mathrm{Err}}+ n\\sigma^{2} -2\\sigma^{2}\\sum_{i=1}^{n}D_{i} \\\\ \\mathrm{Err} = \\mathrm{err} - n\\sigma^{2}+\\underbrace{2\\sigma^{2}\\sum_{i=1}^{n}D_{i}}_{\\mathrm{Complexity \\ of \\ model}} \\Rightarrow \\mathrm{Stein’s \\ Unbiased \\ Risk \\ Estimator \\ (SURE)} $$\nCoplexity control for RBN Let\u0026rsquo;s apply SURE to RBF:\n$$ \\left. \\begin{array} \\\\ D_{i} = \\frac{\\partial \\hat{f}_{i}}{\\partial y_{i}} \\\\ \\hat{f}_{i} = \\hat{y}_{i} = H_{i:y} \\end{array} \\right \\}D_{i}=\\frac{\\partial \\hat{f}_{i}}{\\partial y_{i}} = \\frac{\\partial H_{i:y}}{\\partial y_{i}} = H_{ii} $$\nThen SURE will be:\n$$ \\begin{align*} \\mathrm{Err} \u0026amp;= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}\\sum_{i=1}^{n}H_{ii} \\\\ \u0026amp;= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr(H) \\\\ \u0026amp;= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr[\\phi^{T}(\\phi\\phi^{T})^{-1}\\phi] \\\\ \u0026amp;= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr[\\underbrace{\\phi\\phi^{T}(\\phi\\phi^{T})^{-1}}_{I\\to m\\times m}] \\\\ \u0026amp;= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr[I_{m}] \\\\ \u0026amp;= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}m \\\\ \\end{align*} $$\nFor computing SURE, we need to know the value of $\\sigma$. But we do not know it. Therefore we need to estimate it.\n$$ \\sigma^{2} = \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}}{n-1} $$\nis the function of complexity (more complex, smaller $\\sigma^{2}$), in practice, we do not consider the $\\hat{y}$ to be the function of complexity and instead we consider it to be a low bias and high variance estimation (for example a line). With this assumption the $\\sigma$ will be considered to be constant and independent from the complexity of model.\nReference [1] McCormick, C. (2013, August 15). Radial Basis Function Network (RBFN) Tutorial. Radial Basis Function Network (RBFN) Tutorial · Chris McCormick. https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/.\n","permalink":"https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/","summary":"In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).\n RBNN is composed of input, hidden, and output layer.","title":"Neural Network: Radial Basis Function Neural Networks (RBN)"},{"content":"Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.\n The first thing that comes to our mind when we think of \u0026ldquo;neural networks\u0026rdquo; is biology, and indeed, neural nets are inspired by our brains. In machine learning, the neurons' dendrites refer to as input, and the nucleus process the data and forward the calculated output through the axon. In a biological neural network, the width (thickness) of dendrites defines the weight associated with it.\n Perceptron A perceptron is a neural network without any hidden layer. A perceptron only has an input layer and an output layer. Perceptrons can be represented graphically as:\n Where $x_{i}$ is the $i$-th feature of a sample and $\\beta_{i}$ is the $i$-th weight. $\\beta_{0}$ is defined as the bias. The bias alters the position of the decision boundary between the two classes. From a geometrical point of view, Perceptron assigns label \u0026ldquo;1\u0026rdquo; to elements on one side of $\\beta^{T}x+\\beta_{0}$ and label \u0026ldquo;-1\u0026rdquo; to elements on the other side. Define a cost function, $\\phi(\\beta,\\beta_{0})$, as a summation of the distance between all misclassified points and the hyperplane, or the decision boundary. To minimize the cost function, we need to estimate $\\beta$, $\\beta_{0}$.\n$$ \\min_{\\beta,\\beta_{0}}\\phi(\\beta,\\beta_{0})=\\mathrm{distance \\ of \\ all \\ misclassified \\ points} \\\\ $$\n 1. A hyperplane $L$ can be defined as:\n$$ L=\\{x:f(x)= \\beta^{T}x+\\beta_{0}=0\\} \\\\ $$\nFor any arbitrary points $x_{1}$ and $x_{2}$ on $L$, we have\n$$ \\beta^{T}x_{1}+\\beta_{0}=0 \\\\ \\beta^{T}x_{2}+\\beta_{0}=0 \\\\ \\mathrm{Such \\ that \\ } \\beta^{T}(x_{1}-x_{2})=0 \\\\ $$\n2. For any $x_{0}$ on the hyperplane,\n$$ \\beta^{T}x_{0}+\\beta_{0}=0 \\Rightarrow \\beta^{T}x_{0} = -\\beta_{0} \\\\ $$\n3. We set $\\beta^{*}=\\frac{\\beta}{\\parallel\\beta\\parallel}$ as the unit normal vector of the hyperplane $L$. For simplicity we can call $\\beta^{*}$ norm vector. The distance of point $x$ to $L$ is given by\n$$ \\beta^{*T}(x-x_{0}) = \\beta^{*T}x - \\beta^{*T}x_{0}= \\frac{\\beta^{T}x}{\\parallel\\beta\\parallel}+\\frac{\\beta_{0}}{\\parallel\\beta\\parallel} = \\frac{(\\beta^{T}x+\\beta_{0})}{\\parallel\\beta\\parallel} \\\\ $$\nWhere $x_{0}$ is any point on $L$. Hence, $\\beta^{T}x+\\beta_{0}$ is proportional to the distance of the point $x$ to the hyperplane $L$.\n4. The distance from a misclassified data point $x_{i}$ to the hyperplane $L$ is\n$$ d_{i}=-y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \\\\ $$\nWhere $y_{i}$ is a target value, such that $y_{i}=1$ if $\\beta^{T}x_{i}+\\beta_{0}\u0026lt;0$, $y_{i}=-1$ if $\\beta^{T}x_{i}+\\beta_{0}\u0026gt;0$\nSince we need to find the distance from the hyperplane to the misclassified data points, we need to add a negative sign in front. When the data point is misclassified, $\\beta^{T}x_{i}+\\beta_{0}$ will produce an opposite sign of $y_{i}$. Since we need a positive sign for distance, we add a negative sign.\nPerceptron Learning using Gradient Descent The gradient descent is an optimization method that finds the minimum of an objective function by incrementally updating its parameters in the negative direction of the derivative of this function. In our case, the objective function to be minimized is classification error and the parameters of this function are the weights associated with the inputs $\\beta$. The gradient descent algorithm updates the weights as follows:\n$$ \\beta^{new} \\leftarrow \\beta^{old} - \\rho \\frac{\\partial Err}{\\partial \\beta} \\\\ $$\nWhere $\\rho$ is called the learning rate.\nThe classification error can be defined as the distance of misclassified observations to the decision boundary,\n$$ D(\\beta) = -\\sum_{i\\in M}y_{i}\\beta^{T}x_{i} \\\\ $$\nWhere $M$ is the set of misclassified points. The quantity $y_{i}\\beta^{T}x_{i}$ will be negative if $x_{i}$ is misclassified. By taking the derivative of $D(\\beta)$ with respect to $\\beta$\n$$ \\begin{align*} \\frac{\\partial D}{\\partial \\beta} \u0026amp;= - \\sum_{i\\in M}y_{i}x_{i} \\\\ \\frac{\\partial D}{\\partial \\beta_{0}} \u0026amp;= - \\sum_{i\\in M}y_{i} \\\\ \\end{align*} $$\nThe update formula becomes\n$$ \\beta^{new} \\leftarrow \\beta^{old} + \\rho \\sum_{i\\in M}y_{i}x_{i} \\\\ $$\nWhich is equivalent to incrementally updating $\\beta$ for each misclassified point $x_{i}$\n$$ \\beta^{new} \\leftarrow \\beta^{old} + \\rho y_{i}x_{i} \\\\ $$\nThe intuition behind this update is that for misclassified point $x_{i}$, $\\beta$ should be changed in the direction that makes $x_{i}$ as close as possible to the right side. Figure 2 shows how $\\beta$ is updated.\n Separability and Convergence The training set $D$ is said to be linearly separable if there exits a positive constant $\\gamma$ and a weight vector $\\beta$ such that $(\\beta^{T}x_{i}+\\beta_{0})y_{i}\u0026gt;\\gamma$ for all $1\u0026lt;i\u0026lt;n$. That is, if we say that $\\beta$ is the weight vector of Perceptron and $y_{i}$ is the true label of $x_{i}$, then the signd distance of the $x_{i}$ from $\\beta$ is greater than a positive constant $\\gamma$ for any $(x_{i},y_{i})\\in D$.\nIf data is linearly-separable, the solution is theoretically guranteed to converge to a separating hyperplane in a finite numver of iterations. In this situation the number of iterations depends on the learning rate and the margin. However, if the data is not linearly separable there is no guarantee that the algorithm converges.\nFeatures   A Perceptron can only discriminate between two classes at a time.\n  When data is (linearly) separable, there are an infinite number of solutions depending on the starting point.\n  Even though convergence to a solution is guaranteed if the solution exists, the finite number of steps until convergence can be very large.\n  The smaller the gap between the two classes, the longer the time of convergence.\n  When the data is not separable, the algorithm will not converge (it should be stopped after N steps).\n  A learning rate that is too high will make the perceptron periodically oscillate around the solution unless additional steps are taken.\n  The L.S. compute a linear combination of feature of input and return the sign.\n  Learning rate affects the accuracy of the solution and the number of iterations directly.\n  Feedforward Deep Networks Feedforward neural networks are artificial neural networks where the connections between units do not form a cycle. Feedforward neural networks were the first type of artificial neural network invented and are simpler than their counterpart, recurrent neural networks. They are called feedforward because information only travels forward in the network (no loops), first through the input nodes, then through the hidden nodes (if present), and finally through the output nodes.\nFeedforward neural network is a multistage regression or classification model typically represented by a graphical diagram. Regression usually produces one output unit $Y_{1}$ while for $k$ - classification there are $k$ output units $Y_{1...k}$ with each $Y_{k}$ coded as 0 − 1 to represent the $k^{th}$ class.\n where $a_{i} = u \\cdot x$ and $z_{i} = \\phi(a_{i})$ which is a non-linear function with an example being $\\phi(a) = \\frac{1}{1+e^{−a}}$. The function $\\phi$ is called the activation function and is used in classification not regression.\nFeedforward deep networks, a.k.a. multilayer perceptrons (MLPs), are parametric function composed of several parametric function. Each layer of the network defines one of these sub-functions. Each layer (sub-function) has multiple inputs and multiple outputs. Each layer composed of many units (scalar output of the layer). We sometimes refer to each unit as a feature. Each unit is usually a simple transformation of its input. Also, the entire network can be very complex.\n 深度前馈网络(deep feedforward network)，也叫作 前馈神经网络(feedforward neural network) 或者 多层感知机(multilayer perceptron, MLP) ，是典型的深度学习模型。前馈网络的目标是近似某个函数 $f^{*}$。\n Backpropagation Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.\nBackpropagation is a short form for \u0026ldquo;backward propagation of errors.\u0026rdquo; It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network.\n $$ Error = |\\hat{Y}-Y|^{2}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ a_{i} = \\sum_{l}z_{l}u_{il}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ z_{i} = \\sigma(a_{i}), \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma(a) = \\frac{1}{1+e^{-a}} $$\nTake the derivative with respect to weight $u_{il}$\n$$ \\frac{\\partial Error}{\\partial u_{il}} = \\underbrace{\\frac{\\partial Error}{\\partial a_{i}}}_{\\delta_{i}\\ (Unknown)} \\cdot \\underbrace{\\frac{\\partial a_{i}}{\\partial u_{il}}}_{z_{l} \\ (Known)} \\\\ $$\n$$ \\begin{align*} \\delta_{i} = \\frac{\\partial Error}{\\partial a_{i}} \u0026amp;= \\sum_{j}\\underbrace{\\frac{\\partial Error}{\\partial a_{j}}}_{\\delta_{j}} \\cdot \\frac{\\partial a_{j}}{\\partial a_{i}} \\to (\\frac{\\partial a_{j}}{\\partial z_{i}}\\cdot \\frac{\\partial z_{i}}{\\partial a_{i}}) \\\\ \u0026amp;= \\sum_{j}\\delta_{j} \\cdot u_{ji} \\cdot \\sigma'(a_{i}) = \\sigma'(a_{i})\\sum_{j}\\delta_{j} \\cdot u_{ji} \\end{align*} $$\nNote that if $\\sigma(x)$ is the sigmoid function, then\n$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) $$\nNow considering $\\delta_{k}$ for the output layer:\n$$ \\delta_{k} = \\frac{\\partial Error}{\\partial a_{k}} = \\frac{\\partial (y-\\hat{y})^{2}}{\\partial a_{k}} = -2(y-\\hat{y}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where \\ a_{k} = \\hat{y} $$\nThe network weights are updated using the backpropagation algorithm when each training data point $x$ is fed into the feed forward neural network (FFNN).\n$$ u_{il}^{new} \\leftarrow u_{il}^{old} - \\rho \\cdot \\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}} $$\nBackpropagation procedure   First arbitrarily choose some random weights (preferably close to zero) for your network.\n  Apply $x$ to the FFNN\u0026rsquo;s input layer, and calculate the outputs of all input neurons.\n  Propagate the outputs of each hidden layer forward, one hidden layer at a time, and calculate the outputs of all hidden neurons.\n  Once $x$ reaches the output layer, calculate the output(s) of all output neuron(s) given the outputs of the previous hidden layer.\n  At the output layer, compute $\\delta_{k} = −2(y_{k} − \\hat{y}_{k} )$ for each output neuron(s).\n  Compute each $\\delta_{i}$, starting from $i = k − 1$ all the way to the first hidden layer, where $\\delta_{i}=\\sigma'(a_{i})\\sum_{j}\\delta_{j} \\cdot u_{ji}$\n  Compute $\\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}} = \\delta_{i}z_{l}$ for all weights $u_{il}$.\n  Then update $u_{il}^{new} \\leftarrow u_{il}^{old} - \\rho\\cdot \\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}}$ for all weights $u_{il}$.\n  Continue for next data points and iterate on the training set until weights converge.\n  It is common to cycle through the all of the data points multiple times in order to reach convergence. An epoch represents one cycle in which you feed all of your datapoints through the neural network. It is good practice to randomized the order you feed the points to the neural network within each epoch; this can prevent your weights changing in cycles. The number of epochs required for convergence depends greatly on the learning rate \u0026amp; convergence requirements used.\nReference [1] Odegua, R. (2021, April 8). Building a Neural Network From Scratch Using Python (Part 1). Medium. https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432.\n[2] Shukla, P., \u0026amp; Iriondo, R. (2021, April 2). Neural Networks from Scratch with Python Code and Math in Detail- I. Medium. https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#3a44.\n","permalink":"https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/","summary":"Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.","title":"Neural Network: Perceptron and Backpropagation"},{"content":"Learning Algorithms A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning? Mitchell (1997) provides the definition \u0026ldquo;A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.\u0026rdquo;\nThe Task, $T$ Machine learning allows us to tackle tasks that are too difficult to solve with fixed programs written and designed by human beings. From a scientific and philosophical point of view, machine learning is interesting because developing our understanding of machine learning entails developing our understanding of the principles that underlie intelligence.\nIn this relatively formal definition of the word \u0026ldquo;task,\u0026rdquo; the process of learning itself is not the task. Learning is our means of attaining the ability to perform the task.\nMachine learning tasks are usually described in terms of how the machine learning system should process an example. An example is a collection of features that have been quantitatively measured from some object or event that we want the machine learning system to process. We typically represent an example as a vector $x \\in \\mathbb{R}$ where each entry $x_{i}$ of the vector is another feature. For example, the features of an image are usually the values of the pixels in the image.\n 通常机器学习任务定义为机器学习系统应该如何处理 样本(example). 样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的 特征 (feature) 的集合.\n Many kinds of tasks can be solved with machine learning. Some of the most common machine learning tasks include the following:\n Classification: In this type of task, the computer program is asked to specify which of $k$ categories some input belongs to. To solve this task, the learning algorithm is usually asked to produce a function $f: \\mathbb{R}^{n} \\to \\{1, \\cdots,k \\}$. When $y=f(x)$, the model assigns an input described by vector $x$ to a category identified by numeric code $y$. There are other variants of the classification task, for example, where $f$ outputs a probability distribution over classes. An example of a classification task is object recognition, where the input is an image (usually described as a set of pixel brightness values), and the output is a numeric code identifying the object in the image. Object recognition is the same basic technology that allows computers to recognize faces (Taigman et al., 2014), which can be used to automatically tag people in photo collections and allow computers to interact more naturally with their users.   分类: 在这类任务中, 计算机程序需要指定某些输入属于 $k$ 类中的哪一类. 为了完成这个任务, 学习算法通常会返回一个函数 $f: \\mathbb{R}^{n} \\to \\{1, \\cdots,k \\}$.当 $y=f(x)$ 时, 模型将向量 $x$ 所代表的输入分类到数字码 $y$ 所代表的类别.还有一些其他的分类问题, 例如, $f$ 输出的是不同类别的概率分布. 代表示例有： 人脸识别.\n  Classification with missing inputs: Classification becomes more challenging if the computer program is not guaranteed that every measurement in its input vector will always be provided. In order to solve the classification task, the learning algorithm only has to define a single function mapping from a vector input to a categorical output. When some of the inputs may be missing, rather than providing a single classification function, the learning algorithm must learn a set of functions. Each function corresponds to classifying $x$ with a different subset of its inputs missing. This kind of situation arises frequently in medical diagnosis, because many kinds of medical tests are expensive or invasive. One way to efficiently define such a large set of functions is to learn a probability distribution over all of the relevant variables, then solve the classification task by marginalizing out the missing variables. With $n$ input variables, we can now obtain all $2^{n}$ different classification functions needed for each possible set of missing inputs, but we only need to learn a single function describing the joint probability distribution.   输入缺失分类: 当输入向量的每个度量不被保证的时候, 分类问题将会变得更有挑战性. 为了解决分类任务, 学习算法只需要定义一个从输入向量映射到输出类别的函数. 当一些输入可能丢失时, 学习算法必须学习一组函数, 而不是单个分类函数. 每个函数对应着分类具有不同缺失输入子集的 $x$. 这种情况在 医疗诊断中经常出现, 因为很多类型的医学测试是昂贵的, 对身体有害的.\n  Regression: In this type of task, the computer program is asked to predict a numerical value given some input. To solve this task, the learning algorithm is asked to output a function $f:\\mathbb{R}^{n}\\to\\mathbb{R}$. This type of task is similar to classification, except that the format of output is different. An example of a regression task is the prediction of the expected claim amount that an insured person will make (used to set insurance premiums), or the prediction of future prices of securities. These kinds of predictions are also used for algorithmic trading.   回归: 在这类任务中, 计算机程序需要对给定输入预测数值.为了解决这个任务, 学习算法需要输出函数 $f:\\mathbb{R}^{n}\\to\\mathbb{R}$.除了返回结果的形式不一样外, 这类问题和分类问题是很像的. 代表示例有：预测证券未来的价格.\n  Transcription: In this type of task, the machine learning system is asked to observe a relatively unstructured representation of some kind of data and transcribe it into discrete, textual form. For example, in optical character recognition, the computer program is shown a photograph containing an image of text and is asked to return this text in the form of a sequence of characters (e.g., in ASCII or Unicode format). Another example is speech recognition, where the computer program is provided an audio waveform and emits a sequence of characters or word ID codes describing the words that were spoken in the audio recording. Deep learning is a crucial component of modern speech recognition systems used at major companies.   转录: 这类任务中, 机器学习系统观测一些相对非结构化表示的数据, 并转录信息为离散的文本形式.代表示例有：光学字符识别要求计算机程序根据文本图片返回文字序列、语音识别.\n  Machine translation: In a machine translation task, the input already consists of a sequence of symbols in some language, and the computer program must convert this into a sequence of symbols in another language. This is commonly applied to natural languages, such as translating from English to French.   机器翻译: 在机器翻译任务中, 输入是一种语言的符号序列, 计算机程序必须将其转化成另一种语言的符号序列. 代表示例有：英语翻译成法语.\n  Structured output: Structured output tasks involve any task where the output is a vector (or other data structure containing multiple values) with important relationships between the different elements. This is a broad category, and subsumes the transcription and translation tasks described above, but also many other tasks. One example is parsing—mapping a natural language sentence into a tree that describes its grammatical structure and tagging nodes of the trees as being verbs, nouns, or adverbs, and so on. Another example is pixel-wise segmentation of images, where the computer program assigns every pixel in an image to a specific category.   结构化输出: 结构化输出任务的输出是向量或者其他包含多个值的数据结构, 并且构成输出的这些不同元素间具有重要关系.这是一个很大的范畴, 包括上述转录任务和翻译任务在内的很多其他任务. 代表示例有：语法分析、图像的像素级分割, 将每一个像素分配到特定类别.\n  Anomaly detection: In this type of task, the computer program sifts through a set of events or objects, and flags some of them as being unusual or atypical. An example of an anomaly detection task is credit card fraud detection. By modeling your purchasing habits, a credit card company can detect misuse of your cards. If a thief steals your credit card or credit card information, the thief’s purchases will often come from a different probability distribution over purchase types than your own. The credit card company can prevent fraud by placing a hold on an account as soon as that card has been used for an uncharacteristic purchase.   异常检测: 在这类任务中, 计算机程序在一组事件或对象中筛选, 并标记不正常或非典型的个体. 异常检测任务的一个示例是信用卡欺诈检测.\n  Synthesis and sampling: In this type of task, the machine learning algorithm is asked to generate new examples that are similar to those in the training data. Synthesis and sampling via machine learning can be useful for media applications where it can be expensive or boring for an artist to generate large volumes of content by hand. For example, video games can automatically generate textures for large objects or landscapes, rather than requiring an artist to manually label each pixel.   合成和采样: 在这类任务中, 机器学习程序生成一些和训练数据相似的新样本. 通过机器学习, 合成和采样可能在媒体应用中非常有用, 可以避免艺术家大量昂贵或者乏味费时的手动工作. 例如, 视频游戏可以自动生成大型物体或风景的纹理, 而不是让艺术家手动标记每个像素.\n  Imputation of missing values: In this type of task, the machine learning algorithm is given a new example $x \\in \\mathbb{R}^{n}$, but with some entries $x_{i}$ of $x$ missing. The algorithm must provide a prediction of the values of the missing entries.   缺失值填补: 在这类任务中, 机器学习算法给定一个新样本 $x \\in \\mathbb{R}^{n}$, $x$ 中某些元素 $x_{i}$ 缺失.算法必须填补这些缺失值.\n  Denoising: In this type of task, the machine learning algorithm is given in input a corrupted example $\\tilde{x} \\in \\mathbb{R}^{n}$ obtained by an unknown corruption process from a clean example $x\\in \\mathbb{R}^{n}$. The learner must predict the clean example $x$ from its corrupted version $\\tilde{x}$, or more generally predict the conditional probability distribution $p(x \\ | \\ \\tilde{x})$.   去噪: 在这类任务中, 机器学习算法的输入是, 干净样本 $x\\in \\mathbb{R}^{n}$ 经过未知损坏过程后得到的损坏样本 $\\tilde{x} \\in \\mathbb{R}^{n}$.算法根据损坏后的样本 $\\tilde{x}$ 预测干净的样本 $x$, 或者更一般地预测条件概率分布 $p(x \\ | \\ \\tilde{x})$.\n Of course, many other tasks and types of tasks are possible. The types of tasks we list here are intended only to provide examples of what machine learning can do, not to define a rigid taxonomy of tasks.\nThe Performance Measure, $P$ In order to evaluate the abilities of a machine learning algorithm, we must design a quantitative measure of its performance. Usually this performance measure $P$ is specific to the task $T$ being carried out by the system.\nFor tasks such as classification, classification with missing inputs, and transcription, we often measure the accuracy of the model. Accuracy is just the proportion of examples for which the model produces the correct output. We can also obtain equivalent information by measuring the error rate, the proportion of examples for which the model produces an incorrect output. We often refer to the error rate as the expected 0-1 loss. The 0-1 loss on a particular example is 0 if it is correctly classified and 1 if it is not. For tasks such as density estimation, it does not make sense to measure accuracy, error rate, or any other kind of 0-1 loss. Instead, we must use a different performance metric that gives the model a continuous-valued score for each example. The most common approach is to report the average log-probability the model assigns to some examples.\n 对于诸如分类、缺失输入分类和转录任务, 我们通常度量模型的 准确率(accuracy).准确率是指该模型输出正确结果的样本比率. 我们也可以通过 错误率(error rate) 得到相同的信息. 错误率是指该模型输出错误结果的样本比率.\n Usually we are interested in how well the machine learning algorithm performs on data that it has not seen before, since this determines how well it will work when deployed in the real world. We therefore evaluate these performance measures using a test set of data that is separate from the data used for training the machine learning system.\nThe choice of performance measure may seem straightforward and objective, but it is often difficult to choose a performance measure that corresponds well to the desired behavior of the system.\nThe Experience, $E$ Machine learning algorithms can be broadly categorized as unsupervised or supervised by what kind of experience they are allowed to have during the learning process.\nMost of the learning algorithms can be understood as being allowed to experience an entire dataset. A dataset is a collection of many examples. Sometimes we will also call examples data points.\nUnsupervised learning algorithms experience a dataset containing many features, then learn useful properties of the structure of this dataset. In the context of deep learning, we usually want to learn the entire probability distribution that generated a dataset, whether explicitly as in density estimation or implicitly for tasks like synthesis or denoising. Some other unsupervised learning algorithms perform other roles, like clustering, which consists of dividing the dataset into clusters of similar examples.\n 无监督学习算法(unsupervised learning algorithm) 中, 数据没有标签.无监督学习使我们能够在几乎不知道结果应该是什么样子的情况下解决问题. 我们可以从数据中推导出结构, 而我们不一定知道变量的影响. 我们可以通过基于数据中变量之间的关系对数据进行聚类来推导出这种结构. 对于无监督学习, 没有基于预测结果的反馈.\n Supervised learning algorithms experience a dataset containing features, but each example is also associated with a label or target.\n 监督学习算法(supervised learning algorithm) 中, 我们得到了一个数据集, 并且已经知道我们的正确输出应该是什么样子, 并且知道输入和输出之间存在关系. 监督学习问题分为“回归”和“分类”问题. 在回归问题中, 我们试图预测连续输出中的结果, 这意味着我们试图将输入变量映射到某个连续函数.在分类问题中, 我们试图在离散输出中预测结果. 换句话说, 我们试图输入变量映射到离散类别中.\n Unsupervised learning and supervised learning are not formally defined terms. The lines between them are often blurred. Many machine learning technologies can be used to perform both tasks.\nThough unsupervised learning and supervised learning are not completely formal or distinct concepts, they do help to roughly categorize some of the things we do with machine learning algorithms. Traditionally, people refer to regression, classification and structured output problems as supervised learning. Density estimation in support of other tasks is usually considered unsupervised learning.\nOther variants of the learning paradigm are possible. For example, in semi-supervised learning, some examples include a supervision target but others do not. In multi-instance learning, an entire collection of examples is labeled as containing or not containing an example of a class, but the individual members of the collection are not labeled.\n 半监督学习(semi-supervised learning) 中, 一些样本有监督目标, 但其他样本没有.\n Some machine learning algorithms do not just experience a fixed dataset. For example, reinforcement learning algorithms interact with an environment, so there is a feedback loop between the learning system and its experiences.\n 强化算法(reinforcement learning algorithms) 通过反复试验来实现明确的目标. 它尝试了许多不同的事情, 并根据其行为是帮助还是阻碍其实现目标而受到奖励或惩罚. 这就像在教狗新把戏时给予和扣留零食一样. 强化学习是谷歌 AlphaGo 的基础.\n Most machine learning algorithms simply experience a dataset. A dataset can be described in many ways. In all cases, a dataset is a collection of examples, which are in turn collections of features.\nCapacity, Overfitting and Underfitting The central challenge in machine learning is that we must perform well on new, previously unseen inputs—not just those on which our model was trained. The ability to perform well on previously unobserved inputs is called generalization.\n 机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好, 而不只是在训练集上表现良好.在先前未观测到的输入上表现良好的能力被称为 泛化(generalization) .\n Typically, when training a machine learning model, we have access to a training set, we can compute some error measure on the training set called the training error, and we reduce this training error. So far, what we have described is simply an optimization problem. What separates machine learning from optimization is that we want the generalization error, also called the test error, to be low as well. The generalization error is defined as the expected value of the error on a new input. Here the expectation is taken across different possible inputs, drawn from the distribution of inputs we expect the system to encounter in practice.\n 通常情况下, 当我们训练机器学习模型时, 我们可以使用某个训练集, 在训练集上计算一些被称为 训练误差(training error) 的度量误差, 目标是降低训练误差.机器学习和优化不同的地方在于, 我们也希望 泛化误差(generalization error) (也被称为 测试误差(test error) )很低.\n We typically estimate the generalization error of a machine learning model by measuring its performance on a test set of examples that were collected separately from the training set.\nThe train and test data are generated by a probability distribution over datasets called the data generating process. We typically make a set of assumptions known collectively as the i.i.d. assumptions. These assumptions are that the examples in each dataset are independent from each other, and that the train set and test set are identically distributed, drawn from the same probability distribution as each other. This assumption allows us to describe the data generating process with a probability distribution over a single example. The same distribution is then used to generate every train example and every test example. We call that shared underlying distribution the data generating distribution, denoted $p_{data}$. This probabilistic framework and the i.i.d. assumptions allow us to mathematically study the relationship between training error and test error.\nOne immediate connection we can observe between the training and test error is that the expected training error of a randomly selected model is equal to the expected test error of that model. Suppose we have a probability distribution $p(x,y)$ and we sample from it repeatedly to generate the train set and the test set. For some fixed value $w$, the expected training set error is exactly the same as the expected test set error, because both expectations are formed using the same dataset sampling process. The only difference between the two conditions is the name we assign to the dataset we sample.\nOf course, when we use a machine learning algorithm, we do not fix the parameters ahead of time, then sample both datasets. We sample the training set, then use it to choose the parameters to reduce training set error, then sample the test set. Under this process, the expected test error is greater than or equal to the expected value of training error. The factors determining how well a machine learning algorithm will perform are its ability to:\n Make the training error small. Make the gap between training and test error small.  These two factors correspond to the two central challenges in machine learning: underfitting and overfitting. Underfitting occurs when the model is not able to obtain a sufficiently low error value on the training set. Overfitting occurs when the gap between the training error and test error is too large.\n 欠拟合(underfitting) 是指模型不能在训练集上获得足够低的误差. 而 过拟合 (overfitting) 是指训练误差和和测试误差之间的差距太大.\n  We can control whether a model is more likely to overfit or underfit by altering its capacity. Informally, a model’s capacity is its ability to fit a wide variety of functions. Models with low capacity may struggle to fit the training set. Models with high capacity can overfit by memorizing properties of the training set that do not serve them well on the test set.\nOne way to control the capacity of a learning algorithm is by choosing its hypothesis space, the set of functions that the learning algorithm is allowed to select as being the solution. For example, the linear regression algorithm has the set of all linear functions of its input as its hypothesis space. We can generalize linear regression to include polynomials, rather than just linear functions, in its hypothesis space. Doing so increases the model’s capacity.\nWe must remember that while simpler functions are more likely to generalize (to have a small gap between training and test error) we must still choose a sufficiently complex hypothesis to achieve low training error. Typically, training error decreases until it asymptotes to the minimum possible error value as model capacity increases (assuming the error measure has a minimum value). Typically, generalization error has a U-shaped curve as a function of model capacity.\n The No Free Lunch Theorem Learning theory claims that a machine learning algorithm can generalize well from a finite training set of examples. This seems to contradict some basic principles of logic. Inductive reasoning, or inferring general rules from a limited set of examples, is not logically valid. To logically infer a rule describing every member of a set, one must have information about every member of that set.\nIn part, machine learning avoids this problem by offering only probabilistic rules, rather than the entirely certain rules used in purely logical reasoning. Machine learning promises to find rules that are probably correct about most members of the set they concern.\nUnfortunately, even this does not resolve the entire problem. The no free lunch theorem for machine learning (Wolpert, 1996) states that, averaged over all possible data generating distributions, every classification algorithm has the same error rate when classifying previously unobserved points. In other words, in some sense, no machine learning algorithm is universally any better than any other. The most sophisticated algorithm we can conceive of has the same average performance (over all possible tasks) as merely predicting that every point belongs to the same class.\n 机器学习的 没有 免费午餐定理(no free lunch theorem) 表明 (Wolpert, 1996), 在所有可能的数据生成分布上平均之后, 每一个分类算法在未事先观测的点上都有相同的错误率. 换言之, 在某种意义上, 没有一个机器学习算法总是比其他的要好.\n Fortunately, these results hold only when we average over all possible data generating distributions. If we make assumptions about the kinds of probability distributions we encounter in real-world applications, then we can design learning algorithms that perform well on these distributions.\nThis means that the goal of machine learning research is not to seek a universal learning algorithm or the absolute best learning algorithm. Instead, our goal is to understand what kinds of distributions are relevant to the \u0026ldquo;real world\u0026rdquo; that an AI agent experiences, and what kinds of machine learning algorithms perform well on data drawn from the kinds of data generating distributions we care about.\nRegularization The no free lunch theorem implies that we must design our machine learning algorithms to perform well on a specific task. We do so by building a set of preferences into the learning algorithm. When these preferences are aligned with the learning problems we ask the algorithm to solve, it performs better.\nWe can regularize a model that learns a function f(x; θ) by adding a penalty called a regularizer to the cost function.\nExpressing preferences for one function over another is a more general way of controlling a model’s capacity than including or excluding members from the hypothesis space. We can think of excluding a function from a hypothesis space as expressing an infinitely strong preference against that function.\nIn weight decay example, we expressed our preference for linear functions defined with smaller weights explicitly, via an extra term in the criterion we minimize. There are many other ways of expressing preferences for different solutions, both implicitly and explicitly. Together, these different approaches are known as regularization. Regularization is any modification we make to a learning algorithm that is intended to reduce its generalization error but not its training error. Regularization is one of the central concerns of the field of machine learning, rivaled in its importance only by optimization.\n 正则化(regularization) 是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。\n The no free lunch theorem has made it clear that there is no best machine learning algorithm, and, in particular, no best form of regularization. Instead we must choose a form of regularization that is well-suited to the particular task we want to solve.\nHyperparameters and Validation Sets Most machine learning algorithms have several settings that we can use to control the behavior of the learning algorithm. These settings are called hyperparame- ters. The values of hyperparameters are not adapted by the learning algorithm itself (though we can design a nested learning procedure where one learning algorithm learns the best hyperparameters for another learning algorithm).\nSometimes a setting is chosen to be a hyperparameter that the learning algorithm does not learn because it is difficult to optimize. More frequently, the setting must be a hyperparameter because it is not appropriate to learn that hyperparameter on the training set. This applies to all hyperparameters that control model capacity. If learned on the training set, such hyperparameters would always choose the maximum possible model capacity, resulting in overfitting. For example, we can always fit the training set better with a higher degree polynomial and a weight decay setting of $\\lambda = 0$ than we could with a lower degree polynomial and a positive weight decay setting.\nTo solve this problem, we need a validation set of examples that the training algorithm does not observe.\nEarlier we discussed how a held-out test set, composed of examples coming from the same distribution as the training set, can be used to estimate the generalization error of a learner, after the learning process has completed. It is important that the test examples are not used in any way to make choices about the model, including its hyperparameters. For this reason, no example from the test set can be used in the validation set. Therefore, we always construct the validation set from the training data. Specifically, we split the training data into two disjoint subsets. One of these subsets is used to learn the parameters. The other subset is our validation set, used to estimate the generalization error during or after training, allowing for the hyperparameters to be updated accordingly. The subset of data used to learn the parameters is still typically called the training set, even though this may be confused with the larger pool of data used for the entire training process. The subset of data used to guide the selection of hyperparameters is called the validation set. Typically, one uses about 80% of the training data for training and 20% for validation. Since the validation set is used to \u0026ldquo;train\u0026rdquo; the hyperparameters, the validation set error will underestimate the generalization error, though typically by a smaller amount than the training error. After all hyperparameter optimization is complete, the generalization error may be estimated using the test set.\nCross-Validation Dividing the dataset into a fixed training set and a fixed test set can be problematic if it results in the test set being small. A small test set implies statistical uncertainty around the estimated average test error, making it difficult to claim that algorithm $A$ works better than algorithm $B$ on the given task.\nWhen the dataset has hundreds of thousands of examples or more, this is not a serious issue. When the dataset is too small, are alternative procedures enable one to use all of the examples in the estimation of the mean test error, at the price of increased computational cost. These procedures are based on the idea of repeating the training and testing computation on different randomly chosen subsets or splits of the original dataset. The most common of these is the $k$-fold cross-validation procedure, in which a partition of the dataset is formed by splitting it into $k$ non-overlapping subsets. The test error may then be estimated by taking the average test error across $k$ trials. On trial $i$, the $i$-th subset of the data is used as the test set and the rest of the data is used as the training set.\nEstimators, Bias and Variance The field of statistics gives us many tools that can be used to achieve the machine learning goal of solving a task not only on the training set but also to generalize. Foundational concepts such as parameter estimation, bias and variance are useful to formally characterize notions of generalization, underfitting and overfitting.\nPoint Estimation Point estimation is the attempt to provide the single “best” prediction of some quantity of interest. In general the quantity of interest can be a single parameter or a vector of parameters in some parametric model.\nIn order to distinguish estimates of parameters from their true value, our convention will be to denote a point estimate of a parameter $\\theta$ by $\\hat{\\theta}$.\nLet $\\{x^{(1)},\\cdots,x^{(m)}\\}$ be a set of $m$ independent and identically distributed (i.i.d.) data points. A point estimator or statistic is any function of the data:\n$$ \\hat{\\theta}_{m}=g(x^{(1)},\\cdots,x^{(m)}) \\\\ $$\nThe definition does not require that $g$ return a value that is close to the true $\\theta$ or even that the range of $g$ is the same as the set of allowable values of $\\theta$. This definition of a point estimator is very general and allows the designer of an estimator great flexibility. While almost any function thus qualifies as an estimator, a good estimator is a function whose output is close to the true underlying $\\theta$ that generated the training data.\nFor now, we take the frequentist perspective on statistics. That is, we assume that the true parameter value $\\theta$ is fixed but unknown, while the point estimate $\\hat{\\theta}$ is a function of the data. Since the data is drawn from a random process, any function of the data is random. Therefore $\\hat{\\theta}$ is a random variable.\nPoint estimation can also refer to the estimation of the relationship between input and target variables. We refer to these types of point estimates as function estimators.\nFunction Estimation: As we mentioned above, sometimes we are interested in performing function estimation (or function approximation). Here we are trying to predict a variable $y$ given an input vector $x$. We assume that there is a function $f(x)$ that describes the approximate relationship between $y$ and $x$. For example, we may assume that $y=f(x)+\\epsilon$, where $\\epsilon$ stands for the part of $y$ that is not predictable from $x$. In function estimation, we are interested in approximating $f$ with a model or estimate $\\hat{f}$. Function estimation is really just the same as estimating a parameter $\\theta$; the function estimator $\\hat{f}$ is simply a point estimator in function space.\n 函数估计(Function Estimation): 有时我们会关注函数估计(或函数近似)。这时我们试图从输入向量 $x$ 预测变量 $y$。我们假设有一个函数 $f(x)$ 表示 $y$ 和 $x$ 之间的近似关系。\n Bias The bias of an estimator is defined as:\n$$ bias(\\hat{\\theta}_{m})=E(\\hat{\\theta}_{m})-\\theta \\\\ $$\nwhere the expectation is over the data (seen as samples from a random variable) and $\\theta$ is the true underlying value of $\\theta$ used to define the data generating distribution. An estimator $\\hat{\\theta}_{m}$ is said to be unbiased if $bias(\\hat{\\theta}_{m})=0$, which implies that $E[\\hat{\\theta}_{m}]=\\theta$. An estimator $\\hat{\\theta}_{m}$ is said to be asymptotically unbiased if $\\lim_{m\\to\\infty}bias(\\hat{\\theta}_{m})=0$, which implies that $\\lim_{m\\to\\infty}E[\\hat{\\theta}_{m}]=\\theta$.\n 如果 $bias(\\hat{\\theta}_{m})=0$，那么估计量 $\\hat{\\theta}_{m}$ 被称为是 无偏 (unbiased) ，这意味着 $E[\\hat{\\theta}_{m}]=\\theta$。如果 $\\lim_{m\\to\\infty}bias(\\hat{\\theta}_{m})=0$，那么估计量 $\\hat{\\theta}_{m}$ 被称为是 渐近无偏(asymptotically unbiased) ，这意味着 $\\lim_{m\\to\\infty}E[\\hat{\\theta}_{m}]=\\theta$。\n Variance and Standard Error Another property of the estimator that we might want to consider is how much we expect it to vary as a function of the data sample. Just as we computed the expectation of the estimator to determine its bias, we can compute its variance. The variance of an estimator is simply the variance\n$$ Var(\\hat{\\theta}) \\\\ $$\nwhere the random variable is the training set. Alternately, the square root of the variance is called the standard error, denoted $SE(\\hat{\\theta})$.\nThe variance or the standard error of an estimator provides a measure of how we would expect the estimate we compute from data to vary as we independently resample the dataset from the underlying data generating process. Just as we might like an estimator to exhibit low bias we would also like it to have relatively low variance.\n 估计量的 方差(variance) 或 标准差(standard error) 告诉我们，当独立地从潜在的数据生成过程中重采样数据集时，如何期望估计的变化。正如我们希望估计的偏差较小，我们也希望其方差较小。\n When we compute any statistic using a finite number of samples, our estimate of the true underlying parameter is uncertain, in the sense that we could have obtained other samples from the same distribution and their statistics would have been different. The expected degree of variation in any estimator is a source of error that we want to quantify.\nThe standard error of the mean is given by\n$$ SE(\\hat{\\mu}_{m})=\\sqrt{Var \\left[\\frac{1}{m}\\sum_{i=1}^{m}x^{(i)}\\right]} = \\frac{\\sigma}{\\sqrt{m}} \\\\ $$\nwhere $\\sigma^{2}$ is the true variance of the samples $x^{i}$. The standard error is often estimated by using an estimate of $\\sigma$. Unfortunately, neither the square root of the sample variance nor the square root of the unbiased estimator of the variance provide an unbiased estimate of the standard deviation. Both approaches tend to underestimate the true standard deviation, but are still used in practice. The square root of the unbiased estimator of the variance is less of an underestimate. For large $m$, the approximation is quite reasonable.\nThe standard error of the mean is very useful in machine learning experiments. We often estimate the generalization error by computing the sample mean of the error on the test set. The number of examples in the test set determines the accuracy of this estimate. Taking advantage of the central limit theorem, which tells us that the mean will be approximately distributed with a normal distribution, we can use the standard error to compute the probability that the true expectation falls in any chosen interval. For example, the 95% confidence interval centered on the mean $\\hat{\\mu}_{m}$ is\n$$ (\\hat{\\mu}_{m}-1.96SE(\\hat{\\mu}_{m}),\\hat{\\mu}_{m}+1.96SE(\\hat{\\mu}_{m})) \\\\ $$\nunder the normal distribution with mean $\\mu_{m}$ and variance $SE(\\hat{\\mu}_{m})^{2}$. In machine learning experiments, it is common to say that algorithm $A$ is better than algorithm $B$ if the upper bound of the 95% confidence interval for the error of algorithm $A$ is less than the lower bound of the 95% confidence interval for the error of algorithm $B$.\nTrading off Bias and Variance to Minimize Mean Squared Error Bias and variance measure two different sources of error in an estimator. Bias measures the expected deviation from the true value of the function or parameter. Variance on the other hand, provides a measure of the deviation from the expected estimator value that any particular sampling of the data is likely to cause.\nWhat happens when we are given a choice between two estimators, one with more bias and one with more variance? How do we choose between them?\nThe most common way to negotiate this trade-off is to use cross-validation. Empirically, cross-validation is highly successful on many real-world tasks. Alternatively, we can also compare the mean squared error (MSE) of the estimates:\n$$ \\begin{align*} MSE \u0026amp;= E[(\\hat{\\theta}_{m}-\\theta)^{2}] \\\\ \u0026amp;= Bias(\\hat{\\theta}_{m})^{2} + Var(\\hat{\\theta}_{m}) \\\\ \\end{align*} $$\nThe MSE measures the overall expected deviation—in a squared error sense— between the estimator and the true value of the parameter $\\theta$.\nThe relationship between bias and variance is tightly linked to the machine learning concepts of capacity, underfitting and overfitting. In the case where generalization error is measured by the MSE (where bias and variance are meaningful components of generalization error), increasing capacity tends to increase variance and decrease bias.\n Consistency So far we have discussed the properties of various estimators for a training set of fixed size. Usually, we are also concerned with the behavior of an estimator as the amount of training data grows. In particular, we usually wish that, as the number of data points m in our dataset increases, our point estimates converge to the true value of the corresponding parameters. More formally, we would like that\n$$ \\mathrm{plim}_{m\\to\\infty}\\hat{\\theta}_{m}=\\theta \\\\ $$\nThe symbol $\\mathrm{plim}$ indicates convergence in probability, meaning that for any $\\epsilon \u0026gt;0$, $P(|\\hat{\\theta}_{m}-\\theta|\u0026gt; \\epsilon)\\to 0$ as $m \\to \\infty$. The condition described by equation $\\mathrm{plim}_{m\\to\\infty}\\hat{\\theta}_{m}=\\theta$ is known as consistency. It is sometimes referred to as weak consistency, with strong consistency referring to the almost sure convergence of $\\hat{\\theta}$ to $\\theta$. Almost sure convergence of a sequence of random variables $x^{(1)},x^{(2)},\\cdots$ to a value $x$ occurs when $p(\\lim_{m\\to\\infty} x^{(m)} = x) = 1$.\nConsistency ensures that the bias induced by the estimator diminishes as the number of data examples grows. However, the reverse is not true—asymptotic unbiasedness does not imply consistency.\nMaximum Likelihood Estimation We have seen some definitions of common estimators and analyzed their properties. But where did these estimators come from? Rather than guessing that some function might make a good estimator and then analyzing its bias and variance, we would like to have some principle from which we can derive specific functions that are good estimators for different models.\nThe most common such principle is the maximum likelihood principle.\nConsider a set of m examples $\\mathbb{X}=x^{(1)},\\cdots,x^{(m)}$ drawn independently from the true but unknown data generating distribution $p_{data}(x)$.\nLet $p_{model}(x;\\theta)$ be a parametric family of probability distributions over the same space indexed by $\\theta$. In other words, $p_{model}(x;\\theta)$ maps any configuration $x$ to a real number estimating the true probability $p_{data}(x)$.\nThe maximum likelihood estimator for $\\theta$ is then defined as\n$$ \\begin{align*} \\theta_{ML}\u0026amp;=\\arg\\max_{\\theta}p_{model}(\\mathbb{X};\\theta) \\\\ \u0026amp;= \\arg\\max_{\\theta}\\prod_{i=1}^{m}p_{model}(x^{(i)};\\theta) \\\\ \\end{align*} $$\nThis product over many probabilities can be inconvenient for a variety of reasons. For example, it is prone to numerical underflow. To obtain a more convenient but equivalent optimization problem, we observe that taking the logarithm of the likelihood does not change its arg max but does conveniently transform a product into a sum:\n$$ \\theta_{ML}=\\arg\\max_{\\theta}\\sum_{i=1}^{m}\\log p_{model}(x^{(i)};\\theta) \\\\ $$\nBecause the $\\arg\\max$ does not change when we rescale the cost function, we can divide by $m$ to obtain a version of the criterion that is expressed as an expectation with respect to the empirical distribution $\\hat{p}_{data}$ defined by the training data:\n$$ \\theta_{ML}=\\arg\\max_{\\theta}\\mathbb{E}_{x\\sim \\hat{p}_{data}}\\log p_{model}(x;\\theta) \\\\ $$\nConditional Log-Likelihood and Mean Squared Error The maximum likelihood estimator can readily be generalized to the case where our goal is to estimate a conditional probability $P(y|x;\\theta)$ in order to predict $y$ given $x$. This is actually the most common situation because it forms the basis for most supervised learning. If $X$ represents all our inputs and $Y$ all our observed targets, then the conditional maximum likelihood estimator is\n$$ \\theta_{ML}=\\arg\\max_{\\theta}P(Y|X;\\theta) \\\\ $$\nIf the examples are assumed to be i.i.d., then this can be decomposed into\n$$ \\theta_{ML}=\\arg\\max_{\\theta}\\sum_{i=1}^{m}\\log p_{model}(y^{(i)}|x^{(i)};\\theta) \\\\ $$\nProperties of Maximum Likelihood The main appeal of the maximum likelihood estimator is that it can be shown to be the best estimator asymptotically, as the number of examples $m\\to\\infty$, in terms of its rate of convergence as $m$ increases.\nUnder appropriate conditions, the maximum likelihood estimator has the property of consistency, meaning that as the number of training examples approaches infinity, the maximum likelihood estimate of a parameter converges to the true value of the parameter. These conditions are:\n The true distribution $p_{data}$ must lie within the model family $p_{model}(\\cdot;\\theta)$. Otherwise, no estimator can recover $p_{data}$ . The true distribution $p_{data}$ must correspond to exactly one value of $\\theta$. Other- wise, maximum likelihood can recover the correct $p_{data}$ , but will not be able to determine which value of $\\theta$ was used by the data generating processing.  Bayesian Statistics We have discussed frequentist statistics and approaches based on estimating a single value of $\\theta$, then making all predictions thereafter based on that one estimate. Another approach is to consider all possible values of $\\theta$ when making a prediction. The latter is the domain of Bayesian statistics.\nThe frequentist perspective is that the true parameter value $\\theta$ is fixed but unknown, while the point estimate $\\hat{\\theta}$ is a random variable on account of it being a function of the dataset (which is seen as random).\nThe Bayesian perspective on statistics is quite different. The Bayesian uses probability to reflect degrees of certainty of states of knowledge. The dataset is directly observed and so is not random. On the other hand, the true parameter $\\theta$ is unknown or uncertain and thus is represented as a random variable.\n 频率派(frequentist statistics) 的视角是真实参数 $\\theta$ 是未知的定值，而点估计 $\\hat{\\theta}$ 是考虑数据集上函数(可以看作是随机的)的随机变量。 贝叶斯统计(Bayesian statistics) 的视角完全不同。贝叶斯用概率反映知识状态的确定性程度。数据 集能够被直接观测到，因此不是随机的。另一方面，真实参数 $\\theta$ 是未知或不确定的， 因此可以表示成随机变量。\n Before observing the data, we represent our knowledge of $\\theta$ using the prior probability distribution, $p(\\theta)$ (sometimes referred to as simply “the prior”). Generally, the machine learning practitioner selects a prior distribution that is quite broad (i.e. with high entropy) to reflect a high degree of uncertainty in the value of $\\theta$ before observing any data. For example, one might assume a priori that $\\theta$ lies in some finite range or volume, with a uniform distribution. Many priors instead reflect a preference for “simpler” solutions (such as smaller magnitude coefficients, or a function that is closer to being constant).\nNow consider that we have a set of data samples $\\{x^{(1)},\\cdots,x^{(m)}\\}$. We can recover the effect of data on our belief about $\\theta$ by combining the data likelihood $p(x^{(1)},\\cdots,x^{(m)}|\\theta)$ with the prior via Bayes’ rule:\n$$ p(\\theta|x^{(1)},\\cdots,x^{(m)})=\\frac{p(x^{(1)},\\cdots,x^{(m)}|\\theta)p(\\theta)}{p(x^{(1)},\\cdots,x^{(m)})} \\\\ $$\nIn the scenarios where Bayesian estimation is typically used, the prior begins as a relatively uniform or Gaussian distribution with high entropy, and the observation of the data usually causes the posterior to lose entropy and concentrate around a few highly likely values of the parameters.\nRelative to maximum likelihood estimation, Bayesian estimation offers two important differences. First, unlike the maximum likelihood approach that makes predictions using a point estimate of $\\theta$, the Bayesian approach is to make predictions using a full distribution over $\\theta$. For example, after observing $m$ examples, the predicted distribution over the next data sample, $x^{(m+1)}$ , is given by\n$$ p(x^{(m+1)}|x^{(1)},\\cdots,x^{(m)})=\\int p(x^{(m+1)}|\\ \\theta)p(\\theta \\ |x^{(1)},\\cdots,x^{(m)})d\\theta \\\\ $$\nHere each value of $\\theta$ with positive probability density contributes to the prediction of the next example, with the contribution weighted by the posterior density itself. After having observed $\\{x^{(1)},\\cdots,x^{(m)}\\}$, if we are still quite uncertain about the value of $\\theta$, then this uncertainty is incorporated directly into any predictions we might make.\nThe second important difference between the Bayesian approach to estimation and the maximum likelihood approach is due to the contribution of the Bayesian prior distribution. The prior has an influence by shifting probability mass density towards regions of the parameter space that are preferred a priori. In practice, the prior often expresses a preference for models that are simpler or more smooth. Critics of the Bayesian approach identify the prior as a source of subjective human judgment impacting the predictions.\nBayesian methods typically generalize much better when limited training data is available, but typically suffer from high computational cost when the number of training examples is large.\nReference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/ml.html.\n","permalink":"https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/04_machine_learning_basics_for_ml/","summary":"Learning Algorithms A machine learning algorithm is an algorithm that is able to learn from data. But what do we mean by learning? Mitchell (1997) provides the definition \u0026ldquo;A computer program is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.\u0026rdquo;\nThe Task, $T$ Machine learning allows us to tackle tasks that are too difficult to solve with fixed programs written and designed by human beings.","title":"[ML Basics] Machine Learning Basics"},{"content":"Overflow and Underflow The fundamental difficulty in performing continuous math on a digital computer is that we need to represent infinitely many real numbers with a finite number of bit patterns. This means that for almost all real numbers, we incur some approximation error when we represent the number in the computer. In many cases, this is just rounding error. Rounding error is problematic, especially when it compounds across many operations, and can cause algorithms that work in theory to fail in practice if they are not designed to minimize the accumulation of rounding error.\nOne form of rounding error that is particularly devastating is underflow. Underflow occurs when numbers near zero are rounded to zero. Many functions behave qualitatively differently when their argument is zero rather than a small positive number. For example, we usually want to avoid division by zero (some software environments will raise exceptions when this occurs, others will return a result with a placeholder not-a-number value) or taking the logarithm of zero (this is usually treated as $-\\infty$, which then becomes not-a-number if it is used for many further arithmetic operations).\nAnother highly damaging form of numerical error is overflow. Overflow occurs when numbers with large magnitude are approximated as $\\infty$ or $-\\infty$. Further arithmetic will usually change these infinite values into not-a-number values.\n 一种极具毁灭性的舍入误差是 下溢(underflow). 当接近零的数被四舍五入为零时发生下溢. 许多函数在其参数为零而不是一个很小的正数时才会表现出质的不同.例如, 我们通常要避免被零除(一些软件环境将在这种情况下抛出异常, 有些会返回一个非数字 (not-a-number, NaN) 的占位符)或避免取零的对数(这通常被 视为 $-\\infty$, 进一步的算术运算会使其变成非数字).\n另一个极具破坏力的数值错误形式是 上溢(overflow). 当大量级的数被近似为 $\\infty$ 或 $-\\infty$ 时发生上溢. 进一步的运算通常会导致这些无限值变为非数字.\n Poor Conditioning Conditioning refers to how rapidly a function changes with respect to small changes in its inputs. Functions that change rapidly when their inputs are perturbed slightly can be problematic for scientific computation because rounding errors in the inputs can result in large changes in the output.\nConsider the function $f(x) = A^{−1}x$. When $A \\in \\mathbb{R}^{n \\times n}$ has an eigenvalue decomposition, its condition number is\n$$ \\max_{i,j}|\\frac{\\lambda_{i}}{\\lambda_{j}}| $$\nThis is the ratio of the magnitude of the largest and smallest eigenvalue. When this number is large, matrix inversion is particularly sensitive to error in the input.\nThis sensitivity is an intrinsic property of the matrix itself, not the result of rounding error during matrix inversion. Poorly conditioned matrices amplify pre-existing errors when we multiply by the true matrix inverse. In practice, the error will be compounded further by numerical errors in the inversion process itself.\nGradient-Based Optimization Most deep learning algorithms involve optimization of some sort. Optimization refers to the task of either minimizing or maximizing some function $f(x)$ by altering $x$. We usually phrase most optimization problems in terms of minimizing $f(x)$. Maximization may be accomplished via a minimization algorithm by minimizing $-f(x)$.\nThe function we want to minimize or maximize is called the objective function or criterion. When we are minimizing it, we may also call it the cost function, loss function, or error function.\n 大多数深度学习算法都涉及某种形式的优化. 优化指的是改变 $x$ 以最小化或最大化某个函数 $f(x)$ 的任务. 我们通常以最小化 $f(x)$ 指代大多数最优化问题. 我们把要最小化或最大化的函数称为 目标函数(objective function) 或准则 (criterion).当我们对其进行最小化时,我们也把它称为 代价函数(cost function)、损失函数(loss function) 或 误差函数(error function).\n We often denote the value that minimizes or maximizes a function with a superscript $\\ast$ . For example, we might say $x^{*}=\\arg \\min f(x)$.\nSuppose we have a function $y = f(x)$, where both $x$ and $y$ are real numbers. The derivative of this function is denoted as $f'(x)$ or as $\\frac{dy}{dx}$ . The derivative $f'(x)$ gives the slope of $f(x)$ at the point $x$. In other words, it specifies how to scale a small change in the input in order to obtain the corresponding change in the output: $f(x+\\epsilon) \\approx f(x) \\ +$ $ \\epsilon f'(x)$.\nThe derivative is therefore useful for minimizing a function because it tells us how to change $x$ in order to make a small improvement in $y$. For example, we know that $f(x-\\epsilon \\mathrm{sign}(f'(x)))$ is less than $f(x)$ for small enough $\\epsilon$. We can thus reduce $f(x)$ by moving $x$ in small steps with opposite sign of the derivative. This technique is called gradient descent.\n When $f'(x)=0$, the derivative provides no information about which direction to move. Points where $f'(x)=0$ are known as critical points or stationary points. A local minimum is a point where $f(x)$ is lower than at all neighboring points, so it is no longer possible to decrease $f(x)$ by making infinitesimal steps. A local maximum is a point where $f(x)$ is higher than at all neighboring points, so it is not possible to increase $f(x)$ by making infinitesimal steps. Some critical points are neither maxima nor minima. These are known as saddle points.\n 当 $f'(x)=0$,导数无法提供往哪个方向移动的信息.$f'(x)=0$ 的点称为 临界点(critical point) 或 驻点(stationary point).一个 局部极小点(local minimum) 意味着这个点的 $f(x)$ 小于所有邻近点,因此不可能通过移动无穷小的步长来减小 $f(x)$.一个 局部极大点(local maximum) 意味着这个点的 $f(x)$ 大于所有邻近点,因此不可能通过移动无穷小的步长来增大 $f(x)$.有些临界点既不是最小点也不是最大点.这些点被称为 鞍点(saddle point).\n  A point that obtains the absolute lowest value of $f(x)$ is a global minimum. It is possible for there to be only one global minimum or multiple global minima of the function. It is also possible for there to be local minima that are not globally optimal. In the context of deep learning, we optimize functions that may have many local minima that are not optimal, and many saddle points surrounded by very flat regions. All of this makes optimization very difficult, especially when the input to the function is multidimensional. We therefore usually settle for finding a value of f that is very low, but not necessarily minimal in any formal sense.\n 使 $f(x)$ 取得绝对的最小值(相对所有其他值)的点是 全局最小点(global minimum).函数可能只有一个全局最小点或存在多个全局最小点,还可能存在不是全局最优的局部极小点.\n  We often minimize functions that have multiple inputs: $f: \\mathbb{R}^{n} \\to \\mathbb{R}$. For the concept of “minimization” to make sense, there must still be only one (scalar) output.\nFor functions with multiple inputs, we must make use of the concept of partial derivatives. The partial derivative $\\frac{\\partial}{\\partial x_{i}}$ measures how $f$ changes as only the variable $x_{i}$ increases at point $x$. The gradient generalizes the notion of derivative to the case where the derivative is with respect to a vector: the gradient of $f$ is the vector containing all of the partial derivatives, denoted $\\nabla_{x} \\ f(x)$. Element $i$ of the gradient is the partial derivative of $f$ with respect to $x_{i}$. In multiple dimensions, critical points are points where every element of the gradient is equal to zero.\n 梯度(gradient) 是相对一个向量求导的导数: $f$ 的导数是包含所有偏导数的向量，记为 $\\nabla_{x} \\ f(x)$。在多维情况下，临界点是梯度中所有元素都为零的点。\n The directional derivative in direction $u$ (a unit vector) is the slope of the function $f$ in direction $u$. In other words, the directional derivative is the derivative of the function $f(x+\\alpha u)$ with respect to $\\alpha$, evaluated at $\\alpha=0$. Using the chain rule, we can see that $\\frac{\\partial}{\\partial \\alpha}f(x+\\alpha u)$ evaluates to $u^{T}\\nabla_{x} \\ f(x)$ when $\\alpha=0$.\nTo minimize $f$, we would like to find the direction in which $f$ decreases the fastest. We can do this using the directional derivative:\n$$ \\min_{u,u^{T}u=1} u^{T}\\nabla_{x} \\ f(x) \\\\ = \\min_{u,u^{T}u=1} \\lVert u \\rVert_{2} \\lVert \\nabla_{x} \\ f(x) \\rVert_{2} \\cos \\theta \\\\ $$\nwhere $\\theta$ is the angle between $u$ and the gradient. Substituting in $\\lVert u \\rVert_{2}=1$ and ignoring factors that do not depend on $u$, this simplifies to $\\min_{u} \\cos \\theta$. This is minimized when $u$ points in the opposite direction as the gradient. In other words, the gradient points directly uphill, and the negative gradient points directly downhill. We can decrease $f$ by moving in the direction of the negative gradient. This is known as the method of steepest descent or gradient descent.\nSteepest descent proposes a new point\n$$ x' = x - \\epsilon \\nabla_{x} \\ f(x) \\\\ $$\nwhere $\\epsilon$ is the learning rate, a positive scalar determining the size of the step. We can choose $\\epsilon$ in several different ways. A popular approach is to set $\\epsilon$ to a small constant. Sometimes, we can solve for the step size that makes the directional derivative vanish. Another approach is to evaluate $f(x - \\epsilon \\nabla_{x} \\ f(x))$ for several values of $\\epsilon$ and choose the one that results in the smallest objective function value. This last strategy is called a line search.\nSteepest descent converges when every element of the gradient is zero (or, in practice, very close to zero). In some cases, we may be able to avoid running this iterative algorithm, and just jump directly to the critical point by solving the equation $\\nabla_{x} \\ f(x)=0$ for $x$.\nAlthough gradient descent is limited to optimization in continuous spaces, the general concept of repeatedly making a small move (that is approximately the best small move) towards better configurations can be generalized to discrete spaces. Ascending an objective function of discrete parameters is called hill climbing.\nJacobian Hessian Matrices / Newton\u0026rsquo;s Methods Sometimes we need to find all of the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a Jacobian matrix. Specifically, if we have a function $f: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$, then the Jacobian matrix $J \\in \\mathbb{R}^{n \\times m}$ of $f$ is defined such that $J_{i,j}=\\frac{\\partial}{\\partial x_{j}}f(x)_{i}$.\n$$ J = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial x_{n}} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\nabla^{T}f_{1} \\\\ \\vdots \\\\ \\nabla^{T}f_{m} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_{1}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{m}}{\\partial x_{n}} \\\\ \\end{bmatrix} \\\\ $$\n 有时我们需要计算输入和输出都为向量的函数的所有偏导数.包含所有这样的偏导数的矩阵被称为 Jacobian 矩阵.\n We are also sometimes interested in a derivative of a derivative. This is known as a second derivative. For example, for a function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$, the derivative with respect to $x_{i}$ of the derivative of $f$ with respect to $x_{j}$ is denoted as $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f$. In a single dimension, we can denote $\\frac{d^{2}}{dx^{2}}f$ by $f''(x)$. The second derivative tells us how the first derivative will change as we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring curvature.\nWhen our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the Hessian matrix. The Hessian matrix $H(f)(x)$ is defined such that $H(f)(x)_{i,j} = \\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(x)$\n$$ H(f)(x)_{i,j} = \\begin{bmatrix} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}} \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{n}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{1}} \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial^{2} f}{\\partial x_{n}\\partial x_{1}} \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{n}\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{n}^{2}} \\\\ \\end{bmatrix} \\\\ $$\nEquivalently, the Hessian is the Jacobian of the gradient.\n 当我们的函数具有多维输入时,二阶导数也有很多.我们可以将这些导数合并成一个矩阵,称为 Hessian 矩阵.\n Anywhere that the second partial derivatives are continuous, the differential operators are commutative, i.e. their order can be swapped:\n$$ \\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(x)=\\frac{\\partial^{2}}{\\partial x_{j}\\partial x_{i}}f(x) \\\\ $$\nThis implies that $H_{i,j}=h_{j,i}$, so the Hessian matrix is symmetric at such points. Most of the functions we encounter in the context of deep learning have a symmetric Hessian almost everywhere. Because the Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors. The second derivative in a specific direction represented by a unit vector $f$ is given by $d^{T}Hd$. When $d$ is an eigenvector of $H$ , the second derivative in that direction is given by the corresponding eigenvalue. For other directions of $d$, the directional second derivative is a weighted average of all of the eigenvalues, with weights between 0 and 1, and eigenvectors that have smaller angle with $d$ receiving more weight. The maximum eigenvalue determines the maximum second derivative and the minimum eigenvalue determines the minimum second derivative.\nThe (directional) second derivative tells us how well we can expect a gradient descent step to perform. We can make a second-order Taylor series approximation to the function $f(x)$ around the current point $x^{(0)}$:\n$$ f(x) \\approx f(x^{(0)}) +(x - x^{(0)})^{\\mathrm{T}}g + \\frac{1}{2}(x - x^{(0)})^{\\mathrm{T}}H(x - x^{(0)}) \\\\ $$\nwhere $g$ is the gradient and H is the Hessian at $x^{(0)}$. If we use a learning rate of $\\epsilon$, then the new point $x$ will be given by $x^{(0)}-\\epsilon g$. Substituting this into our approximation, we obtain\n$$ f(x^{(0)}-\\epsilon g) \\approx f(x^{(0)}) -\\epsilon g^{\\mathrm{T}}g + \\frac{1}{2}\\epsilon^{2}g^{\\mathrm{T}}Hg \\\\ $$\nThere are three terms here: the original value of the function, the expected improvement due to the slope of the function, and the correction we must apply to account for the curvature of the function. When this last term is too large, the gradient descent step can actually move uphill. When $g^{\\mathrm{T}}Hg$ is zero or negative, the Taylor series approximation predicts that increasing $\\epsilon$ forever will decrease $f$ forever. In practice, the Taylor series is unlikely to remain accurate for large $\\epsilon$, so one must resort to more heuristic choices of $\\epsilon$ in this case. When $g^{\\mathrm{T}}Hg$ is positive, solving for the optimal step size that decreases the Taylor series approximation of the function the most yields\n$$ \\varepsilon^{*} = \\frac{g^{\\mathrm{T}}g}{g^{\\mathrm{T}}Hg} \\\\ $$\nIn the worst case, when $g$ aligns with the eigenvector of $H$ corresponding to the maximal eigenvalue $\\lambda_{\\max}$, then this optimal step size is given by $\\frac{1}{\\lambda_{\\max}}$ . To the extent that the function we minimize can be approximated well by a quadratic function, the eigenvalues of the Hessian thus determine the scale of the learning rate.\nIn multiple dimensions, there is a different second derivative for each direction at a single point. The condition number of the Hessian at this point measures how much the second derivatives differ from each other. When the Hessian has a poor condition number, gradient descent performs poorly. This is because in one direction, the derivative increases rapidly, while in another direction, it increases slowly. Gradient descent is unaware of this change in the derivative so it does not know that it needs to explore preferentially in the direction where the derivative remains negative for longer. It also makes it difficult to choose a good step size. The step size must be small enough to avoid overshooting the minimum and going uphill in directions with strong positive curvature. This usually means that the step size is too small to make significant progress in other directions with less curvature.\n  多维情况下，单个点处每个方向上的二阶导数是不同。Hessian 的条件数衡量 这些二阶导数的变化范围。当 Hessian 的条件数很差时，梯度下降法也会表现得很差。这是因为一个方向上的导数增加得很快，而在另一个方向上增加得很慢。梯度下降不知道导数的这种变化，所以它不知道应该优先探索导数长期为负的方向。病态条件也导致很难选择合适的步长。\n This issue can be resolved by using information from the Hessian matrix to guide the search. The simplest method for doing so is known as Newton’s method. Newton’s method is based on using a second-order Taylor series expansion to approximate $f(x)$ near some point $x^{(0)}$:\n$$ f(x) \\approx f(x^{(0)}) +(x - x^{(0)})^{\\mathrm{T}}\\nabla_{x}f(x^{(0)}) + \\frac{1}{2}(x - x^{(0)})^{\\mathrm{T}}H(f)(x^{(0)})(x - x^{(0)}) \\\\ $$\nIf we then solve for the critical point of this function, we obtain:\n$$ x^{*} = x^{(0)} - H(f)(x^{(0)})^{-1}\\nabla_{x}f(x^{(0)}) \\\\ $$\n 当 $f$ 是一个正定二次函数时，牛顿法只要应用一次式就能直接跳到函数的最小点。如果 $f$ 不是一个真正二次但能在局部近似为正定二次，牛顿法则需要多次迭代应用式。迭代地更新近似函数和跳到近似函数的最小点可以比梯度下降 更快地 到达临界点。这在接近局部极小点时是一个特别有用的性质，但是在鞍点附近 是有害的。当附近的临界点是最小点(Hessian 的所有特征值都是正的)时牛顿法才适用，而梯度下降不会被吸引到鞍点(除非梯度指向鞍点)。\n Optimization algorithms that use only the gradient, such as gradient descent, are called first-order optimization algorithms. Optimization algorithms that also use the Hessian matrix, such as Newton’s method, are called second-order optimization algorithms.\nConstrained Optimization Sometimes we wish not only to maximize or minimize a function $f(x)$ over all possible values of $x$. Instead we may wish to find the maximal or minimal value of $f(x)$ for values of $s$ in some set $\\mathbb{S}$. This is known as constrained optimization. Points $x$ that lie within the set $\\mathbb{S}$ are called feasible points in constrained optimization terminology.\n 有时候,在 $x$ 的所有可能值下最大化或最小化一个函数 $f(x)$ 不是我们所希望的.相反,我们可能希望在 $x$ 的某些集合 $\\mathbb{S}$ 中找 $f(x)$ 的最大值或最小值.这被称为 约束优化(constrained optimization).在约束优化术语中,集合 $\\mathbb{S}$ 内的点 $x$ 被称为 可行(feasible)点.\n We often wish to find a solution that is small in some sense. A common approach in such situations is to impose a norm constraint, such as $\\lVert x \\rVert \\leq 1$.\nThe Karush–Kuhn–Tucker (KKT) approach provides a very general solution to constrained optimization. With the KKT approach, we introduce a new function called the generalized Lagrangian or generalized Lagrange function.\nTo define the Lagrangian, we first need to describe $\\mathbb{S}$ in terms of equations and inequalities. We want a description of $\\mathbb{S}$ in terms of m functions $g^{(i)}$ and $n$ functions $h^{(j)}$ so that $\\mathbb{S} = \\{x|\\forall i,g^{(i)}(x)=0 \\ \\mathrm{and} \\ \\forall j,h^{(j)}(x) \\leq 0 \\}$.Theequations involving $g^{(i)}$ are called the equality constraints and the inequalities involving $h^{(j)}$ are called inequality constraints.\nWe introduce new variables $\\lambda_{i}$ and $\\alpha_{j}$ for each constraint, these are called the KKT multipliers. The generalized Lagrangian is then defined as\n$$ L(x,\\lambda,\\alpha)=f(x)+ \\sum_{i}\\lambda_{i}g^{(i)}(x)+\\sum_{j}\\alpha_{j}h^{(j)}(x) \\\\ $$\nWe can now solve a constrained minimization problem using unconstrained optimization of the generalized Lagrangian. Observe that, so long as at least one feasible point exists and $f(x)$ is not permitted to have value $\\infty$, then\n$$ \\mathrm{min}_{x}\\mathrm{max}_{\\lambda}\\mathrm{max}_{\\alpha,\\alpha \\geq 0} L(x,\\lambda,\\alpha) \\\\ $$\nhas the same optimal objective function value and set of optimal points $x$ as\n$$ \\mathrm{min}_{x \\in \\mathbb{S}} f(x) \\\\ $$\nThis follows because any time the constraints are satisfied,\n$$ \\mathrm{max}_{\\lambda}\\mathrm{max}_{\\alpha,\\alpha \\geq 0} L(x,\\lambda,\\alpha) = f(x) \\\\ $$\nwhile any time a constraint is violated,\n$$ \\mathrm{max}_{\\lambda}\\mathrm{max}_{\\alpha,\\alpha \\geq 0} L(x,\\lambda,\\alpha) = \\infty \\\\ $$\nThese properties guarantee that no infeasible point can be optimal, and that the optimum within the feasible points is unchanged.\nTo perform constrained maximization, we can construct the generalized Lagrange function of $-f(x)$, which leads to this optimization problem:\n$$ \\mathrm{min}_{x}\\mathrm{max}_{\\lambda}\\mathrm{max}_{\\alpha,\\alpha \\geq 0} -f(x) + \\sum_{i}\\lambda_{i}g^{(i)}(x)+\\sum_{j}\\alpha_{j}h^{(j)}(x)\\\\ $$\nWe may also convert this to a problem with maximization in the outer loop:\n$$ \\mathrm{max}_{x}\\mathrm{min}_{\\lambda}\\mathrm{min}_{\\alpha,\\alpha \\geq 0} -f(x) + \\sum_{i}\\lambda_{i}g^{(i)}(x)+\\sum_{j}\\alpha_{j}h^{(j)}(x) \\\\ $$\nThe sign of the term for the equality constraints does not matter; we may define it with addition or subtraction as we wish, because the optimization is free to choose any sign for each $\\lambda_{i}$.\nA simple set of properties describe the optimal points of constrained opti- mization problems. These properties are called the Karush-Kuhn-Tucker (KKT) conditions. They are necessary conditions, but not always sufficient conditions, for a point to be optimal. The conditions are:\n The gradient of the generalized Lagrangian is zero. All constraints on both $x$ and the KKT multipliers are satisfied. The inequality constraints exhibit \u0026ldquo;complementary slackness\u0026rdquo;: $\\alpha \\odot h(x) = 0$  Reference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/numerical.html.\n","permalink":"https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/03_numerical_computation_for_ml/","summary":"Overflow and Underflow The fundamental difficulty in performing continuous math on a digital computer is that we need to represent infinitely many real numbers with a finite number of bit patterns. This means that for almost all real numbers, we incur some approximation error when we represent the number in the computer. In many cases, this is just rounding error. Rounding error is problematic, especially when it compounds across many operations, and can cause algorithms that work in theory to fail in practice if they are not designed to minimize the accumulation of rounding error.","title":"[ML Basics] Numerical Computation"},{"content":"Random Variables A random variable is a variable that can take on different values randomly. We typically denote the random variable itself with a lower case letter in plain typeface, and the values it can take on with lower case script letters. For example, $x_{1}$ and $x_{2}$ are both possible values that the random variable $x$ can take on. For vector-valued variables, we would write the random variable as $\\mathrm{x}$ and one of its values as $x$. On its own, a random variable is just a description of the states that are possible; it must be coupled with a probability distribution that specifies how likely each of these states are.\nRandom variables may be discrete or continuous. A discrete random variable is one that has a finite or countably infinite number of states. Note that these states are not necessarily the integers; they can also just be named states that are not considered to have any numerical value. A continuous random variable is associated with a real value.\n 随机变量(random variable) 是可以随机地取不同值的变量. 随机变量可以是离散的或者连续的. 离散随机变量拥有有限或者可数无限多的状态, 连续随机变量伴随着实数值.\n Probability Distributions A probability distribution is a description of how likely a random variable or set of random variables is to take on each of its possible states. The way we describe probability distributions depends on whether the variables are discrete or continuous.\n 概率分布(probability distribution) 用来描述随机变量或一簇随机变量在每一个可能取到的状态的可能性大小. 我们描述概率分布的方式取决于随机变量是离散的还是连续的.\n Discrete Variables and Probability Mass Functions A probability distribution over discrete variables may be described using a probability mass function (PMF). We typically denote probability mass functions with a capital $P$. Often we associate each random variable with a different probability mass function and the reader must infer which probability mass function to use based on the identity of the random variable, rather than the name of the function; $P(x)$ is usually not the same as $P(y)$.\nThe probability mass function maps from a state of a random variable to the probability of that random variable taking on that state. The probability that $\\mathrm{x}=x$ is denoted as $P(x)$, with a probability of 1 indicating that $\\mathrm{x}=x$ is certain and a probability of 0 indicating that $\\mathrm{x}=x$ is impossible. Sometimes to disambiguate which PMF to use, we write the name of the random variable explicitly: $P(\\mathrm{x}=x)$. Sometimes we define a variable first, then use $\\sim$ notation to specify which distribution it follows later: $\\mathrm{x} \\sim P(\\mathrm{x})$.\n 离散型变量的概率分布可以用概率质量函数(probability mass function, PMF) 来描述. 概率质量函数将随机变量能够取得的每个状态映射到随机变量取得该状态的概率. 通常 $\\mathrm{x}=x$ 的概率用 $P(x)$ 来表示.\n Probability mass functions can act on many variables at the same time. Such a probability distribution over many variables is known as a joint probability distribution. $P(\\mathrm{x}=x,$ $\\mathrm{y}=y)$ denotes the probability that $\\mathrm{x}=x$ and $\\mathrm{y}=y$ simultaneously. We may also write $P(x,y)$ for brevity.\n 概率质量函数可以同时作用于多个随机变量。这种多个变量的概率分布被称为联合概率分布(joint probability distribution). $P(\\mathrm{x}=x,$ $\\mathrm{y}=y)$ 表示 $\\mathrm{x}=x$ 和 $\\mathrm{y}=y$ 同时发生的概率。我们也可以简写为 $P(x,y)$.\n To be a probability mass function on a random variable $x$, a function $P$ must satisfy the following properties:\n The domain of $P$ must be the set of all possible states of $\\mathrm{x}$. $\\forall x \\in \\mathrm{x}, 0 \\leq P(x) \\leq 1$. An impossible event has probability 0 and no state can be less probable than that. Likewise, an event that is guaranteed to happen has probability 1, and no state can have a greater chance of occurring. $\\sum_{\\forall x \\in \\mathrm{x}}P(x)=1$. We refer to this property as being normalized. Without this property, we could obtain probabilities greater than one by computing the probability of one of many events occurring.   如果一个函数 $P$ 是随机变量 $x$ 的概率质量函数, 必须满足下面这几个条件:\n $P$ 的定义域必须是 $x$ 所有可能状态的集合. $\\forall x \\in \\mathrm{x}, 0 \\leq P(x) \\leq 1$. $\\sum_{\\forall x \\in \\mathrm{x}}P(x)=1$. 我们把这条性质称之为归一化的(normalized) .   For example, consider a single discrete random variable $\\mathrm{x}=x$ with $k$ different states. We can place a uniform distribution on $\\mathrm{x}=x$, that is, make each of its states equally likely—by setting its probability mass function to\n$$ P(\\mathrm{x}=x_{i})= \\frac{1}{k} \\\\ $$\nfor all $i$. We can see that this fits the requirements for a probability mass function. The value $\\frac{1}{k}$ is positive because $k$ is a positive integer. We also see that\n$$ \\sum_{i}P(\\mathrm{x}=x_{i})= \\frac{1}{k}=\\sum_{i}\\frac{1}{k}=\\frac{k}{k}=1 \\\\ $$\nso the distribution is properly normalized.\n Continuous Variables and Probability Density Functions When working with continuous random variables, we describe probability distributions using a probability density function (PDF) rather than a probability mass function. To be a probability density function, a function $p$ must satisfy the following properties:\n The domain of $P$ must be the set of all possible states of $\\mathrm{x}$. $\\forall x \\in \\mathrm{x}, P(x) \\geq 0$. Note that we do not require $P(x) \\leq 1$. $\\int p(x)dx=1$   当我们研究的对象是连续型随机变量时, 我们用概率密度函数(probability density function, PDF) 而不是概率质量函数来描述它的概率分布. 如果一个函数 $p$ 是概率密度函数，必须满足下面这几个条件:\n $p$ 的定义域必须是 $x$ 所有可能状态的集合. $\\forall x \\in \\mathrm{x}, P(x) \\geq 0$ $\\int_{\\forall x \\in \\mathrm{x}} p(x)dx=1$   A probability density function $p(x)$ does not give the probability of a specific state directly, instead the probability of landing inside an infinitesimal region with volume $\\delta x$ is given by $p(x)\\delta x$.\nWe can integrate the density function to find the actual probability mass of a set of points. Specifically, the probability that $x$ lies in some set $\\mathbb{S}$ is given by the integral of $p(x)$ over that set. In the univariate example, the probability that $x$ lies in the interval $[a,b]$ is given by $\\int_{[a,b]} p(x)dx$.\nFor an example of a probability density function corresponding to a specific probability density over a continuous random variable, consider a uniform distribution on an interval of the real numbers. We can do this with a function $u(x;a,b)$, where $a$ and $b$ are the endpoints of the interval, with $b\u0026gt;a$. The \u0026ldquo;;\u0026rdquo; notation means \u0026ldquo;parametrized by\u0026rdquo; ; we consider $x$ to be the argument of the function, while $a$ and $b$ are parameters that define the function. To ensure that there is no probability mass outside the interval, we say $u(x;a,b)=0$ for all $x \\notin [a,b]$. Within $[a,b]$, $u(x;a,b)=\\frac{1}{b-a}$. We can see that this is nonnegative everywhere. Additionally, it integrates to 1. We often denote that $xd$ follows the uniform distribution on $[a,b]$ by writing $x \\sim U(a,b)$.\n Marginal Probability Sometimes we know the probability distribution over a set of variables and we want to know the probability distribution over just a subset of them. The probability distribution over the subset is known as the marginal probability distribution.\n 有时候，我们知道了一组变量的联合概率分布, 但想要了解其中一个子集的概率分布. 这种定义在子集上的概率分布被称为 边缘概率分布(marginal probability distribution) .\n For example, suppose we have discrete random variables $\\mathrm{x}$ and $\\mathrm{y}$, and we know $P(\\mathrm{x},\\mathrm{y})$. We can find $P(\\mathrm{x})$ with the sum rule:\n$$ \\forall x \\in \\mathrm{x}, P(\\mathrm{x}=x) = \\sum_{y} P(\\mathrm{x}=x,\\mathrm{y}=y) \\\\ $$\n 假设有离散型随机变量 $\\mathrm{x}$ 和 $\\mathrm{y}$, 并且我们知道 $P(\\mathrm{x},\\mathrm{y})$. 我们可以依据下面的求和法则(sum rule) 来计算 $P(\\mathrm{x})$: $$ \\forall x \\in \\mathrm{x}, P(\\mathrm{x}=x) = \\sum_{y} P(\\mathrm{x}=x,\\mathrm{y}=y) \\\\ $$\n The name “marginal probability” comes from the process of computing marginal probabilities on paper. When the values of $P(\\mathrm{x},\\mathrm{y})$ are written in a grid with different values of $x$ in rows and different values of $y$ in columns, it is natural to sum across a row of the grid, then write $P(x)$ in the margin of the paper just to the right of the row.\nFor continuous variables, we need to use integration instead of summation:\n$$ p(x) = \\int p(x,y)dy \\\\ $$\n 对于连续型变量，我们需要用积分替代求和: $$ p(x) = \\int p(x,y)dy \\\\ $$\n Conditional Probability In many cases, we are interested in the probability of some event, given that some other event has happened. This is called a conditional probability. We denote the conditional probability that $\\mathrm{y}=y$ given $\\mathrm{x}=x$ as $P(\\mathrm{y}=y|\\mathrm{x}=x)$. This conditional probability can be computed with the formula\n$$ P(\\mathrm{y}=y|\\mathrm{x}=x) = \\frac{P(\\mathrm{y}=y,\\mathrm{x}=x)}{P(\\mathrm{x}=x)} \\\\ $$\nThe conditional probability is only defined when $P(\\mathrm{x}=x)\u0026gt;0$. We cannot compute the conditional probability conditioned on an event that never happens.\n 在很多情况下, 我们感兴趣的是某个事件, 在给定其他事件发生时出现的概率. 这种概率叫做条件概率(conditional probability) . 我们将给定 $\\mathrm{x}=x$, $\\mathrm{y}=y$ 发生的条件概率记为 $P(\\mathrm{y}=y|\\mathrm{x}=x)$. 这个条件概率可以通过下面的公式计算: $$ P(\\mathrm{y}=y|\\mathrm{x}=x) = \\frac{P(\\mathrm{y}=y,\\mathrm{x}=x)}{P(\\mathrm{x}=x)} \\\\ $$ 条件概率只在 $P(\\mathrm{x}=x)\u0026gt;0$ 时有定义.\n It is important not to confuse conditional probability with computing what would happen if some action were undertaken. The conditional probability that a person is from Germany given that they speak German is quite high, but if a randomly selected person is taught to speak German, their country of origin does not change. Computing the consequences of an action is called making an intervention query. Intervention queries are the domain of causal modeling.\nThe Chain Rule of Conditional Probabilities Any joint probability distribution over many random variables may be decomposed into conditional distributions over only one variable:\n$$ P(\\mathrm{x}^{(1)},\\cdots,\\mathrm{x}^{(n)})=P(\\mathrm{x}^{(1)}\\prod_{i=2}^{n}P(\\mathrm{x}^{(i)}|\\mathrm{x}^{(1)},\\cdots,\\mathrm{x}^{(i-1)}) \\\\ $$\nThis observation is known as the chain rule or product rule of probability. It follows immediately from the definition of conditional probability. For example, applying the definition twice, we get\n$$ \\begin{align*} P(a,b,c) \u0026amp;= P(a|b,c)P(b,c) \\\\ P(b,c) \u0026amp;= P(b|c)P(c) \\\\ P(a,b,c) \u0026amp;= P(a|b,c)P(b|c)P(c) \\\\ \\end{align*} $$\nIndependence and Conditional Independence Two random variables $x$ and $y$ are independent if their probability distribution can be expressed as a product of two factors, one involving only $x$ and one involving only $y$:\n$$ \\forall x \\in \\mathrm{x}, y \\in \\mathrm{y}, p(\\mathrm{x}=x, \\mathrm{y}=y) = p(\\mathrm{x}=x)p(\\mathrm{y}=y) \\\\ $$\n 两个随机变量 $x$ 和 $y$, 如果它们的概率分布可以表示成两个因子的乘积形式, 并且一个因子只包含 $x$ 另一个因子只包含 $y$, 我们就称这两个随机变量是相互独立的(independent)\n Two random variables $x$ and $y$ are conditionally independent given a random variable $z$ if the conditional probability distribution over $x$ and $y$ factorizes in this way for every value of $z$:\n$$ \\forall x \\in \\mathrm{x}, y \\in \\mathrm{y}, z \\in \\mathrm{z}, p(\\mathrm{x}=x, \\mathrm{y}=y | \\mathrm{z}=z) = p(\\mathrm{x}=x| \\mathrm{z}=z)p(\\mathrm{y}=y| \\mathrm{z}=z) \\\\ $$\n 如果关于 $x$ 和 $y$ 的条件概率分布对于 $z$ 的每一个值都可以写成乘积的形式, 那么这两个随机变量 $x$ 和 $y$ 在给定随机变量 $z$ 时是条件独立的(conditionally independent).\n We can denote independence and conditional independence with compact notation: $\\mathrm{x} \\bot \\mathrm{y}$ means that $\\mathrm{x}$ and $\\mathrm{y}$ are independent, while $\\mathrm{x} \\bot \\mathrm{y} | \\mathrm{z}$ means that $\\mathrm{x}$ and $\\mathrm{y}$ are conditionally independent given $\\mathrm{z}$.\nExpectation, Variance and Covariance The expectation or expected value of some function $f(x)$ with respect to a probability distribution $P(x)$ is the average or mean value that $f$ takes on when $x$ is drawn from $P$ . For discrete variables this can be computed with a summation:\n$$ E_{\\mathrm{x} \\sim P}[f(x)] = \\sum_{x}P(x)f(x) \\\\ $$\n 函数 $f(x)$ 关于某分布 $P(x)$ 的期望(expectation) 或者期望值(expected value) 是指，当 $x$ 由 $P$ 产生，$f$ 作用于 $x$ 时，$f(x)$ 的平均值. 对于离散型随机变量，这可以通过求和得到: $$ E_{\\mathrm{x} \\sim P}[f(x)] = \\sum_{x}P(x)f(x) \\\\ $$\n while for continuous variables, it is computed with an integral:\n$$ E_{\\mathrm{x} \\sim P}[f(x)] = \\int p(x)f(x)dx \\\\ $$\n 对于连续型随机变量可以通过求积分得到: $$ E_{\\mathrm{x} \\sim P}[f(x)] = \\int p(x)f(x)dx \\\\ $$\n When the identity of the distribution is clear from the context, we may simply write the name of the random variable that the expectation is over, as in $E_{\\mathrm{x}}[f(x)]$. If it is clear which random variable the expectation is over, we may omit the subscript entirely, as in $E[f(x)]$. By default, we can assume that $E[\\cdot]$ averages over the values of all the random variables inside the brackets. Likewise, when there is no ambiguity, we may omit the square brackets.\nExpectations are linear, for example,\n$$ E_{\\mathrm{x}}[\\alpha f(x) + \\beta g(x)] = \\alpha E_{\\mathrm{x}}[f(x)] + \\beta E_{\\mathrm{x}}[g(x)] \\\\ $$\nwhen $\\alpha$ and $\\beta$ are not dependent on $x$.\n 期望是线性, 例如 $$ E_{\\mathrm{x}}[\\alpha f(x) + \\beta g(x)] = \\alpha E_{\\mathrm{x}}[f(x)] + \\beta E_{\\mathrm{x}}[g(x)] \\\\ $$ 其中 $\\alpha$ 和 $\\beta$ 不依赖于 $x$ .\n The variance gives a measure of how much the values of a function of a random variable x vary as we sample different values of $\\mathrm{x}$ from its probability distribution:\n$$ Var(f(x)) = E[(f(x)-E[f(x)])^{2}] \\\\ $$\nWhen the variance is low, the values of $f(x)$ cluster near their expected value. The square root of the variance is known as the standard deviation.\n 方差(variance) 衡量的是当我们对 $x$ 依据它的概率分布进行采样时, 随机变量 $x$ 的函数值会呈现多大的差异: $$ Var(f(x)) = E[(f(x)-E[f(x)])^{2}] \\\\ $$ 当方差很小时, $f(x)$ 的值形成的簇比较接近它们的期望值. 方差的平方根被称为 标准差(standard deviation) .\n The covariance gives some sense of how much two values are linearly related to each other, as well as the scale of these variables:\n$$ Cov(f(x),g(y)) = E[(f(x)-E[f(x)])(g(y)-E[g(y)])] \\\\ $$\n 协方差(covariance) 在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度: $$ Cov(f(x),g(y)) = E[(f(x)-E[f(x)])(g(y)-E[g(y)])] \\\\ $$\n High absolute values of the covariance mean that the values change very much and are both far from their respective means at the same time. If the sign of the covariance is positive, then both variables tend to take on relatively high values simultaneously. If the sign of the covariance is negative, then one variable tends to take on a relatively high value at the times that the other takes on a relatively low value and vice versa. Other measures such as correlation normalize the contribution of each variable in order to measure only how much the variables are related, rather than also being affected by the scale of the separate variables.\n 协方差的绝对值如果很大则意味着变量值变化很大并且它们同时距离各自的均值很远. 如果协方差是正的, 那么两个变量都倾向于同时取得相对较大的值. 如果协方差是负的, 那么其中一个变量倾向于取得相对较大的值的同时, 另一个变量倾向于取得相对较小的值, 反之亦然. 其他的衡量指标如相关系数(correlation) 将每个变量的贡献归一化, 为了只衡量变量的相关性而不受各个变量尺度大小的影响.\n The covariance matrix of a random vector $x \\in \\mathbb{R}^{n}$ is an $n \\times n$ matrix, such that\n$$ Cov(\\mathrm{x})_{i,j} = Cov(\\mathrm{x}_{i},\\mathrm{x}_{j}) \\\\ $$\nThe diagonal elements of the covariance give the variance:\n$$ Cov(\\mathrm{x}_{i},\\mathrm{x}_{i}) = Var(\\mathrm{x}_{i}) \\\\ $$\nCommon Probability Distributions Several simple probability distributions are useful in many contexts in machine learning.\nBernoulli Distribution The Bernoulli distribution is a distribution over a single binary random variable. It is controlled by a single parameter $\\phi \\in [0,1]$, which gives the probability of the random variable being equal to 1. It has the following properties:\n$$ P(\\mathrm{x}=1) = \\phi \\\\ P(\\mathrm{x}=0) = 1-\\phi \\\\ P(\\mathrm{x}=x) =\\phi^{x}(1-\\phi)^{1-x} \\\\ E_{\\mathrm{x}}[\\mathrm{x}] = \\phi \\\\ Var_{\\mathrm{x}}(\\mathrm{x}) = \\phi(1-\\phi) \\\\ $$\nGaussian Distribution The most commonly used distribution over real numbers is the normal distribution, also known as the Gaussian distribution:\n$$ \\mathcal{N}(x;\\mu, \\sigma^{2}) = \\sqrt{\\frac{1}{2\\pi \\sigma^{2}}}exp \\left(-\\frac{1}{2\\sigma^{2}}(x-\\mu)^{2} \\right) \\\\ $$\nThe two parameters $\\mu \\in \\mathbb{R}$ and $\\sigma \\in (0, \\infty)$ control the normal distribution. The parameter $\\mu$ gives the coordinate of the central peak. This is also the mean of the distribution: $E[x]=\\mu$. The standard deviation of the distribution is given by $\\sigma$, and the variance by $\\sigma^{2}$.\n Normal distributions are a sensible choice for many applications. In the absence of prior knowledge about what form a distribution over the real numbers should take, the normal distribution is a good default choice for two major reasons.\nFirst, many distributions we wish to model are truly close to being normal distributions. The central limit theorem shows that the sum of many independent random variables is approximately normally distributed. This means that in practice, many complicated systems can be modeled successfully as normally distributed noise, even if the system can be decomposed into parts with more structured behavior.\nSecond, out of all possible probability distributions with the same variance, the normal distribution encodes the maximum amount of uncertainty over the real numbers. We can thus think of the normal distribution as being the one that inserts the least amount of prior knowledge into a model.\nThe normal distribution generalizes to $\\mathbb{R}^{n}$, in which case it is known as the multivariate normal distribution. It may be parametrized with a positive definite symmetric matrix $\\Sigma$:\n$$ \\mathcal{N}(x;\\mu, \\Sigma) = \\sqrt{\\frac{1}{(2\\pi)^{n} det(\\Sigma)}}exp\\left(-\\frac{1}{2}(x-\\mu)^{T} \\Sigma^{-1} (x-\\mu) \\right) \\\\ $$\nThe parameter $\\mu$ still gives the mean of the distribution, though now it is vector-valued. The parameter $\\Sigma$ gives the covariance matrix of the distribution.\nExponential and Laplace distributions In the context of deep learning, we often want to have a probability distribution with a sharp point at $x=0$. To accomplish this, we can use the exponential distribution:\n$$ p(x;\\lambda)=\\begin{cases} \\begin{align*} \u0026amp;\\lambda e^{-\\lambda x} \u0026amp;x \\geq 0 \\\\ \u0026amp;0 \u0026amp;x \u0026lt; 0 \\\\ \\end{align*} \\end{cases} $$\n A closely related probability distribution that allows us to place a sharp peak of probability mass at an arbitrary point $\\mu$ is the Laplace distribution\n$$ \\mathrm{Laplace}(x;\\mu, \\gamma) = \\frac{1}{2\\gamma}exp{\\left(-\\frac{|x-\\mu|}{\\gamma}\\right)} \\\\ $$\n Useful Properties of Common Functions Certain functions arise often while working with probability distributions, especially the probability distributions used in deep learning models.\nOne of these functions is the logistic sigmoid:\n$$ \\sigma(x) = \\frac{1}{1+exp(-x)} \\\\ $$\n The logistic sigmoid is commonly used to produce the $\\phi$ parameter of a Bernoulli distribution because its range is $(0,1)$, which lies within the valid range of values for the $\\phi$ parameter. The sigmoid function saturates when its argument is very positive or very negative, meaning that the function becomes very flat and insensitive to small changes in its input.\nAnother commonly encountered function is the softplus function (Dugas et al., 2001):\n$$ \\zeta(x) = log(1+ exp(x)) \\\\ $$\n The softplus function can be useful for producing the $\\beta$ or $\\sigma$ parameter of a normal distribution because its range is $(0, \\infty)$. It also arises commonly when manipulating expressions involving sigmoids. The name of the softplus function comes from the fact that it is a smoothed or “softened” version of\n$$ x^{+} = max(0,x) \\\\ $$\nThe following properties are all useful:\n$$ \\sigma(x) = \\frac{exp(x)}{exp(x)+exp(0)} \\\\ \\frac{d}{dx}\\sigma(x) = \\sigma(x)(1-\\sigma(x)) \\\\ 1 - \\sigma(x) = \\sigma(-x) \\\\ log \\sigma(x) = -\\zeta(-x) \\\\ \\frac{d}{dx}\\zeta(x) = \\sigma(x) \\\\ \\forall x \\in (0,1), \\sigma^{-1}(x) = log \\left( \\frac{x}{1-x} \\right) \\\\ \\forall x \u0026gt; 0, \\zeta^{-1}(x) = log(exp(x)-1) \\\\ \\zeta(x) = \\int_{-\\infty}^{x}\\sigma(y)dy \\\\ \\zeta(x)-\\zeta(-x) = x \\\\ $$\nBayes’ Rule We often find ourselves in a situation where we know $P(\\mathrm{y}|\\mathrm{x})$ and need to know $P(\\mathrm{x}|\\mathrm{y})$. Fortunately, if we also know $P(\\mathrm{x})$, we can compute the desired quantity using Bayes’ rule:\n$$ P(\\mathrm{x}|\\mathrm{y}) = \\frac{P(\\mathrm{x})P(\\mathrm{y}|\\mathrm{x})}{P(\\mathrm{y})} \\\\ $$\nNote that while $P(\\mathrm{y})$ appears in the formula, it is usually feasible to compute $P(\\mathrm{y})=$ $\\sum_{x}P(\\mathrm{y}|\\mathrm{x})P(\\mathrm{x})$, so we do not need to begin with knowledge of $P(\\mathrm{y})$.\n 我们经常会需要在已知 $P(\\mathrm{y}|\\mathrm{x})$ 时计算 $P(\\mathrm{x}|\\mathrm{y})$. 如果还知道 $P(\\mathrm{x})$, 我们可以用贝叶斯规则(Bayes’ rule) 来实现这一目的: $$ P(\\mathrm{x}|\\mathrm{y}) = \\frac{P(\\mathrm{x})P(\\mathrm{y}|\\mathrm{x})}{P(\\mathrm{y})} \\\\ $$ 注意到 $P(\\mathrm{y})$ 出现在上面的公式中, 它通常使用 $P(\\mathrm{y})=\\sum_{x}P(\\mathrm{y}|\\mathrm{x})P(\\mathrm{x})$ 来计算, 所以我们并不需要事先知道 $P(\\mathrm{y})$ 的信息.\n Information Theory Information theory is a branch of applied mathematics that revolves around quantifying how much information is present in a signal. It was originally invented to study sending messages from discrete alphabets over a noisy channel, such as communication via radio transmission. In this context, information theory tells how to design optimal codes and calculate the expected length of messages sampled from specific probability distributions using various encoding schemes. In the context of machine learning, we can also apply information theory to continuous variables where some of these message length interpretations do not apply. This field is fundamental to many areas of electrical engineering and computer science.\nThe basic intuition behind information theory is that learning that an unlikely event has occurred is more informative than learning that a likely event has occurred. We would like to quantify information in a way that formalizes this intuition. Specifically,\n Likely events should have low information content, and in the extreme case, events that are guaranteed to happen should have no information content whatsoever. Less likely events should have higher information content. Independent events should have additive information. For example, finding out that a tossed coin has come up as heads twice should convey twice as much information as finding out that a tossed coin has come up as heads once.   信息论(information theory) 是应用数学的一个分支, 主要研究的是对一个信号包含信息的多少进行量化. 信息论的基本想法是一个不太可能的事件居然发生了, 要比一个非常可能的事件发生, 能提供更多的信息. 我们想要通过这种基本想法来量化信息:\n 非常可能发生的事件信息量要比较少, 并且极端情况下, 确保能够发生的事件应该没有信息量. 较不可能发生的事件具有更高的信息量. 独立事件应具有增量的信息. 例如, 投掷的硬币两次正面朝上传递的信息量, 应该是投掷一次硬币正面朝上的信息量的两倍.   In order to satisfy all three of these properties, we define the self-information of an event $\\mathrm{x}=x$ to be\n$$ I(x) = -log P(x) \\\\ $$\nwe always use log to mean the natural logarithm, with base $e$. Our definition of $I(x)$ is therefore written in units of nats. One nat is the amount of information gained by observing an event of probability $\\frac{1}{e}$ . Other texts use base-2 logarithms and units called bits or shannons; information measured in bits is just a rescaling of information measured in nats.\n 为了满足上述三个性质, 我们定义一个事件 $\\mathrm{x}=x$ 的自信息(self-information) 为: $$ I(x) = -log P(x) \\\\ $$ 我们定义的 $I(x)$ 单位是 奈特(nats) . 一奈特是以 $\\frac{1}{e}$ 的概率观测到一个事件时获得的信息量.\n When $\\mathrm{x}$ is continuous, we use the same definition of information by analogy, but some of the properties from the discrete case are lost. For example, an event with unit density still has zero information, despite not being an event that is guaranteed to occur.\nSelf-information deals only with a single outcome. We can quantify the amount of uncertainty in an entire probability distribution using the Shannon entropy:\n$$ H(x) = E_{\\mathrm{x} \\sim P}[I(x)] = -E_{\\mathrm{x} \\sim P}[log P(x)] \\\\ $$\nalso denoted $H(P)$. In other words, the Shannon entropy of a distribution is the expected amount of information in an event drawn from that distribution. It gives a lower bound on the number of bits (if the logarithm is base 2, otherwise the units are different) needed on average to encode symbols drawn from a distribution $P$. Distributions that are nearly deterministic (where the outcome is nearly certain) have low entropy; distributions that are closer to uniform have high entropy. When $x$ is continuous, the Shannon entropy is known as the differential entropy.\n 我们可以用 香农熵(Shannon entropy) 来对整个概率分布中的不确定性总量进行量化: $$ H(x) = E_{\\mathrm{x} \\sim P}[I(x)] = -E_{\\mathrm{x} \\sim P}[log P(x)] \\\\ $$ 也记作 $H(P)$. 换言之, 一个分布的香农熵是指遵循这个分布的事件所产生的期望信息总量. 那些接近确定性的分布 (输出几乎可以确定) 具有较低的熵; 那些接近均匀分布的概率分布具有较高的熵. 当 $x$ 是连续的, 香农熵被称为微分熵(differential entropy) .\n  This plot shows how distributions that are closer to deterministic have low Shannon entropy while distributions that are close to uniform have high Shannon entropy. On the horizontal axis, we plot $p$, the probability of a binary random variable being equal to 1. The entropy is given by $(p−1)log(1−p)−plog(p)$. When $p$ is near 0,the distribution is nearly deterministic, because the random variable is nearly always 0. When $p$ is near 1, the distribution is nearly deterministic, because the random variable is nearly always 1. When $p=0.5$, the entropy is maximal, because the distribution is uniform over the two outcomes.\nReference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/prob.html.\n","permalink":"https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/02_probability_and_information_theory_for_ml/","summary":"Random Variables A random variable is a variable that can take on different values randomly. We typically denote the random variable itself with a lower case letter in plain typeface, and the values it can take on with lower case script letters. For example, $x_{1}$ and $x_{2}$ are both possible values that the random variable $x$ can take on. For vector-valued variables, we would write the random variable as $\\mathrm{x}$ and one of its values as $x$.","title":"[ML Basics] Probability and Information Theory"},{"content":"Scalars, Vectors, Matrices and Tensors The study of linear algebra involves several types of mathematical objects:\n  Scalars: A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers. We write scalars in italics. We usually give scalars lowercase variable names. When we introduce them, we specify what kind of number they are. For example, we might say \u0026ldquo;Let $s \\in \\mathbb{R}$ be the slope of the line,\u0026rdquo; while defining a real-valued scalar, or \u0026ldquo;Let $n \\in \\mathbb{N}$ be the number of units, while defining an natural number scalar.\n  Vectors: A vector is an array of numbers. The numbers are arranged in order. We can identify each individual number by its index in that ordering. Typically we give vectors lowercase names in bold typeface, such as $\\mathbf{x}$. The elements of the vector are identified by writing its name in italic typeface, with a subscript. We also need to say what kind of numbers are stored in the vector. If each element is in $\\mathbb{R}$, and the vector has $n$ elements, then the vector lies in the set formed by taking the Cartesian product of $\\mathbb{R}$ $n$ times, denoted as $\\mathbb{R}^{n}$. When we need to explicitly identify the elements of a vector, we write them as a column enclosed in square brackets:\n  $$ \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\\\ \\end{bmatrix} $$\n Matrices: A matrix is a 2-D array of numbers, so each element is identified by two indices instead of just one. We usually give matrices upper-case variable names with bold typeface, such as $\\mathbf{A}$. If a real-valued matrix $\\mathbf{A}$ has a height of $m$ and a width of $n$, then we say that $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$. We usually identify the elements of a matrix using its name in italic but not bold font, and the indices are listed with separating commas. For example, $A_{1,1}$ is the upper left entry of $\\mathbf{A}$ and $A_{m,n}$ is the bottom right entry. We can identify all of the numbers with vertical coordinate $i$ by writing a \u0026ldquo;:\u0026rdquo; for the horizontal coordinate. For example, $A_{i,:}$ denotes the horizontal cross section of $\\mathbf{A}$ with vertical coordinate $i$. This is known as the $i$-th row of $\\mathbf{A}$. Likewise, $A_{:,i}$ is the $i$-th column of $\\mathbf{A}$. When we need to explicitly identify the elements of a matrix, we write them as an array enclosed in square brackets:  $$ \\begin{bmatrix} A_{1,1} \u0026amp; A_{1,2} \\\\ A_{2,1} \u0026amp; A_{2,2} \\\\ \\end{bmatrix} $$\n Tensors: In some cases we will need an array with more than two axes. In the general case, an array of numbers arranged on a regular grid with a variable number of axes is known as a tensor. We denote a tensor named \u0026ldquo;A\u0026rdquo; with this typeface: $\\mathsf{A}$. We identify the element of $\\mathsf{A}$ at coordinates $(i,j,k)$ by writing $A_{i,j,k}$.   标量(Scalar): 标量是一个单独的数. 向量(Vector): 向量是一列有序排列的数. 矩阵(Matrix): 矩阵是一个二维数组, 其中的每一个元素被两个索引. 张量(Tensor): 张量是一个超过二维的数组.  One important operation on matrices is the transpose. The transpose of a matrix is the mirror image of the matrix across a diagonal line, called the main diagonal, running down and to the right, starting from its upper left corner. We denote the transpose of a matrix $A$ as $A^{T}$, and it is defined such that\n$$ (A^{T})_{i,j} = A_{j,i} $$\n Vectors can be thought of as matrices that contain only one column. The transpose of a vector is therefore a matrix with only one row. Sometimes we define a vector by writing out its elements in the text inline as a row matrix, then using the transpose operator to turn it into a standard column vector, e.g., $x = \\begin{bmatrix} x_{1}, x_{2}, x_{3} \\\\ \\end{bmatrix}^{T}$.\nA scalar can be thought of as a matrix with only a single entry. From this, we can see that a scalar is its own transpose: $a = a^{T}$.\n 转置(Transpose) 是矩阵的一种重要操作, 矩阵的转置是以对角线为轴的镜像, 这条从左上角到右下角的对角线被称为主对角线. 换句话来说, 矩阵的转置可以看成以主对角线为轴的一个镜像. 向量可以看作只有一列的矩阵。对应地, 向量的转置可以看作是只有一行的矩阵.\n标量可以看作是只有一个元素的矩阵. 因此, 标量的转置等于它本身.  We can add matrices to each other, as long as they have the same shape, just by adding their corresponding elements: $C = A + B$ where $C_{i,j} = A_{i,j} + B_ {i,j}$.\nWe can also add a scalar to a matrix or multiply a matrix by a scalar, just by performing that operation on each element of a matrix: $D = a \\cdot B + c$ where $D_{i,j} =a \\cdot B_{i,j} +c$.\nIn the context of deep learning, we also use some less conventional notation. We allow the addition of matrix and a vector, yielding another matrix: $C = A + b$, where $C_{i,j} = A_{i,j} + b_{j}$ . In other words, the vector $b$ is added to each row of the matrix. This shorthand eliminates the need to define a matrix with $b$ copied into each row before doing the addition. This implicit copying of $b$ to many locations is called broadcasting.\n 只要矩阵的形状一样, 我们可以把两个矩阵相加是指对应位置的元素相加. 标量和矩阵相乘, 或是和矩阵相加时, 我们只需将其与矩阵的每个元素相乘或相加.   深度学习中, 我们允许矩阵和向量相加. 方法是将向量和矩阵的每一行相加. 这个简写方法使我们无需在加法操作前定义一个将向量复制到每一行而生成的矩阵. 这种隐式地复制向量到很多位置的方式, 被称为广播(Broadcasting).\n Multiplying Matrices and Vectors One of the most important operations involving matrices is multiplication of two matrices. The matrix product of matrices $A$ and $B$ is a third matrix $C$. In order for this product to be defined, $A$ must have the same number of columns as $B$ has rows. If $A$ is of shape $m \\times n$ and $B$ is of shape $n \\times p$, then $C$ is of shape $m \\times p$. We can write the matrix product just by placing two or more matrices together, e.g.\n$$ C_{m \\times p} = A_{m \\times n}B_{n \\times p} \\\\ $$\nThe product operation is defined by\n$$ C_{i,j} = \\sum_{k}A_{i,k}B_{k,j} \\\\ $$\nFor example, if $A = \\begin{bmatrix} A_{1,1} \u0026amp; A_{1,2} \\\\ A_{2,1} \u0026amp; A_{2,2} \\\\ A_{3,1} \u0026amp; A_{3,2} \\\\ \\end{bmatrix}$ and $B = \\begin{bmatrix} B_{1,1} \u0026amp; B_{1,2} \\\\ B_{2,1} \u0026amp; B_{2,2} \\\\ \\end{bmatrix}$, then\n$$ C = A \\times B = \\begin{bmatrix} A_{1,1}B_{1,1} + A_{1,2}B_{2,1} \u0026amp; A_{1,1}B_{1,2} + A_{1,2}B_{2,2} \\\\ A_{2,1}B_{1,1} + A_{2,2}B_{2,1} \u0026amp; A_{2,1}B_{1,2} + A_{2,2}B_{2,2} \\\\ A_{3,1}B_{1,1} + A_{3,2}B_{2,1} \u0026amp; A_{3,1}B_{1,2} + A_{3,2}B_{2,2} \\\\ \\end{bmatrix} $$\n 两个矩阵的乘法仅当第一个矩阵 $A$ 的列数(column) 和另一个矩阵 $B$ 的行数(row) 相等时才能定义. 如 $A$ 是一个 $m \\times n$ 的矩阵, $B$ 是一个 $n \\times p$ 的矩阵，那它们的乘积 $AB$ 就会是一个 $m \\times p$ 的矩阵.\n Note that the standard product of two matrices is not just a matrix containing the product of the individual elements. Such an operation exists and is called the element-wise product or Hadamard product, and is denoted as $A \\odot B$.\nFor example,\n$$ \\begin{bmatrix} A_{1,1} \u0026amp; A_{1,2} \\\\ A_{2,1} \u0026amp; A_{2,2} \\\\ \\end{bmatrix} \\odot \\begin{bmatrix} B_{1,1} \u0026amp; B_{1,2} \\\\ B_{2,1} \u0026amp; B_{2,2} \\\\ \\end{bmatrix} = \\begin{bmatrix} A_{1,1} \\cdot B_{1,1} \u0026amp; A_{1,2} \\cdot B_{1,2} \\\\ A_{2,1} \\cdot B_{2,1} \u0026amp; A_{2,2} \\cdot B_{2,2} \\\\ \\end{bmatrix} \\\\ $$\n 两个矩阵中对应元素的乘积被称为元素对应乘积(element-wise product) 或者Hadamard乘积(Hadamard product), 记为 $A \\odot B$.\n The dot product between two vectors $x$ and $y$ of the same dimensionality is the matrix product $x^{T}y$. We can think of the matrix product $C=AB$ as computing $C_{i,j}$ as the dot product between row $i$ of $A$ and column $j$ of $B$.\nMatrix product operations have many useful properties that make mathematical analysis of matrices more convenient. For example, matrix multiplication is distributive:\n$$ A(B+C)=AB+AC \\\\ $$\nIt is also associative:\n$$ A(BC)=(AB)C \\\\ $$\n 矩阵的乘法满足结合律和对矩阵加法的分配律:\n 结合律：$A(BC)=(AB)C$  分配律：$A(B+C)=AB+AC$\n   Matrix multiplication is not commutative (the condition $AB = BA$ does not always hold), unlike scalar multiplication. However, the dot product between two vectors is commutative:\n$$ x^{T}y = y^{T}x \\\\ $$\nThe transpose of a matrix product has a simple form:\n$$ (AB)^{T} = B^{T}A^{T} \\\\ $$\n 矩阵的乘法与数乘运算之间也满足类似结合律的规律；与转置之间则满足倒置的分配律:\n $c(AB)=(cA)B=A(cB)$  $(AB)^{T} = B^{T}A^{T}$    Now, We can write down a system of linear equations:\n$$ Ax = b \\\\ $$\nWhere $A \\in R^{m \\times n}$ is a known matrix, $b \\in R^{m}$ is a known vector, and $x \\in R^{n}$ is a vector of unknown variables we would like to solve for. Each element $x_{i}$ of $x$ is one of these unknown variables. Each row of $A$ and each element of $b$ provide another constraint.\n$$ Ax = b \\Rightarrow \\begin{bmatrix} A_{1,1} \u0026amp; A_{1,2} \u0026amp; \\cdots \u0026amp; A_{1,n}\\\\ A_{2,1} \u0026amp; A_{2,2} \u0026amp; \\cdots \u0026amp; A_{2,n}\\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots\\\\ A_{m,1} \u0026amp; A_{m,2} \u0026amp; \\cdots \u0026amp; A_{m,n}\\\\ \\end{bmatrix} \\times \\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\\\ \\end{bmatrix} = \\begin{bmatrix} b_{1} \\\\ b_{2} \\\\ \\vdots \\\\ b_{m} \\\\ \\end{bmatrix} \\\\ $$\n$$ \\Rightarrow \\begin{cases} A_{1,1}x_{1}+A_{1,2}x_{2}+ \\cdots +A_{1,n}x_{n} = b_{1} \\\\ A_{2,1}x_{1}+A_{2,2}x_{2}+ \\cdots +A_{2,n}x_{n} = b_{2} \\\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\cdots \\\\ A_{m,1}x_{1}+A_{m,2}x_{2}+ \\cdots +A_{m,n}x_{n} = b_{m} \\\\ \\end{cases} $$\nIdentity and Inverse Matrices Linear algebra offers a powerful tool called matrix inversion that allows us to analytically solve equation $Ax=b$ for many values of $A$.\nTo describe matrix inversion, we first need to define the concept of an identity matrix. An identity matrix is a matrix that does not change any vector when we multiply that vector by that matrix. We denote the identity matrix that preserves $n$-dimensional vectors as $I_{n}$. Formally, $I_{n} \\in \\mathbb{R}^{n \\times n}$, and\n$$ \\forall x \\in \\mathbb{R}^{n}, I_{n}x=x \\\\ $$\nThe structure of the identity matrix is simple: all of the entries along the main diagonal are 1, while all of the other entries are zero. For example,\n$$ I_{3} = \\begin{bmatrix} 1 \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; 1 \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; 1 \\\\ \\end{bmatrix} $$\n 单位矩阵(identity matrix) 的概念为, 任意向量和单位矩阵相乘, 都不会改变. 我们将保持$n$维向量不变的单位矩阵记作 $I_{n}$. 单位矩阵的结构很简单: 所有沿主对角线的元素都是1, 而所有其他位置的元素都是0.\n The matrix inverse of $A$ is denoted as $A^{-1}$, and it is defined as the matrix such that\n$$ A^{-1}A = I_{n} \\\\ $$\n 矩阵$A$的矩阵逆(matrix inversion) 记作 $A^{-1}$, 其定义的矩阵满足如下条件: $$A^{-1}A = I_{n}$$\n We can now solve equation $Ax=b$ by the following steps:\n$$ Ax=b \\Rightarrow A^{-1}Ax=A^{-1}b \\Rightarrow x = A^{-1}b \\\\ $$\nWhen $A^{-1}$ exists, several different algorithms exist for finding it in closed form. In theory, the same inverse matrix can then be used to solve the equation many times for different values of $b$ . However, $A^{-1}$ is primarily useful as a theoretical tool, and should not actually be used in practice for most software applications. Because $A^{-1}$ can be represented with only limited precision on a digital computer, algorithms that make use of the value of $b$ can usually obtain more accurate estimates of $x$.\nLinear Dependence and Span In order for $A^{-1}$ to exist, equation $Ax=b$ must have exactly one solution for every value of $b$. However, it is also possible for the system of equations to have no solutions or infinitely many solutions for some values of $b$. It is not possible to have more than one but less than infinitely many solutions for a particular $b$; if both $x$ and $y$ are solutions then\n$$ z = \\alpha x + (1-\\alpha)y \\\\ $$\nis also a solution for any real $\\alpha$.\nTo analyze how many solutions the equation has, we can think of the columns of $A$ as specifying different directions we can travel from the origin (the point specified by the vector of all zeros), and determine how many ways there are of reaching $b$. In this view, each element of $x$ specifies how far we should travel in each of these directions, with $x_{i}$ specifying how far to move in the direction of column $i$:\n$$ Ax = \\sum_{i}x_{i}A_{:,i} \\\\ $$\nIn general, this kind of operation is called a linear combination. Formally, a linear combination of some set of vectors $\\{v^{(1)}, \\cdots, v^{(n)}\\}$ is given by multiplying each vector $v^{(i)}$ by a corresponding scalar coefficient and adding the results:\n$$ \\sum_{i}c_{i}v^{(i)} \\\\ $$\nThe span of a set of vectors is the set of all points obtainable by linear combination of the original vectors.\n 一组向量的生成子空间(span) 是原始向量线性组合后所能抵达的点的集合.\n Determining whether $Ax=b$ has a solution thus amounts to testing whether $b$ is in the span of the columns of $A$. This particular span is known as the column space or the range of $A$.\nA set of vectors is linearly independent if no vector in the set is a linear combination of the other vectors. If we add a vector to a set that is a linear combination of the other vectors in the set, the new vector does not add any points to the set’s span. This means that for the column space of the matrix to encompass all of $\\mathbb{R}^{m}$, the matrix must contain at least one set of $m$ linearly independent columns. This condition is both necessary and sufficient for equation $Ax=b$ to have a solution for every value of $b$. Note that the requirement is for a set to have exactly $m$ linear independent columns, not at least $m$. No set of $m$-dimensional vectors can have more than m mutually linearly independent columns, but a matrix with more than $m$ columns may have more than one such set.\n 如果一组向量中的任意一个向量都不能表示成其他向量的线性组合，那么这组向量称为线性无关 (linearly independent) .\n In order for the matrix to have an inverse, we additionally need to ensure that equation $Ax=b$ has at most one solution for each value of $b$. To do so, we need to ensure that the matrix has at most $m$ columns. Otherwise there is more than one way of parametrizing each solution.\nTogether, this means that the matrix must be square, that is, we require that $m=n$ and that all of the columns must be linearly independent. A square matrix with linearly dependent columns is known as singular.\nIf $A$ is not square or is square but singular, it can still be possible to solve the equation. However, we can not use the method of matrix inversion to find the solution.\nNorms Sometimes we need to measure the size of a vector. In machine learning, we usually measure the size of vectors using a function called a norm. Formally, the $L^{p}$ norm is given by\n$$ \\lVert x \\rVert_{p} = \\left(\\sum_{i}|x_{i}|^{p}\\right)^{\\frac{1}{p}} \\\\ $$\nfor $p \\in \\mathbb{R}$, $p\u0026gt;1$.\n 我们经常使用被称为范数 (norm) 的函数衡量向量大小. 范数是将向量映射到非负值的函数. 直观上来说, 向量 $x$ 的范数衡量从原点到点 $x$ 的距离.\n Norms, including the $L^{p}$ norm, are functions mapping vectors to non-negative values. On an intuitive level, the norm of a vector $x$ measures the distance from the origin to the point $x$. More rigorously, a norm is any function $f$ that satisfies the following properties:\n $f(x) = 0 \\Rightarrow x = 0$ $f(x+y) \\leq f(x) + f(y)$ (the triangle inequality) $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha x) = |\\alpha|f(x)$   范数 (norm) 满足以下性质：\n $f(x) = 0 \\Rightarrow x = 0$  $f(x+y) \\leq f(x) + f(y)$ (满足三角不等式, 或称次可加性)\n $\\forall \\alpha \\in \\mathbb{R}, f(\\alpha x) = |\\alpha|f(x)$ (具有绝对一次齐次性)\n   The $L^{2}$ norm, with $p=2$, is known as the Euclidean norm. It is simply the Euclidean distance from the origin to the point identified by $x$. The $L^{2}$ norm is used so frequently in machine learning that it is often denoted simply as $\\lVert x \\rVert$, with the subscript 2 omitted. It is also common to measure the size of a vector using the squared $L^{2}$ norm, which can be calculated simply as $x^{T}x$.\n$$ \\lVert x \\rVert_{2} = \\sqrt{\\sum_{i}|x_{i}|^{2}} \\\\ $$\n 当 $p=2$ 时，$L^{2}$ 范数被称为欧几里得范数(Euclidean norm) .它表示从原点出发到向量 $x$ 确定的点的欧几里得距离.\n The $L^{1}$ norm is commonly used in machine learning when the difference between zero and nonzero elements is very important. Every time an element of $x$ moves away from 0 by $\\varepsilon$, the $L^{1}$ norm increases by $\\varepsilon$. The $L^{1}$ norm may be simplified to\n$$ \\lVert x \\rVert_{1} = \\sum_{i}|x_{i}| \\\\ $$\nOne other norm that commonly arises in machine learning is the $L^{\\infty}$ norm, also known as the max norm. This norm simplifies to the absolute value of the element with the largest magnitude in the vector,\n$$ \\lVert x \\rVert_{\\infty} = max_{i}|x_{i}| \\\\ $$\nSometimes we may also wish to measure the size of a matrix. In the context of deep learning, the most common way to do this is with the otherwise obscure Frobenius norm:\n$$ \\lVert x \\rVert_{F} = \\sqrt{\\sum_{i,j}A_{i,j}^{2}} \\\\ $$\nThe dot product of two vectors can be rewritten in terms of norms. Specifically,\n$$ x^{T}y = \\lVert x \\rVert_{2}\\lVert y \\rVert_{2} \\cos{\\theta} \\\\ $$\nwhere $\\theta$ is the angle between $x$ and $y$.\nSpecial Kinds of Matrices and Vectors Some special kinds of matrices and vectors are particularly useful.\nDiagonal matrices consist mostly of zeros and have non-zero entries only along the main diagonal. Formally, a matrix $D$ is diagonal if and only if $D_{i,j}=0$ for all $i \\neq j$. We have already seen one example of a diagonal matrix: the identity matrix, where all of the diagonal entries are 1. We write $diag(v)$ to denote a square diagonal matrix whose diagonal entries are given by the entries of the vector $v$, e.g.,\n$$ v = \\begin{bmatrix} a_{1} \\\\ a_{2} \\\\ a_{3} \\\\ \\end{bmatrix} \\Rightarrow diag(v) = \\begin{bmatrix} a_{1} \u0026amp; 0 \u0026amp; 0 \\\\ 0 \u0026amp; a_{2} \u0026amp; 0 \\\\ 0 \u0026amp; 0 \u0026amp; a_{3} \\\\ \\end{bmatrix} $$\nDiagonal matrices are of interest in part because multiplying by a diagonal matrix is very computationally efficient. To compute $diag(v)x$, we only need to scale each element $x_{i}$ by $v_{i}$. In other words, $diag(v)x = v \\odot x$. Inverting a square diagonal matrix is also efficient. The inverse exists only if every diagonal entry is nonzero, $diag(v)^{-1}= diag([\\frac{1}{v_{1}}, \\cdots, \\frac{1}{v_{n}}]^{T})$.\n 对角矩阵(diagonal matrix) 只在主对角线上含有非零元素, 其他位置都是零.\n A symmetric matrix is any matrix that is equal to its own transpose:\n$$ A = A^{T} \\\\ $$\nFor example,\n$$ A = A^{T} = \\begin{bmatrix} 1 \u0026amp; 2 \\\\ 2 \u0026amp; 3 \\\\ \\end{bmatrix} \\ \\ \\ \\ \\ \\ \\ \\ B = B^{T} = \\begin{bmatrix} 5 \u0026amp; 6 \u0026amp; 7 \\\\ 6 \u0026amp; 3 \u0026amp; 2 \\\\ 7 \u0026amp; 2 \u0026amp; 1 \\\\ \\end{bmatrix} $$\nSymmetric matrices often arise when the entries are generated by some function of two arguments that does not depend on the order of the arguments. For example, if $A$ is a matrix of distance measurements, with $A_{i,j}$ giving the distance from point $i$ to point $j$, then $A_{i,j} = A_{j,i}$ because distance functions are symmetric.\n 对称矩阵(symmetric matrix) 是转置和自己相等的矩阵.\n A unit vector is a vector with unit norm:\n$$ \\lVert x \\rVert_{2} = 1 \\\\ $$\nA vector $x$ and a vector $y$ are orthogonal to each other if $x^{T}y=0$. If both vectors have nonzero norm, this means that they are at a 90 degree angle to each other. In $\\mathbb{R}^{n}$, at most $n$ vectors may be mutually orthogonal with nonzero norm. If the vectors are not only orthogonal but also have unit norm, we call them orthonormal.\n 单位向量(unit vector) 是具有单位范数(unit norm) 的向量. 如果 $x^{T}y=0$，那么向量 $x$ 和向量 $y$ 互相正交(orthogonal) . 如果两个向量都有非零范数, 那么这两个向量之间的夹角是90度. 如果这些向量不仅互相正交，并且范数都为 1，那么我们称它们是标准正交(orthonormal) .\n An orthogonal matrix is a square matrix whose rows are mutually orthonormal and whose columns are mutually orthonormal:\n$$ A^{T}A = AA^{T} = I \\\\ $$\nThis implies that\n$$ A^{-1} = A^{T} \\\\ $$\nso orthogonal matrices are of interest because their inverse is very cheap to compute. Pay careful attention to the definition of orthogonal matrices. Counterintuitively, their rows are not merely orthogonal but fully orthonormal. There is no special term for a matrix whose rows or columns are orthogonal but not orthonormal.\nEigendecomposition Much as we can discover something about the true nature of an integer by decomposing it into prime factors, we can also decompose matrices in ways that show us information about their functional properties that is not obvious from the representation of the matrix as an array of elements.\nOne of the most widely used kinds of matrix decomposition is called eigendecomposi-tion, in which we decompose a matrix into a set of eigenvectors and eigenvalues.\nAn eigenvector of a square matrix $A$ is a non-zero vector $v$ such that multiplication by $A$ alters only the scale of $v$:\n$$ Av = \\lambda v \\\\ $$\nThe scalar $\\lambda$ is known as the eigenvalue corresponding to this eigenvector. If $v$ is an eigenvector of $A$, then so is any rescaled vector $sv$ for $s \\in \\mathbb{R}$, $s \\neq 0$. Moreover, $sv$ still has the same eigenvalue. For this reason, we usually only look for unit eigenvectors.\n 特征分解(eigendecomposition) 是使用最广的矩阵分解之一, 即我们将矩阵分解成一组特征向量和特征值. 方阵 $A$ 的特征向量(eigenvector) 是指与 $A$ 相乘后相当于对该向量进行缩放的非零向量 $v$: $$Av = \\lambda v \\\\$$ 标量 $\\lambda$ 被称为这个特征向量对应的特征值(eigenvalue) .\n Suppose that a matrix $A$ has $n$ linearly independent eigenvectors, $\\{v^{(1)}, \\cdots, v^{(n)}\\}$, with corresponding eigenvalues $\\{\\lambda_{1}, \\cdots, \\lambda_{n}\\}$. We may concatenate all of the eigenvectors to form a matrix $V$ with one eigenvector per column: $V=[v^{(1)}, \\cdots, v^{(n)}]$. Likewise, we can concatenate the eigenvalues to form a vector $\\lambda = [\\lambda_{1}, \\cdots, \\lambda_{n}]^{T}$. The eigendecomposi-tion of $A$ is then given by\n$$ A = V diag(\\lambda)V^{-1} \\\\ $$\n $A$的特征分解(eigendecomposition) 可以记作: $$A = V diag(\\lambda)V^{-1} $$\n We have seen that constructing matrices with specific eigenvalues and eigenvectors allows us to stretch space in desired directions. However, we often want to decompose matrices into their eigenvalues and eigenvectors. Doing so can help us to analyze certain properties of the matrix, much as decomposing an integer into its prime factors can help us understand the behavior of that integer. Not every matrix can be decomposed into eigenvalues and eigenvectors. In some cases, the decomposition exists, but may involve complex rather than real numbers.\nSingular Value Decomposition We already known how to decompose a matrix into eigenvectors and eigenvalues. The singular value decomposition (SVD) provides another way to factorize a matrix, into singular vectors and singular values. The SVD allows us to discover some of the same kind of information as the eigendecomposition. However, the SVD is more generally applicable. Every real matrix has a singular value decomposition, but the same is not true of the eigenvalue decomposition. For example, if a matrix is not square, the eigendecom-position is not defined, and we must use a singular value decomposition instead.\nRecall that the eigendecomposition involves analyzing a matrix $A$ to discover a matrix $V$ of eigenvectors and a vector of eigenvalues $\\lambda$ such that we can rewrite $A$ as\n$$ A = V diag(\\lambda)V^{-1} \\\\ $$\nThe singular value decomposition is similar, except this time we will write $A$ as a product of three matrices:\n$$ A = UDV^{T} \\\\ $$\nSuppose that $A$ is an $m \\times n$ matrix. Then $U$ is defined to be an $m \\times m$ matrix, $D$ to be an $m \\times n$ matrix, and $V$ to be an $n \\times n$ matrix.\n 奇异值分解(singular value decomposition, SVD), 将矩阵分解为奇异向量(singular vector) 和奇异值(singular value) : $$A = UDV^{T} \\\\$$ 如果 $A$ 是一个 $m \\times n$ 的矩阵. 那么 $U$ 是一个 $m \\times m$ 的矩阵, $D$ 是一个 $m \\times n$ 的矩阵, $V$ 是一个 $n \\times n$ 的矩阵.\n Each of these matrices is defined to have a special structure. The matrices $U$ and $V$ are both defined to be orthogonal matrices. The matrix $D$ is defined to be a diagonal matrix. Note that $D$ is not necessarily square.\nThe elements along the diagonal of $D$ are known as the singular values of the matrix $A$. The columns of $U$ are known as the left-singular vectors. The columns of $V$ are known as as the right-singular vectors.\nThe Trace Operator The trace operator gives the sum of all of the diagonal entries of a matrix:\n$$ Tr(A) = \\sum_{i}A_{i,i} \\\\ $$\n 迹运算(trace operator) 返回的是矩阵对角元素的和.\n For example, let $A$ be a matrix, with\n$$ A = \\begin{bmatrix} a_{1,1} \u0026amp; a_{1,2} \\\\ a_{2,1} \u0026amp; a_{2,2} \\\\ \\end{bmatrix} $$\nThen,\n$$ Tr(A) = \\sum_{i=1}^{2}a_{i,i} = a_{1,1} + a_{2,2} $$\nThe trace operator is useful for a variety of reasons. Some operations that are difficult to specify without resorting to summation notation can be specified using matrix products and the trace operator. For example, the trace operator provides an alternative way of writing the Frobenius norm of a matrix:\n$$ \\lVert x \\rVert_{F} = \\sqrt{Tr(AA^{T})} \\\\ $$\nWriting an expression in terms of the trace operator opens up opportunities to manipulate the expression using many useful identities. For example, the trace operator is invariant to the transpose operator:\n$$ Tr(A) = Tr(A^{T}) \\\\ $$\nThe trace of a square matrix composed of many factors is also invariant to moving the last factor into the first position, if the shapes of the corresponding matrices allow the resulting product to be defined:\n$$ Tr(ABC) = Tr(CAB) = Tr(BCA) \\\\ $$\nThis invariance to cyclic permutation holds even if the resulting product has a different shape. For example, for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times m}$, we have\n$$ Tr(AB) = Tr(BA) \\\\ $$\neven though $AB \\in \\mathbb{R}^{m \\times m}$ and $BA \\in \\mathbb{R}^{n \\times n}$\nAnother useful fact to keep in mind is that a scalar is its own trace: $a = Tr(a)$.\nThe Determinant The determinant of a square matrix, denoted $det(A)$, is a function mapping matrices to real scalars. The determinant is equal to the product of all the eigenvalues of the matrix. The absolute value of the determinant can be thought of as a measure of how much multiplication by the matrix expands or contracts space. If the determinant is 0, then space is contracted completely along at least one dimension, causing it to lose all of its volume. If the determinant is 1, then the transformation preserves volume.\n 行列式(determinant) 是一个将方阵映射到实数的函数. 行列式等于矩阵特征值的乘积.\n For example, for a $2 \\times 2$ matrix:\n$$ A = \\begin{bmatrix} a \u0026amp; b \\\\ c \u0026amp; d \\\\ \\end{bmatrix} $$\nThe determinant is:\n$$ det(A) = |A| = ad - bc $$\nJacobian and Hessian Matrices Sometimes we need to find all of the partial derivatives of a function whose input and output are both vectors. The matrix containing all such partial derivatives is known as a Jacobian matrix. Specifically, if we have a function $f: \\mathbb{R}^{m} \\to \\mathbb{R}^{n}$, then the Jacobian matrix $J \\in \\mathbb{R}^{n \\times m}$ of $f$ is defined such that $J_{i,j}=\\frac{\\partial}{\\partial x_{j}}f(x)_{i}$.\n$$ J = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f}{\\partial x_{n}} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\nabla^{T}f_{1} \\\\ \\vdots \\\\ \\nabla^{T}f_{m} \\\\ \\end{bmatrix} = \\begin{bmatrix} \\frac{\\partial f_{1}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{1}}{\\partial x_{n}} \\\\ \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial f_{m}}{\\partial x_{1}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial f_{m}}{\\partial x_{n}} \\\\ \\end{bmatrix} \\\\ $$\n 有时我们需要计算输入和输出都为向量的函数的所有偏导数.包含所有这样的偏导数的矩阵被称为 Jacobian 矩阵.\n We are also sometimes interested in a derivative of a derivative. This is known as a second derivative. For example, for a function $f: \\mathbb{R}^{n} \\to \\mathbb{R}$, the derivative with respect to $x_{i}$ of the derivative of $f$ with respect to $x_{j}$ is denoted as $\\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f$. In a single dimension, we can denote $\\frac{d^{2}}{dx^{2}}f$ by $f''(x)$. The second derivative tells us how the first derivative will change as we vary the input. This is important because it tells us whether a gradient step will cause as much of an improvement as we would expect based on the gradient alone. We can think of the second derivative as measuring curvature.\nWhen our function has multiple input dimensions, there are many second derivatives. These derivatives can be collected together into a matrix called the Hessian matrix. The Hessian matrix $H(f)(x)$ is defined such that $H(f)(x)_{i,j} = \\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(x)$\n$$ H(f)(x)_{i,j} = \\begin{bmatrix} \\frac{\\partial^{2} f}{\\partial x_{1}^{2}} \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{1}\\partial x_{n}} \\\\ \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{1}} \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{2}^{2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{2}\\partial x_{n}} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ \\frac{\\partial^{2} f}{\\partial x_{n}\\partial x_{1}} \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{n}\\partial x_{2}} \u0026amp; \\cdots \u0026amp; \\frac{\\partial^{2} f}{\\partial x_{n}^{2}} \\\\ \\end{bmatrix} \\\\ $$\nEquivalently, the Hessian is the Jacobian of the gradient.\n 当我们的函数具有多维输入时,二阶导数也有很多.我们可以将这些导数合并成一个矩阵,称为 Hessian 矩阵.\n Anywhere that the second partial derivatives are continuous, the differential operators are commutative, i.e. their order can be swapped:\n$$ \\frac{\\partial^{2}}{\\partial x_{i}\\partial x_{j}}f(x)=\\frac{\\partial^{2}}{\\partial x_{j}\\partial x_{i}}f(x) \\\\ $$\nThis implies that $H_{i,j}=h_{j,i}$, so the Hessian matrix is symmetric at such points. Most of the functions we encounter in the context of deep learning have a symmetric Hessian almost everywhere. Because the Hessian matrix is real and symmetric, we can decompose it into a set of real eigenvalues and an orthogonal basis of eigenvectors. The second derivative in a specific direction represented by a unit vector $f$ is given by $d^{T}Hd$. When $d$ is an eigenvector of $H$ , the second derivative in that direction is given by the corresponding eigenvalue. For other directions of $d$, the directional second derivative is a weighted average of all of the eigenvalues, with weights between 0 and 1, and eigenvectors that have smaller angle with $d$ receiving more weight. The maximum eigenvalue determines the maximum second derivative and the minimum eigenvalue determines the minimum second derivative.\nReference [1] Goodfellow, I., Bengio, Y., \u0026amp; Courville, A. (2016, Nov 18). Deep Learning. https://www.deeplearningbook.org/contents/linear_algebra.html.\n","permalink":"https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/01_linear_algebra_for_ml/","summary":"Scalars, Vectors, Matrices and Tensors The study of linear algebra involves several types of mathematical objects:\n  Scalars: A scalar is just a single number, in contrast to most of the other objects studied in linear algebra, which are usually arrays of multiple numbers. We write scalars in italics. We usually give scalars lowercase variable names. When we introduce them, we specify what kind of number they are. For example, we might say \u0026ldquo;Let $s \\in \\mathbb{R}$ be the slope of the line,\u0026rdquo; while defining a real-valued scalar, or \u0026ldquo;Let $n \\in \\mathbb{N}$ be the number of units, while defining an natural number scalar.","title":"[ML Basics] Linear Algebra"},{"content":"Boosting is an ensemble modeling technique which attempts to build a strong classifier from the number of weak classifiers. It is done by building a model using weak models in series. First, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added.\nBoosting being a sequential process, each subsequent model attempts to correct the errors of the previous model. It is focused on reducing the bias unlike bagging. It makes the boosting algorithms prone to overfitting. To avoid overfitting, parameter tuning plays an important role in boosting algorithms.\n A Weak Classifier is one whose error rate is only slightly better than random guessing. Theoretically a weak classifier can be boosted to perform pretty well. To find weak learners, we apply base learning (ML) algorithms with a different distribution. As each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\nBoosting combines many weak classifiers to produce one powerful \u0026ldquo;committee\u0026rdquo;. The weak classifiers do not have equal weight. For classification into two categories labeled $\\{-1,1\\}$,\n$$ f(x) = \\mathrm{sign}[\\sum_{j=1}^{m}\\alpha_{j}h_{j}(x)] \\\\ $$\nwhere $h_{j}(x)$ is a weak learner and $\\alpha_{j}$ are weights.\n Boosting 和 bagging 最本质的差别在于他对基础模型不是一致对待的，而是经过不停的考验和筛选来挑选出「精英」，然后给精英更多的投票权，表现不好的基础模型则给较少的投票权，然后综合所有人的投票得到最终结果。大部分情况下，经过 boosting 得到的结果偏差(bias)更小。\n AdaBoost (for classification) Adaptive Boosting, or most commonly known AdaBoost, is a Boosting algorithm. This algorithm uses the method to correct its predecessor. It pays more attention to under fitted training instances by the previous model. Thus, at every new predictor the focus is more on the complicated cases more than the others.\nIt fits a sequence of weak learners on different weighted training data. It starts by predicting the original data set and gives equal weight to each observation. If prediction is incorrect using the first learner, then it gives higher weight to observation which have been predicted incorrectly. Being an iterative process, it continues to add learner(s) until a limit is reached in the number of models or accuracy.\nAdaBoost Algorithm $$ \\{(x_{i},y_{i})\\}_{i=1}^{n}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ y_{i}=\\{-1,1\\}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ f(x) = \\mathrm{sign}[\\sum_{j=1}^{m}\\alpha_{j}h_{j}(x)] $$\n  Initialize weights $w_{i}=\\frac{1}{n}, \\ i = 1,2,..,n$\n  For$\\ j=1$ to $m$:\n a) Fit a classifier $h_{j}(x)$ to the training data b) Compute $err_{j}=L_{j}=\\frac{\\sum_{i=1}^{n}w_{i}I(y_{i}\\neq h(x_{i}))}{\\sum_{i=1}^{n}w_{i}}$ c) Compute $\\alpha_{j}=log(\\frac{1-L_{j}}{L_{j}})$ d) Set $w_{i} := w_{i}\\exp[\\alpha_{j}I(y_{i}\\neq h(x_{j}))]$    Final classification\n  $$ h(x_{i}) =\\mathrm{sign}[\\sum_{j=1}^{m}\\alpha_{j}h_{j}(x)] $$\n If classified correctly, the weight of an observation remains unchanged. If classified incorrectly, the weight is increased by multiplying $\\exp(\\alpha_{j})$ alpha varies with the degree of misclassification  Additive Model Generally, boosting fits an additive model\n$$ f(x)=\\sum_{j=1}^{m}\\beta_{j}\\phi_{j}(x) $$\nWhere $\\phi_{j}(x)$ are basis functions (weak learners).\nAdaboost is restricted to 2‐class classification, boosting is not. To fit the model a loss function has to be minimized:\n$$ \\min_{\\beta_{j},\\gamma_{j}}L(y,f(x))=\\sum_{i=1}^{n}L(y_{i},\\sum_{j=1}^{m}\\beta_{j}\\phi_{j}(x_{i},\\gamma_{j})) $$\nFinding optimal coefficients for all $m$ iterations simultaneously is difficult. We therefore simplify the problem. At each iteration, we find the best fit to the “residual” from the previous iteration.\n What \u0026ldquo;best\u0026rdquo; means depends on the loss function Definition of \u0026ldquo;residual\u0026rdquo; depends on the loss function  Sequential fit: values from earlier iterations are never changed.\nForward Stagewise Additive Modeling Algorithm   Initial $f_{0}(x)$\n  For $j=1$ to $m$. Add to the existing model such that the loss function is minimized.\n a) Minimize the loss $\\sum_{i=1}^{n}L[f_{j-1}(x_{i})+\\beta_{j}\\phi_{j}(x_{i},\\gamma_{j})]$ b) Update the function $f_{j}(x)=f_{j-1}(x)+\\beta_{j}\\phi_{j}(x,\\gamma_{j})$    One can show that Adaboost is a forward stagewise additive model using an exponential loss function\n$$ \\mathrm{Loss}(y,h(x)) = \\exp(-y\\times h(x)) $$\nGradient Boosting Gradient Boosting is another very popular Boosting algorithm which works pretty similar to what we’ve seen for AdaBoost. Gradient Boosting works by sequentially adding the previous predictors underfitted predictions to the ensemble, ensuring the errors made previously are corrected.\nThe difference lies in what it does with the underfitted values of its predecessor. Contrary to AdaBoost, which tweaks the instance weights at every interaction, this method tries to fit the new predictor to the residual errors made by the previous predictor.\nGradient Boosting Algorithm:  A model is built on a subset of data. Using this model, predictions are made on the whole dataset. Errors are calculated by comparing the predictions and actual values. A new model is created using the errors calculated as target variable. Our objective is to find the best split to minimize the error. The predictions made by this new model are combined with the predictions of the previous. New errors are calculated using this predicted value and actual value. This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached.  XGBoost XG Boost or Extreme Gradient Boosting is an advanced implementation of the Gradient Boosting. This algorithm has high predictive power and is ten times faster than any other gradient boosting techniques. Moreover, it includes a variety of regularization which reduces overfitting and improves overall performance.\nAdvantages of XGBoost  It implements regularization which helps in reducing overfit (Gradient Boosting does not have); It implements parallel processing which is much faster than Gradient Boosting; Allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model; XGBoost has an in-built routine to handle missing values; XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain; XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run.  Reference [1] Sarkar, P., S, A., \u0026amp; Shah, P. (2019, October 22). What is Boosting and AdaBoost in Machine Learning? Knowledgehut. https://www.knowledgehut.com/blog/data-science/boosting-and-adaboost-in-machine-learning.\n[2] Navlani, A. (2018, November 18). AdaBoost Classifier in Python. DataCamp Community. https://www.datacamp.com/community/tutorials/adaboost-classifier-python.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/12_boosting/","summary":"Boosting is an ensemble modeling technique which attempts to build a strong classifier from the number of weak classifiers. It is done by building a model using weak models in series. First, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added.","title":"Boosting"},{"content":"Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.\nAn ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model.When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error). The noise-related error is mainly due to noise in the training data and can\u0026rsquo;t be removed. However, the errors due to bias and variance can be reduced. The total error can be expressed as follows:\n$$ \\mathrm{Total \\ Error} = \\mathrm{Bias} + \\mathrm{Variance} + \\mathrm{Irreducible \\ Error} $$\nA measure such as mean square error (MSE) captures all of these errors for a continuous target variable and can be represented as follows:\n$$ \\begin{align*} \\mathrm{MSE} \u0026amp;= E[(Y-\\hat{f}(x))^{2}] \\\\ \u0026amp;= \\underbrace{[E[\\hat{f}(x)]-f(x)]^{2}}_{\\mathrm{Bias}} + \\underbrace{E[(\\hat{f}(x)-E[\\hat{f}(x)])^{2}]}_{\\mathrm{Variance}} + \\underbrace{\\varepsilon}_{\\mathrm{Noise}} \\\\ \\end{align*} $$\nUsing techniques like Bagging and Boosting helps to decrease the variance and increase the robustness of the model. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.\nEnsemble Algorithm The goal of ensemble algorithms is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n There are two families of ensemble methods which are usually distinguished:\n Averaging methods. The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.   Examples: Bagging methods, Forests of randomized trees.  Boosting methods. Base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.   Examples: AdaBoost, Gradient Tree Boosting.  Advantages of Ensemble Algorithm  Ensemble is a proven method for improving the accuracy of the model and works in most of the cases. Ensemble makes the model more robust and stable thus ensuring decent performance on the test cases in most scenarios. You can use ensemble to capture linear and simple as well nonlinear complex relationships in the data. This can be done by using two different models and forming an ensemble of two.  Disadvantages of Ensemble Algorithm  Ensemble reduces the model interpret-ability and makes it very difficult to draw any crucial business insights at the end It is time-consuming and thus might not be the best idea for real-time applications The selection of models for creating an ensemble is an art which is really hard to master  Bootstrap Sampling Sampling is the process of selecting a subset of observations from the population with the purpose of estimating some parameters about the whole population. Resampling methods, on the other hand, are used to improve the estimates of the population parameters. In machine learning, the bootstrap method refers to random sampling with replacement. This sample is referred to as a resample. This allows the model or algorithm to get a better understanding of the various biases, variances and features that exist in the resample. Taking a sample of the data allows the resample to contain different characteristics then it might have contained as a whole.\nBootstrapping is also great for small size data sets that can have a tendency to overfit. In fact, we recommended this to one company who was concerned because their data sets were far from “Big Data”. Bootstrapping can be a solution in this case because algorithms that utilize bootstrapping can be more robust and handle new data sets depending on the methodology chosen(boosting or bagging). The reason behind using the bootstrap method is because it can test the stability of a solution. By using multiple sample data sets and then testing multiple models, it can increase robustness. Perhaps one sample data set has a larger mean than another, or a different standard deviation. This might break a model that was overfit, and not tested using data sets with different variations.\nOne of the many reasons bootstrapping has become very common is because of the increase in computing power. This allows for many times more permutations to be done with different resamples than previously. Bootstrapping is used in both Bagging and Boosting.\n Assume we have a sample of $n$ values (x) and we’d like to get an estimate of the mean of the sample.\n$$ \\mathrm{mean}(x) = \\frac{1}{n} \\times \\mathrm{sum}(x) \\\\ $$\nConsider a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample. We know that our sample is small and that the mean has an error in it. We can improve the estimate of our mean using the bootstrap procedure:\n Create many (e.g. 1000) random sub-samples of the data set with replacement (meaning we can select the same value multiple times). Calculate the mean of each sub-sample Calculate the average of all of our collected means and use that as our estimated mean for the data  Bagging Bootstrap AGgregation, also known as Bagging, is a powerful ensemble method that was proposed by Leo Breiman in 1994 to prevent overfitting. The concept behind bagging is to combine the predictions of several base learners to create a more accurate output. Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.\n  Suppose there are N observations and M features. A sample from observation is selected randomly with replacement (Bootstrapping). A subset of features are selected to create a model with sample of observations and subset of features. Feature from the subset is selected which gives the best split on the training data. This is repeated to create many models and every model is trained in parallel Prediction is given based on the aggregation of predictions from all the models.  This approach can be used with machine learning algorithms that have a high variance, such as decision trees. A separate model is trained on each bootstrap sample of data and the average output of those models used to make predictions. This technique is called bootstrap aggregation or bagging for short. Variance means that an algorithm’s performance is sensitive to the training data, with high variance suggesting that the more the training data is changed, the more the performance of the algorithm will vary.\nThe performance of high variance machine learning algorithms like unpruned decision trees can be improved by training many trees and taking the average of their predictions. Results are often better than a single decision tree. What Bagging does is help reduce variance from models that are might be very accurate, but only on the data they were trained on. This is also known as overfitting. Bagging gets around this by creating its own variance amongst the data by sampling and replacing data while it tests multiple hypothesis(models). In turn, this reduces the noise by utilizing multiple samples that would most likely be made up of data with various attributes(median, average, etc).\nOnce each model has developed a hypothesis. The models use voting for classification or averaging for regression. This is where the \u0026ldquo;Aggregating\u0026rdquo; in \u0026ldquo;Bootstrap Aggregating\u0026rdquo; comes into play. Each hypothesis has the same weight as all the others. Essentially, all these models run at the same time, and vote on which hypothesis is the most accurate. This helps to decrease variance i.e. reduce the overfit.\n Bagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。大部分情况下，经过 bagging 得到的结果方差（variance）更小。\n Baging for Classification Consider a classification problem with $K$ classes. When averaging over the $B$ bootstrap samples, there are two options and conflicting advice\n  Option 1: use the majority of votes (consensus estimate)\n This is the only option mentioned in James et al.    Option 2: average the proportions from the individual classifiers and choose class with greatest average proportion\n The bagged predictor is the class with the largest average proportion:    $$ \\hat{G}_{bag}(x) = \\arg\\max_{k} \\hat{f}_{k}(x) \\\\ $$\nWhere $\\hat{f}_{k}$ is the average proportion for class $k$\n$$ \\hat{f}_{k} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{p}_{b}(x) \\\\ $$ Where $p_{b}$ is the proportion in the corresponding terminal leaf for bag $b$.\nAdvantages of Bagging  Bagging takes advantage of ensemble learning wherein multiple weak learners outperform a single strong learner. It helps reduce variance and thus helps us avoid overfitting.  Disadvantages of Bagging  There is loss of interpretability of the model. There can possibly be a problem of high bias if not modeled properly. While bagging gives us more accuracy, it is computationally expensive and may not be desirable depending on the use case.  Random Forest Random forest is a supervised learning algorithm. The \u0026ldquo;forest\u0026rdquo; it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n Bagging + 决策树(Decision Tree) = 随机森林(Random Forest)\n The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random:\n Random sampling of training data points when building trees Random subsets of features considered when splitting nodes   The basic steps involved in performing the random forest algorithm are mentioned below:\n Pick N random records from the dataset. Build a decision tree based on these N records. Choose the number of trees you want in your algorithm and repeat steps 1 and 2. In case of a regression problem, for a new record, each tree in the forest predicts a value for Y (output). The final value can be calculated by taking the average of all the values predicted by all the trees in the forest. Or, in the case of a classification problem, each tree in the forest predicts the category to which the new record belongs. Finally, the new record is assigned to the category that wins the majority vote.  Advantages of Random Forest  Random forest algorithm is unbiased as there are multiple trees and each tree is trained on a subset of data. Random Forest algorithm is very stable. Introducing a new data in the dataset does not affect much as the new data impacts one tree and is pretty hard to impact all the trees. The random forest algorithm works well when you have both categorical and numerical features. With missing values in the dataset, the random forest algorithm performs very well.  Disadvantages of Random Forest  A major disadvantage of random forests lies in their complexity. More computational resources are required and also results in the large number of decision trees joined together. Due to their complexity, training time is more compared to other algorithms.  Reference [1] VanderPlas, J. (n.d.). In-Depth: Decision Trees and Random Forests. In-Depth: Decision Trees and Random Forests | Python Data Science Handbook. https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html.\n[2] Sarkar, P., S, A., \u0026amp; Shah, P. (2019, October 14). Bagging and Random Forest in Machine Learning: How do they work? Knowledgehut. https://www.knowledgehut.com/blog/data-science/bagging-and-random-forest-in-machine-learning.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/11_bagging_and_random_forest/","summary":"Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.\nAn ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier.","title":"Bagging and Random Forest"},{"content":"A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. Though a commonly used tool in data \\mining for deriving a strategy to reach a particular goal, its also widely used in machine learning.\n Decision Tree consists of :\n Nodes : Test for the value of a certain attribute. Edges/ Branch : Correspond to the outcome of a test and connect to the next node or leaf. Leaf nodes : Ter\\minal nodes that predict the outcome (represent class labels or class distribution).  Applications of Decision trees in real life :\n Biomedical Engineering (decision trees for identifying features to be used in implantable devices). Financial analysis (Customer Satisfaction with a product or service). Astronomy (classify galaxies). System Control. Manufacturing and Production (Quality control, Semiconductor manufacturing, etc). Medicines (diagnosis, cardiology, psychiatry). Physics (Particle detection).  Classification and Regression Trees (CART) Classification and regression trees are machine-learning methods for constructing prediction models from data. The models are obtained by recursively partitioning the data space and fitting a simple prediction model within each partition. As a result, the partitioning can be represented graphically as a decision tree. Classification trees are designed for dependent variables that take a finite number of unordered values, with prediction error measured in terms of misclassification cost. Regression trees are for dependent variables that take continuous or ordered discrete values, with prediction error typically measured by the squared difference between the observed and predicted values.\n Consider the data would be:\n$$ \\{(x_{i},y_{i})\\}_{i=1}^{n} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ x = \\begin{bmatrix} x_{1},x_{2},...,x_{n} \\\\ \\end{bmatrix}_{\\ d \\times n}\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ y = \\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ ... \\\\ y_{n} \\\\ \\end{bmatrix}_{\\ n \\times 1} $$\nDefine:\n $x_{j:}\\to j'th$ row and all columns. (each row of $x$ matrix is a feature) $x_{:i}\\to$ one column and all rows. (each column of $x$ matrix is a data sample)  At each split we have two regions.\n$$ R_{1}(j,s) = \\{ x_{:i}|x_{j:}\\leq s \\} \\\\ R_{2}(j,s) = \\{ x_{:i}|x_{j:}\u0026gt; s \\} \\\\ $$\nWe need to decide about $j$ and $s$. To decide about these parameters we can define an objective function.\nRegression Case For simplicity, consider the model we want to fit for each region be a constant value. Therefore in region one for example, we want all the target values to be as close as possible to constant $C_{1}$.\n$$ \\min_{C_{1}}\\sum_{y_{i}\\in R_{1}}(y_{i}-C_{1})^{2} \\\\ $$\nSince in each split we have two regions, we can write similar objective for region two.\n$$ \\min_{C_{2}}\\sum_{y_{i}\\in R_{2}}(y_{i}-C_{2})^{2} \\\\ $$\nTherefore we have:\n$$ \\min_{C_{1}}\\sum_{y_{i}\\in R_{1}}(y_{i}-C_{1})^{2}+\\min_{C_{2}}\\sum_{y_{i}\\in R_{2}}(y_{i}-C_{2})^{2} \\\\ $$\nFinally we should also decide about $j$ and $s$:\n$$ \\min_{j,s}[\\min_{C_{1}}\\sum_{y_{i}\\in R_{1}}(y_{i}-C_{1})^{2}+\\min_{C_{2}}\\sum_{y_{i}\\in R_{2}}(y_{i}-C_{2})^{2}] \\\\ $$\nIf $j$ and $s$ are known\n$$ C_{1} = \\mathrm{Average}\\{x_{:i}|x_{:i}\\in R_{1} \\} \\\\ C_{2} = \\mathrm{Average}\\{x_{:i}|x_{:i}\\in R_{2} \\} \\\\ $$\nGiven $j$, we have only $n$ possible choice for $s$\n$$ x_{j} = \\begin{bmatrix} x_{j1},x_{j2},...,x_{jn} \\\\ \\end{bmatrix} $$\nOne (trivial)solution for this problem can be a brute force search.\n Consider feature $j$ each time Test all $n$ possible choices for $s$ Find $C_{1}$ and $C_{2}$ for considered $s$ and $j$ Repeat until finding \\minimum value for objective function  Define\n$$ Q_{m}(T)=\\frac{1}{n_{m}}\\sum_{y_{i}\\in R_{m}}(y_{i}-C_{m})^{2} $$\nWhere:\n $m$ is a ter\\minal node of the tree. $R_{m}$ is a region. $n_{m}$ is the number of points in $R_{m}$. $C_{m}$ is the constant that we fit in $R_{m}$. |$T$| is the number of ter\\minal nodes.  Therefore the total error would be:\n$$ \\sum_{m=1}^{|T|}n_{m}Q_{m} \\\\ $$\nStopping criterion\n Option 1 (bad): split only if the split reduces the residual sum of squares (RSS) by at least some threshold value  However, sometimes you have a split with a small improvement followed by one with a large improvement. This stopping criterion would miss such splits   Option 2: over‐build the tree and \u0026ldquo;prune\u0026rdquo; the tree later.  Pruning refers to removing some splits that create ter\\minal leaves    Tree Pruning After a tree is fully extended, remove one leaf at a time to \\minimize this criterion:\n$$ \\sum_{m=1}^{|T|}n_{m}Q_{m} + \\alpha|T| $$\nWhere\n $\\alpha|T|$ is the penalty for having too many leaves $\\alpha$ is a tuning parameter and can be chosen via cross validation Introducing the penalty is also called regularization   Classification Case In regression case, we tried to fit a constant to a region. But in classification case we need to find the regions which contain samples from same class. We can change regression cost with some sort of misclassification cost.\n Consider the classes of $$C = \\{1,2,3,...,𝑘\\} \\\\ $$ Impurity: $$P_{mk}=\\frac{1}{n_{m}}\\sum_{y_{i}\\in R_{m}}I(y_{i}=k) \\\\ $$ Misclassification: $$\\frac{1}{n_{m}}\\sum_{y_{i}\\in R_{m}}I(y_{i}\\neq k) = 1 - P_{mk} \\\\ $$ Gini Index: $$\\sum_{i\\neq j}P_{mi}P_{mj} \\\\ $$ Cross Entropy $$-\\sum_{k=1}^{K}p_{mk}log(P_{mk}) \\\\ $$   Gini Example In the snapshot below, we split the population using two input variables Gender and Class. Now, I want to identify which split is producing more homogeneous sub-nodes using Gini .\n Split on Gender:\n Calculate, Gini for sub-node $\\mathrm{Female} = (0.2)\\times(0.2)+(0.8)\\times(0.8)=0.68$ Gini for sub-node $\\mathrm{Male} = (0.65)\\times(0.65)+(0.35)\\times(0.35)=0.55$ Calculate weighted Gini for Split $\\mathrm{Gender} = (10/30)\\times0.68+(20/30)\\times0.55 = 0.59$  Similar for Split on Class:\n Gini for sub-node Class $\\mathrm{IX} = (0.43)\\times(0.43)+(0.57)\\times(0.57)=0.51$ Gini for sub-node Class $\\mathrm{X} = (0.56)\\times(0.56)+(0.44)\\times(0.44)=0.51$ Calculate weighted Gini for Split $\\mathrm{Class} = (14/30)\\times0.51+(16/30)\\times0.51 = 0.51$  Above, we can see that Gini score for Split on Gender is higher than Split on Class, hence, the node split will take place on Gender.\nPros and Cons of Trees Pros:  Inexpensive to construct. Extremely fast at classifying unknown records. Easy to interpret for small-sized trees Accuracy comparable to other classification techniques for many simple data sets. Excludes unimportant features.  Cons:  Decision Boundary restricted to being parallel to attribute axes. Decision tree models are often biased toward splits on features having a large number of levels. Small changes in the training data can result in large changes to decision logic. Large trees can be difficult to interpret and the decisions they make may seem counter intuitive.  Reference [1] Chakure, A. (2020, November 6). Decision Tree Classification. Medium. https://medium.com/swlh/decision-tree-classification-de64fc4d5aac.\n[2] Płoński, P. (2020, June 22). Visualize a Decision Tree in 4 Ways with Scikit-Learn and Python. MLJAR Automated Machine Learning. https://mljar.com/blog/visualize-decision-tree/.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/10_decision_tree/","summary":"A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification and regression. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. As the name goes, it uses a tree-like model of decisions. Though a commonly used tool in data \\mining for deriving a strategy to reach a particular goal, its also widely used in machine learning.","title":"Decision Trees"},{"content":"Support Vector Machine(SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.\nThe goal is to learn a hyperplane that discriminates between classes; we find a decision boundary which maximum separation between two classes. We consider two cases:\n  Hard Margin: Classes are linear separable. We find a hyperplane with margin that maxmally seperates the twp classes.\n  Soft Margin: Classes are not linear separable. We allow error (slack) in the model fitting and find a decision boundary with the minimum slack required to discriminate between two classes.\n    支持向量机(support vector machine) 是在分类与回归分析中分析数据的监督式学习模型与相关的学习算法。给定一组训练实例，每个训练实例被标记为属于两个类别中的一个或另一个，SVM训练算法创建一个将新的实例分配给两个类别之一的模型，使其成为非概率二元线性分类器。 如果这些训练数据是 线性可分的 ，可以选择分离两类数据的两个平行超平面，使得它们之间的距离尽可能大。在这两个超平面范围内的区域称为“间隔”，最大间隔超平面是位于它们正中间的超平面。如果这些训练数据是 线性不可分的，我们会允许SVM在少量样本上出错，即将之前的硬间隔最大化条件放宽一点，为此引入\u0026quot; 软间隔(soft margin) \u0026ldquo;的概念，为了使不满足上述条件的样本点尽可能少，我们需要在优化的目标函数里面新增一个对这些点的惩罚项。\n Hard Margin SVM Illustration: If the data is linear separable, there exist an infinite number of hyperplanes that can separate the two classes. The algorithm looks for the best hyperplane; the one with largest margin.\n Hyperplane separating the two classes: $\\beta^{T}x+\\beta_{0}=0$ Set the labels: $y_{i}\\in\\{-1,1\\}$ We can classify points as $\\mathrm{sign}\\{d_{i}\\}$ where $d_{i} = \\beta^{T}x_{i}+\\beta_{0}$  Margin: The distance between the hyperplane and the closest points.\nGoal: Maximize the Magin\n$$ \\mathrm{Margin} = \\min\\{y_{i}d_{i}\\}, \\ \\ \\ \\ i=1,2,3,...,n \\\\ $$\nWhere $y_{i}$ is label and $d_{i}$ is distance\n (1). $\\beta$ is orthogonal to the hyperplane. Consider $x_{1}$ and $x_{2}$ not on the hyperplane. Then,\n$$ \\beta^{T}x_{1}+\\beta_{0} = \\beta^{T}x_{2}+\\beta_{0} = 0 \\\\ \\Rightarrow \\beta^{T}(x_{1}-x_{2}) = 0 \\\\ \\Rightarrow \\beta \\ \\mathrm{is \\ orthogonal \\ to} \\ (x_{1}-x_{2}). \\\\ $$\n(2). For any $x_{0}$ on the hyperplane,\n$$ \\beta^{T}x_{0}+\\beta_{0}=0 \\Rightarrow \\beta^{T}x_{0} = -\\beta_{0} \\\\ $$\n(3). The distance $d_{i}$ is the projection of $(x_{i}-x_{0})$ to the direction of $\\beta$.\n$$ \\begin{align*} d_{i}\u0026amp;=\\frac{\\beta^{T}(x_{i}-x_{0})}{\\parallel\\beta\\parallel} \\\\ \u0026amp;=\\frac{\\beta^{T}x_{i}-\\beta^{T}x_{0}}{\\parallel\\beta\\parallel} \\\\ \u0026amp;=\\frac{\\beta^{T}x_{i}-\\beta_{0}}{\\parallel\\beta\\parallel} \\\\ \\end{align*} $$\nThus,\n$$ \\mathrm{Margin} = \\min\\{y_{i}d_{i}\\} = \\min\\{\\frac{y_{i}\\beta^{T}x_{i}-\\beta_{0}}{\\parallel\\beta\\parallel}\\} \\\\ $$\n(4). If the point is not on the hyperplane, then\n$$ y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \u0026gt; 0 \\\\ \\Rightarrow \\ y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \\geq c \\\\ \\Rightarrow \\ y_{i}\\frac{(\\beta^{T}x_{i}+\\beta_{0})}{c} \\geq 1 \\\\ \\Rightarrow \\ \\mathrm{There \\ exist} \\ \\beta \\ \\mathrm{and} \\ \\beta_{0} \\ \\mathrm{such \\ that} \\ y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \\geq 1 \\\\ $$\nTherefore,\n$$ \\mathrm{Margin} = \\frac{1}{\\parallel\\beta\\parallel} \\\\ $$\n The original problem can be reduced now to:\n$$ \\min_{\\beta,\\beta_{0}}\\frac{1}{2}{\\parallel\\beta\\parallel}^{2} \\\\ \\mathrm{Subject \\ to} \\ y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \\geq 1, i =1,2,...,n $$\nLagrange Method From the lagrangian, we can have:\n$$ \\begin{align*} L_{p}\u0026amp;=\\frac{1}{2}{\\parallel\\beta\\parallel}^{2}-\\sum_{i=1}^{n}\\alpha_{i}[ y_{i}(\\beta^{T}x_{i}+\\beta_{0})-1] \\\\ \u0026amp;= \\frac{1}{2}\\beta^{T}\\beta - \\beta^{T}\\sum_{i=1}^{n}\\alpha_{i}y_{i}x_{i}-\\beta_{0}\\sum_{i=1}^{n}\\alpha_{i}y_{i}+\\sum_{i=1}^{n}\\alpha_{i} \\\\ \\end{align*} $$\n$$ \\begin{cases} \\frac{\\partial L_{p}}{\\partial \\beta}=\\beta - \\sum_{i=0}^{n}\\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow \\beta = \\sum_{i=0}^{n}\\alpha_{i}y_{i}x_{i} \\\\ \\frac{\\partial L_{p}}{\\partial \\beta_{0}}=-\\sum_{i=0}^{n}\\alpha_{i}y_{i} = 0 \\Rightarrow \\sum_{i=0}^{n}\\alpha_{i}y_{i} = 0 \\\\ \\end{cases} $$\nSubstituting this in the lagrangian we obtain:\n$$ \\begin{align*} \\max_{\\alpha_{i}}\\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2}\\sum_{i=1}^{n}\u0026amp;\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j} \\\\ \\mathrm{Subject \\ to} \\ \\alpha_{i} \u0026amp;\\geq 0 \\\\ \\sum_{i=1}^{n}\\alpha_{i}y_{i}\u0026amp;=0 \\\\ \\end{align*} $$\nOur primal optimization problem with the Lagrangian becomes the following:\n$$ \\min_{\\beta,\\beta_{0}}(\\max_{\\alpha_{i}}L(\\beta,\\beta_{0},\\alpha)) \\\\ $$\nDual Problem What Antoni and Prof. Patrick Winston have done in their derivation is assume that the optimization function and the constraints meet some technical conditions such that we can do the following:\n$$ \\min_{\\beta,\\beta_{0}}(\\max_{\\alpha_{i}}L(\\beta,\\beta_{0},\\alpha))=\\max_{\\alpha_{i}}(\\min_{\\beta,\\beta_{0}}L(\\beta,\\beta_{0},\\alpha)) \\\\ $$\nThis allows us to take the partial derivatives of $L(\\beta,\\beta_{0},\\alpha)$ with respect to $\\beta$ and $\\beta_{0}$, equate to zero and then plug the results back into the original equation of the Lagrangian, hence generating an equivalent dual optimization problem of the form\n$$ \\begin{align*} \u0026amp;\\max_{\\alpha_{i}}(\\min_{\\beta,\\beta_{0}}L(\\beta,\\beta_{0},\\alpha)) \\\\ \u0026amp;\\max_{\\alpha_{i}}\\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\u0026lt;x_{i},x_{j}\u0026gt; \\\\ \u0026amp;\\mathrm{s.t.} \\ \\alpha_{i} \\geq 0 \\\\ \u0026amp;\\mathrm{s.t.} \\ \\sum_{i=1}^{n}\\alpha_{i}y_{i}=0 \\\\ \\end{align*} $$\nOptimization – Karush–Kuhn–Tucker (KKT) conditions Without going into excessive mathematical technicalities, these conditions are a combination of the Duality and the Karush Kuhn Tucker (KTT) conditions and allow us to solve the dual problem instead of the primal one, while ensuring that the optimal solution is the same. In our case the conditions are the following:\n The primal objective and inequality constraint functions must be convex The equality constraint function must be affine The constraints must be strictly feasible  Then there exists $x^{*}$,$\\alpha^{*}$ which are solutions to the primal and dual problems.\n$$ \\min_{x}f(x) \\\\ \\mathrm{subject \\ to} \\ g(x)\\leq 0 \\\\ $$\n If $x^{*}$ is a local optimum, subject to regularity conditions, then there exist constant multipliers $\\alpha_{i}^{*}$ such that  $$ \\begin{align*} \u0026amp;\\frac{\\partial}{\\partial x}L(x^{*}) = 0 \\\\ \u0026amp;g(x^{*}) \\leq 0 \\\\ \u0026amp;\\alpha_{i}^{*} \\geq 0 \\\\ \u0026amp;\\alpha_{i}^{*}g(x^{*})=0, \\forall i \\\\ \\end{align*} $$\nFrom the complementary slackness (KKT Conditions), we have $\\alpha_{i}^{*}g(x^{*})=\\alpha_{i}^{*}[y_{i}(\\beta^{T}x_{i}+\\beta_{0})-1] = 0$ we can see that\n if $\\alpha_{i}^{*} \u0026gt; 0$, then $y_{i}(\\beta^{T}x_{i}+\\beta_{0})-1 = 0$, i.e. $x_{i}$ is on the margin. This point is called support vector. if $y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \u0026gt; 1$, $x_{i}$ is not on the margin, and $\\alpha_{i}^{*}=0$  This is a convex optimization and can be solved by quadratic programming.\nSoft Margin SVM Having a dataset with linear separable classes is very unlikely in practice, so we remove this requirement. We now consider a dataset where both classes overlap such that no hyperplane exists that can completely seperate the two classes on the training data. Instead, the goal becomes to find a hyperplane which minimizes the amount of datapoints that \u0026ldquo;spill\u0026rdquo; over to the opposite sides.\nWe allow points to violate our previous constraint by some error $\\zeta$, but penalize the objective function the more it is violated. Our new optimization problem becomes:\n$$ \\min_{\\beta,\\beta_{0}}\\frac{1}{2}{\\parallel\\beta\\parallel}^{2}+\\gamma\\sum_{i=1}^{n}\\zeta_{i} \\\\ \\mathrm{Subject \\ to} \\ y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \\geq 1-\\zeta_{i}, \\forall i \\\\ \\zeta_{i} \\geq 0, \\forall i $$\nLagrange Method We formulate the lagrangian function:\n$$ L(\\beta_{0},\\beta,\\alpha,\\zeta,\\lambda)=\\frac{1}{2}{\\parallel\\beta\\parallel}^{2}+\\gamma\\sum_{i=1}^{n}\\zeta_{i}-\\sum_{i=1}^{n}\\alpha_{i}[y_{i}(\\beta^{T}x_{i}+\\beta_{0})-(1-\\zeta_{i})] - \\sum_{i=1}^{n}\\lambda_{i}\\zeta_{i} \\\\ $$\n$$ \\begin{cases} \u0026amp;\\frac{\\partial L}{\\partial \\beta}=\\beta - \\sum_{i=0}^{n}\\alpha_{i}y_{i}x_{i} = 0 \\Rightarrow \\beta = \\sum_{i=0}^{n}\\alpha_{i}y_{i}x_{i} \\\\ \u0026amp;\\frac{\\partial L}{\\partial \\beta_{0}}=-\\sum_{i=0}^{n}\\alpha_{i}y_{i} = 0 \\Rightarrow \\sum_{i=0}^{n}\\alpha_{i}y_{i} = 0 \\\\ \u0026amp;\\frac{\\partial L}{\\partial \\zeta_{i}} \\Rightarrow \\gamma - \\alpha_{i} - \\lambda_{i} = 0 \\\\ \\end{cases} $$\nSubstituting this in the lagrangian we obtain:\n$$ \\begin{align*} L(\\beta_{0},\\beta,\\alpha,\\zeta,\\lambda)\u0026amp;=\\sum_{i=1}^{n}\\alpha_{i} - \\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}x_{i}^{T}x_{j} \\\\ \u0026amp;\\mathrm{s.t.} \\ \\alpha_{i} \\geq 0 \\\\ \u0026amp;\\mathrm{s.t.} \\ \\sum_{i=1}^{n}\\alpha_{i}y_{i}=0 \\\\ \u0026amp;\\mathrm{s.t.} \\ \\gamma - \\alpha_{i} - \\lambda_{i} = 0 \\\\ \\end{align*} $$\nSince $\\lambda_{i}$ is a lagrangian multiplayer, we know that $\\lambda_{i}\\geq0$\n$$ \\lambda_{i} = 0 \\Rightarrow \\alpha_{i} = \\gamma \\\\ \\lambda_{i} \u0026gt; 0 \\Rightarrow \\alpha_{i} \u0026lt; \\gamma \\\\ $$\nThus, we need to\n$$ \\begin{align*} \\max\\sum_{i=1}^{n}\\alpha_{i} - \u0026amp;\\frac{1}{2}\\sum_{i=1}^{n}\\sum_{j=1}^{n}\\alpha_{i}\\alpha_{j}y_{i}y_{j}\u0026lt;x_{i},x_{j}\u0026gt; \\\\ \u0026amp;\\mathrm{s.t.} \\ 0 \\leq \\alpha_{i} \\leq \\gamma \\\\ \u0026amp;\\mathrm{s.t.} \\ 0 \\sum_{i=1}^{n}\\alpha_{i}y_{i}=0 \\\\ \\end{align*} $$\nOptimization – Karush–Kuhn–Tucker (KKT) conditions From the complementary slackness (KKT Conditions), we have\n$$ \\alpha_{i}g(x^{*})=\\alpha_{i}[y_{i}(\\beta^{T}x_{i}+\\beta_{0})-(1-\\zeta_{i})] = 0, \\\\ \\lambda_{i}\\zeta_{i}=0 $$\n  if $\\alpha_{i}^{*} \u0026gt; 0$, then $y_{i}(\\beta^{T}x_{i}+\\beta_{0}) = 1-\\zeta_{i}$, $x_{i}$ is a support vector.\n $\\lambda_{i}\u0026gt;0$, then $\\zeta_{i}=0$. This point is on the margin. We know that $\\gamma - \\alpha_{i}^{*} - \\lambda_{i} = 0$ $\\ \\Rightarrow \\ $ $\\alpha_{i}^{*}\u0026lt;\\gamma$ $\\zeta_{i}\u0026gt;0$, then $\\lambda_{i}=0$. This point is over the margin.    if $\\alpha_{i}^{*}=0$, then this point is NOT support vector.\n  $$ y_{i}(\\beta^{T}x_{i}+\\beta_{0})-(1-\\zeta_{i})\u0026gt; 0 \\\\ y_{i}(\\beta^{T}x_{i}+\\beta_{0})\u0026gt; 1-\\zeta_{i} \\\\ \\mathrm{Since} \\ \\gamma - \\alpha_{i}^{*} - \\lambda_{i} = 0 \\ \\mathrm{and} \\ \\alpha_{i}^{*}=0 \\\\ \\Rightarrow \\lambda_{i}=\\gamma \\Rightarrow \\zeta_{i}=0 \\\\ \\Rightarrow y_{i}(\\beta^{T}x_{i}+\\beta_{0})\u0026gt; 1\\\\ $$\nThis is a convex optimization and can be solved by quadratic programming.\nKernel Methods Kernels or kernel methods (also called Kernel functions) are sets of different types of algorithms that are being used for pattern analysis. They are used to solve a non-linear problem by using a linear classifier. Kernels are a convenient way of expanding the feature space. The fact that kernels expand the feature space will not be obvious.\nThe predicted value for a new vector x can be written as:\n$$ \\hat{f}(x) = \\hat{\\beta}_{0}+ \\sum_{i\\in S}\\hat{\\alpha}_{i}y_{i}\u0026lt;x,x_{i}\u0026gt; \\\\ $$\nWhere\n $x_{i}$ is a vector, $y_{i}$ is scalar(+1 or -1), $\\alpha_{i}$ is a scalar. alpha’s are the Lagrange multipliers arising during optimization S is the set of support vectors, and the inner product $\u0026lt;.,.\u0026gt;$ is defined as $\u0026lt;x_{i},x_{i}'\u0026gt; = \\sum_{j=1}^{p}x_{ij}x_{ij}'$  Remarkably, the solution depends only on the inner product of the observations, not on the observations themselves.\nRather than changing the number of x‐variables to include the quadratic terms, it is possible to expand the feature space as a function of the original x‐variables\n$$ \\hat{f}(x) = \\hat{\\beta}_{0}+ \\sum_{i\\in S}\\hat{\\alpha}_{i}y_{i}\u0026lt;h(x),h(x_{i})\u0026gt; \\\\ $$\nWhere $h(.)$ is a function that maps the set of original $x$‐variables to an expanded set (including derived variables)\n$$ h(x)=(h_{1}(x),h_{2}(x),...,h_{p_{new}}(x)) \\\\ $$\nAgain, this formulation implies that the solution only depends on the inner product of the expanded set of variables(not on the variables themselves).\nKernels expand this idea:\n$$ \\begin{align*} \\hat{f}(x) \u0026amp;= \\hat{\\beta}_{0}+ \\sum_{i\\in S}\\hat{\\alpha}_{i}y_{i}\u0026lt;h(x),h(x_{i})\u0026gt; \\\\ \u0026amp;= \\hat{\\beta}_{0}+ \\sum_{i\\in S}\\hat{\\alpha}_{i}y_{i}K(x,x_{i}) \\\\ \\end{align*} $$\nWhere $K$ is a Kernel function. The Kernel function implies an expanded set of variables(though this is not obvious).\nThe solution to the optimization problem depends only on the inner (dot) product, not on the observations themselves.\n Classical Example: $$ \\begin{align*} \u0026lt;\\phi(x),\\phi(x')\u0026gt; \u0026amp;= (x^{2}_{1},x^{2}_{2}\\sqrt{2}x_{1}x_{2})(x'^{2}_{1},x'^{2}_{2}\\sqrt{2}x'_{1}x'_{2})^{T} \\\\ \u0026amp;= ((x_{1},x_{2}),(x'_{1},x'_{2})^{T})^{2} \\\\ \u0026amp;= \u0026lt;x,x'\u0026gt; \\\\ \u0026amp;=: K(x,x') \\\\ \\end{align*} $$\nKernel trick:  Computing the kernel is sufficient Derived variables need not be explicitly computed  Linear Kernel $$ K(x_{i},x_{i}') = \\ \u0026lt;x_{i},x_{i}'\u0026gt; \\ = \\sum_{j=1}^{d}x_{ij}x_{i'j} \\\\ $$\n No additional tuning parameters Nonlinear kernels have more flexibility, but this is not always needed Often the preferred choice in text mining applications  Sparse matrices (mostly 0’s) Indicator variables (or counts) rather than continuous variables    Polynomial Kernel $$ K(x_{i},x_{i}') = (1+ \u0026lt;x_{i},x_{i}'\u0026gt;)^{p} = (\\beta_{0}+ \\gamma\\sum_{j=1}^{d}x_{ij}x_{i'j})^{p} \\\\ $$\n A polynomial kernel of order $d=2$ corresponds to adding all possible quadratic variables $\\beta_{0}, \\gamma\u0026gt;0$ and $d$ (\u0026gt;0, integer) are tuning parameters In practice, often $\\beta_{0}=0$, $d=2$ or $d=3$ are often sufficient. Set $d=2$ or $d=3$ and tune gamma and $C$  Radial Basis function (RBF) Kernel (or Gaussian Kernel) $$ K(x_{i},x_{i}') = exp(-\\gamma\\parallel x_{i}-x_{i}'\\parallel^{2}) = exp(-\\gamma\\sum_{j=1}^{d}(x_{ij}x_{i'j})^{2}) \\\\ $$\n $\\gamma\u0026gt;0$ is a tuning parameter The kernel is very small for points far apart Most popular choice It can be shown this kernel corresponds to an infinite number of x‐variables  SVM – More than 2 Classes One vs all (more common)  With K classes apply SVM once for each class  Class k vs all remaining classes   Each SVM classifier gives a score for class k (vs all others)  The score is the estimated function value   Predict the class k that has the highest score  One vs One (All pairs)  K classes give (K choose 2) possible pairs Conduct an SVM for each pair Keep track of which class wins each time Predict the class with the most wins  Pros and Cons of SVM Pros:  Works well with when the number of features is large Works well when number of features exceeds the number of observations Memory efficient for prediction because only support vectors (not all observations) are needed  Cons:  When the data sets are really large, training time can be very long Does not work as well when there is a lot of noise Probability estimates are ad‐hoc SVM is sensitive to scaling of x‐variables  Reference [1] Polamuri, S. (2017, February 19). Support vector machine (Svm classifier) implemenation in python with Scikit-learn. Dataaspirant. https://dataaspirant.com/svm-classifier-implemenation-python-scikit-learn/.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/09_support_vector_machine/","summary":"Support Vector Machine(SVM) is a supervised machine learning algorithm which can be used for both classification or regression challenges. However, it is mostly used in classification problems. In the SVM algorithm, we plot each data item as a point in n-dimensional space (where n is number of features you have) with the value of each feature being the value of a particular coordinate. Then, we perform classification by finding the hyper-plane that differentiates the two classes very well.","title":"Support Vector Machine"},{"content":"Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes' theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.\nBeing relatively robust, easy to implement, fast, and accurate, naive Bayes classifiers are used in many different fields. Some examples include the diagnosis of diseases and making decisions about treatment processes, the classification of RNA sequences in taxonomic studies, and spam filtering in e-mail clients.\nHowever, strong violations of the independence assumptions and non-linear classification problems can lead to very poor performances of naive Bayes classifiers. We have to keep in mind that the type of data and the type problem to be solved dictate which classification model we want to choose. In practice, it is always recommended to compare different classification models on the particular dataset and consider the prediction performances as well as computational efficiency.\n 朴素贝叶斯(Naive Bayes) 的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。朴素 : 之所以称为朴素，是因为它假定数据集中的所有要素都是相互独立的。贝叶斯 : 它基于贝叶斯定理。\n Naive Bayes From Bayes Rule, we have:\n$$ P(Y=k|X=x) = \\frac{p(X=x|Y=k)P(Y=k)}{P(X=x)} = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^{K}\\pi_{l}f_{l}(x)} \\\\ $$\nWhere density is $f_{k}(X) = P(X=x|Y=k)$ and prior probability of class $k$ is $\\pi_{k} = P(Y=k)$.\nConditional on class $k$, assume the variables $x_{j}$ are independent:\n$$ f_{k}(x)= \\prod_{j=1}^{p}f_{kj}(x_{j}) \\\\ $$\nWhere p is the number of x‐variables.\n Independence assumption: Conditional on the outcome, there is no multicollinearity This assumption is almost always wrong; but extremely convenient. This method is also called \u0026ldquo;Idiot\u0026rsquo;s Bayes\u0026rdquo;  Plugging the density into Bayes rule, we obtain:\n$$ P(Y=k|X=x) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^{K}\\pi_{l}f_{l}(x)}=\\frac{\\pi_{k}\\prod_{j=1}^{p}f_{kj}(x_{j})}{\\sum_{l=1}^{K}\\pi_{l}\\prod_{j=1}^{p}f_{lj}(x_{j})} \\\\ $$\nThe denominator does not depend on class $k$. It is a constant. To find the class that maximizes the posterior probability, we can ignore the denominator:\n$$ P(Y=k|X=x) \\propto \\pi_{k}\\prod_{j=1}^{p}f_{kj}(x_{j}) \\\\ $$\nwhere $j=1,...,p$ indexes x‐variables\nPredict the class $k$ that maximizes the posterior probability(Decision rule):\n$$ h(x) = \\arg\\max_{k}\\left(P(Y=k)\\prod_{j=1}^{p}P(X_{j}=x_{j}|Y=k)\\right) = \\arg\\max_{k}\\left(\\pi_{k}\\prod_{j=1}^{p}f_{kj}(x_{j})\\right) \\\\ $$\nWhen there are many x‐variables, multiplying many small probabilities may result in an \u0026ldquo;underflow\u0026rdquo;. Numerically, all posterior probabilities are 0. It is unclear which 0 is \u0026ldquo;largest\u0026rdquo;. We can take the log to avoid this problem. Because it is a monotone function, taking the log does not change which class $k$ gives the maximum posterior probability:\n$$ h(x) = \\arg\\max_{k}(log(\\pi_{k}) + \\sum_{j=1}^{p}log[f_{kj}(x_{j})]) \\\\ $$\nTypically, estimate the prior probability as the fraction of time the class occurs in the training data:\n$$ \\pi_{k} = P(Y=k) = \\frac{n_{k}}{n} \\\\ $$\nEstimate the probability as:\n$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \\frac{n_{kj}}{n_{k}} \\\\ $$\n where $n_{k}$ is the number of obs in class $k$ Where $n_{kj}$ is the number of obs in class $k$ taking the value $x_{j}$  LaPlace Smoothing For a given x‐variable, LaPlace smoothing adds one observation to each x‐category\n$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \\frac{n_{kj}+1}{n_{k}+d_{j}} \\\\ $$\n where $d_{j}$ is the number of categories of the corresponding x‐variable  Smoothing in general  The probability estimates “shrink” away from the extremes Instead of adding just one observation, we can add an arbitrary number of observations, $L$, that controls the amount of shrinking:  $$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \\frac{n_{kj}+L}{n_{k}+L*d_{j}} \\\\ $$\n In the limit, for very large L,  $$ P(X_{j}=x_{j}|Y=k) \\to \\frac{L}{L \\times d_{j}} = \\frac{1}{d_{j}} \\\\ $$\n Gaussian Naive Bayes Gaussian Naive Bayes classifier assumes that the likelihoods are Gaussian:\n$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) =\\frac{1}{\\sqrt{2\\pi\\sigma_{jk}^{2}}}exp[\\frac{-(x_{j}-\\mu_{jk})^{2}}{2\\sigma_{jk}}] \\\\ $$\n Gaussian Naive Bayes is not as common as the case where all x‐ variables are categorical. Maximum likelihood estimate of parameters are:  $$ \\begin{align*} \\mu_{jk} \u0026amp;= \\frac{\\sum_{n=1}^{N}I[Y^{(n)}=k]\\cdot x_{j}^{(n)}}{\\sum_{n=1}^{N}I[Y^{(n)}=k]} \\\\ \\sigma_{jk} \u0026amp;= \\frac{\\sum_{n=1}^{N}I[Y^{(n)}=k]\\cdot (x_{j}^{(n)}-\\mu_{jk})^{2}}{\\sum_{n=1}^{N}I[Y^{(n)}=k]} \\\\ \\end{align*} $$\nLet\u0026rsquo;s consider a simple example using the Iris data set:\n There are 3 class labels: Setosa, Versicolor, Virginica which we label as $y\\in\\{0,1,2\\}$ There are two explanatory variables (features): $X_{1}$: sepal length and $X_{2}$: sepal width.  For each feature, we calculate the estimated class mean, class variance and prior probability\n Mean: $\\mu_{x_{1}|Y=0}$, $\\mu_{x_{1}|Y=1}$, $\\mu_{x_{1}|Y=2}$ and $\\mu_{x_{2}|Y=0}$, $\\mu_{x_{2}|Y=1}$, $\\mu_{x_{2}|Y=2}$ Variance: $\\sigma_{x_{1}|Y=0}^{2}$, $\\sigma_{x_{1}|Y=1}^{2}$, $\\sigma_{x_{1}|Y=2}^{2}$ and $\\sigma_{x_{2}|Y=0}^{2}$, $\\sigma_{x_{2}|Y=1}^{2}$, $\\sigma_{x_{2}|Y=2}^{2}$ Prior: $P(Y=0)$, $P(Y=1)$, $P(Y=2)$  For any point $(x1,x2)$ we compute the Gaussian Naive Bayes objective function (i.e. the one we are trying to maximize) for each class :\n$$ \\begin{align*} h(x) \u0026amp;= \\arg\\max_{k}[P(Y=k)\\prod_{j=1}^{2}P(X_{j}=x_{j}|Y=k)] \\\\ \u0026amp;= \\arg\\max_{k}[P(X_{1}=x_{1}|Y=k)P(k)\\cdot P(X_{2}=x_{2}|Y=k)P(k)] \\\\ \u0026amp;= \\arg\\max_{k}[\\phi(x_{1}|\\mu_{x_{1}|k},\\sigma_{x_{1}|k}^{2})P(k)\\cdot \\phi(x_{2}|\\mu_{x_{2}|k},\\sigma_{x_{2}|k}^{2})P(k)] \\\\ \\end{align*} $$\n where $\\phi(x_{1}|\\mu_{x_{1}|k},\\sigma_{x_{1}|k}^{2})$ is the PDF of a Gaussian univariate distribution with parameters $\\mu_{x_{1}|k}$, $\\sigma_{x_{1}|k}^{2}$. Repeat this calculation for each class, and then predict the class which has the highest value.  Reference [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Gaussian Naive Bayes Classifier: Iris data set - Data Blog. https://xavierbourretsicotte.github.io/Naive_Bayes_Classifier.html.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/","summary":"Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes' theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.","title":"Naive Bayes"},{"content":"K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.\nWe say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.\n KNN(K Nearest Neighbor) 是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的 k 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中 K 通常是不大于 20 的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。\n K-Nearest Neighbors   Training example in Euclidean space: $x\\in\\mathbf{R}^{d}$\n  Idea: The value of the target function for a new query is estimated from the known value(s) of the nearest training example(s)\n  Distance typically defined to be Euclidean:\n  $$ \\parallel x^{(a)}-x^{(b)} \\parallel_{2} = \\sqrt{\\sum_{j=1}^{d}(x_{j}^{(a)} - x_{j}^{(b)})^{2}} \\\\ $$\nkNN Algorithom  Load the training and test data Choose the value of K For each point in test data:   find the Euclidean distance to all training data points store the Euclidean distances in a list and sort it choose the first k points assign a class to the test point based on the majority of classes present in the chosen points  End    KNN算法的思想大致为: 在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的 前 K 个数据 ，则该测试数据对应的类别就是 K 个数据中 出现次数最多的那个分类 。如下图，绿色圆要被决定赋予哪个类。如果 K=3，由于红色三角形所占比例为 2/3，绿色圆将被赋予红色三角形那个类，如果 K=5，由于蓝色四方形比例为 3/5，因此绿色圆被赋予蓝色四方形类。\n  Choice of K  $k=1$ can be unstable, particularly if the data are noisy Better to choose an odd number to avoid ties, e.g. $k=3$ or $k=5$ Larger $k$ may lead to better performance. But if we set $k$ too large we may end up looking at samples that are not neighbors (are far away from the query) Rule of thumb is $k \u0026lt; \\mathrm{Sqrt}(n)$, where $n$ is the number of training examples Choose $k$ that yields the smallest error on the test data  Pros and Cons of K-Nearest Neighbors Pros:  It is extremely easy to implement It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc. Since the algorithm requires no training before making predictions, new data can be added seamlessly. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)  Cons:  Accuracy depends on the quality of the data. Poor at classifying data points in a boundary where they can be classified one way or another. Doesn\u0026rsquo;t work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension. Has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher. Doesn\u0026rsquo;t work well with categorical features since it is difficult to find the distance between dimensions with categorical features.  Issues \u0026amp; Remedies   If some attributes (coordinates of x) have larger ranges, they are treated as more important\n Normalize scale  Simple option: Linearly scale the range of each feature to be, e.g., in range $[0,1]$ Linearly scale each dimension to have 0 mean and variance 1 (compute mean $\\mu$ and variance $\\sigma^{2}$ for an attribute $x_{j}$ and scale: $\\frac{(x_{j} - m)}{\\sigma}$)   Be careful: sometimes scale matters    Irrelevant, correlated attributes add noise to distance measure\n eliminate some attributes or vary and possibly adapt weight of attributes    Non-metric attributes (symbols)\n Hamming distance    Expensive at test time: To find one nearest neighbor of a query point $x$, we must compute the distance to all $N$ training examples. Complexity: $O(kdN)$ for kNN\n Use subset of dimensions Pre-sort training examples into fast data structures (e.g., kd-trees) Compute only an approximate distance (e.g., LSH) Remove redundant data (e.g., condensing)    Storage Requirements: Must store all training data\n Remove redundant data (e.g., condensing) Pre-sorting often increases the storage requirements    High Dimensional Data: “Curse of Dimensionality”\n Required amount of training data increases exponentially with dimension Computational cost also increases    Reference [1] Zemel, R., Urtasun, R., \u0026amp; Fidler, S. (n.d.). CSC 411: Lecture 05: Nearest Neighbors. https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/05_nn.pdf.\n[2] Sanjay.M. (2018, November 2). KNN using scikit-learn. Medium. https://towardsdatascience.com/knn-using-scikit-learn-c6bed765be75.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/","summary":"K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.\nWe say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset.","title":"K-Nearest Neighbors"},{"content":"Fisher\u0026rsquo;s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u0026ldquo;curse of dimensionality\u0026rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes.\nFisher’s Linear Discriminant Analysis Assume we have only 2 classes. The idea behind Fisher’s Linear Discriminant Analysis is to reduce the dimensionality of the data to one dimension. That is, to take d-dimensional $x\\in \\mathbf{R}^{d}$ and map it to one dimension by finding $w^{T}x$ where:\n$$z = w^{T}x= \\begin{bmatrix} w_{1} ... w_{d} \\\\ \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ ... \\\\ x_{d} \\\\ \\end{bmatrix} = \\sum_{i=1}^{d}w_{i}x_{i} \\\\ $$\nThe one-dimensional $z$ is then used for classification.\nGoal: To find a direction such that projected data $w^{T}x$ are well separated.\nConsider the two-class problem:\n$$ \\mu_{0}=\\frac{1}{n_{0}}\\sum_{i:y_{i}=0}x_{i} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_{1}=\\frac{1}{n_{1}}\\sum_{i:y_{i}=0}x_{i} \\\\ $$\nWe want to:\n  Maximize the distance between projected class means.\n  Minimize the within class variance.\n   Fisher判别分析(Fisher’s Linear Discriminant Analysis)的目的：给定一个投影向量$w$，将$x$投影到$w$向量上，使得不同类别的$x$投影后的值 $y=w^{T}x$ 尽可能互相分开(far apart)。投影向量的选择很关键，当投影向量选择不合理时，不同类别的$x$投影后的$y$根本无法被分开。所以要找到最优的投影向量，使得投影后的值被最大限度地区分。(这里的前提是：原始数据是线性可分的)\n最大限度的划分投影后的值需要用到两个准则:\n 投影后的两类样本 均值 之间的距离 尽可能大 投影后两类样本各自的 方差尽可能小    The distance between projected class means is:\n$$ \\begin{align*} (w^{T}\\mu_{0} - w^{T}\\mu_{1})^{2} \u0026amp;= (w^{T}\\mu_{0} - w^{T}\\mu_{1})^{T}(w^{T}\\mu_{0} - w^{T}\\mu_{1}) \\\\ \u0026amp;= (\\mu_{0}-\\mu_{1})^{T}ww^{T}(\\mu_{0}-\\mu_{1}) \\\\ \u0026amp;=w^{T}(\\mu_{0}-\\mu_{1})(\\mu_{0}-\\mu_{1})^{T}w \\\\ \u0026amp;= w^{T}S_{B}w \\\\ \\end{align*} $$ where $S_{B}$ is the between-class variance (known).\nMinimizing the within-class variance is equivalent to minimizing the sum of all individual within-class variances. Thus the within class variance is:\n$$ \\begin{align*} w^{T}\\Sigma_{0}w+w^{T}\\Sigma_{1}w \u0026amp;= w^{T}(\\Sigma_{0}+\\Sigma_{1})w \\\\ \u0026amp;= w^{T}S_{W}w \\\\ \\end{align*} $$\nwhere $S_{W}$ is the within-class covariance (known).\nTo maximize the distance between projected class means and minimize the within-class variance, we can maximize the ratio:\n$$ \\max_{w} \\ \\frac{w^{T}S_{B}w}{w^{T}S_{W}w} \\\\ $$\nNote that the numerator is unbounded since we can make any arbitary $w^{T}$. But since we are only interested in direction, length is not important. Therefore we can fix the length of $w$ (i.e. unit length) and find the direction. This is equivalent to finding:\n$$ \\max_{w} \\ w^{T}S_{B}w \\\\ \\mathrm{Subject \\ to} \\ \\ w^{T}S_{W}w = 1 \\\\ $$\nTo turn this constraint optimization problem into a non-constranst optimization problem, we apply Lagrange multipliers:\n$$ L(w,\\lambda) = w^{T}S_{B}w - \\lambda(w^{T}S_{W}w-1) \\\\ $$\nDifferentiating with respect to $w$ we get:\n$$ \\frac{\\partial L}{\\partial w} = 2S_{B} w - \\lambda2S_{W} w = 0 \\\\ S_{B} w = \\lambda S_{W}w \\\\ $$\nThis is a generalized eigenvector problem that is equivalent to (if $S_{W}$ is not singular):\n$$ S_{W}^{-1}S_{B}w = \\lambda w \\\\ $$\nwhere $\\lambda$ and $w$ are the eigenvalues and eigenvectors of $S_{W}^{-1}S_{B}$respectively. $w$ is the eigenvector corresponding to the largest eigenvalue of $S_{W}^{-1}S_{B}$.\nIn fact, for two-classes problems, there exists a simpler solution. Recall that $S_{B}w = (\\mu_{0}-\\mu_{1})(\\mu_{0}-\\mu_{1})^{T}w$ where $(\\mu_{0}-\\mu_{1})^{T}w$ is a scalar. Therefore $S_{B}w\\propto(\\mu_{0}-\\mu_{1})$. That is, $S_{B}w$ is on the same direction as $(\\mu_{0}-\\mu_{1})$. Since $S_{W}^{-1}S_{B}w = \\lambda w$, we get:\n$$ S_{w}^{-1}(\\mu_{0}-\\mu_{1}) \\propto w \\\\ $$\nwhich gives us the direction.\nFisher’s Linear Discriminant Analysis For Multiple Classes  We have defined $\\varepsilon(w)=\\frac{w^{T}S_{B}w}{w^{T}S_{W}w}$ that needs to be maximized. $w$ is the largest eigen vectors of $S_{W}^{-1}S_{B}$. For two classes, $w \\propto S_{w}^{-1}(\\mu_{0}-\\mu_{1})$. For $k$-class problem, Fisher Discriminant Analysis involves $(k - 1)$ discriminant functions. Make $W_{d \\times (K-1)}$ where each column describes a discriminant. So now, we have to update the two notions we have defined for a $2$-class problem, $S_{B}$ and $S_{W}$ .\n$$ S_{w}=\\sum_{i=1}^{K}\\Sigma_{i} \\\\ $$\n$S_{B}$ generalization to multiple classes in not obvious. We will define the total variance $S_{T}$ as the sum of the within class variance and between classes variance.\n$$ S_{T} = S_{B}+S_{W} \\\\ $$\nWhere $S_{T}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)(x_{i}-\\mu)^{T}$ and $\\mu=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$. So,\n$$ S_{B} = S_{T}-S_{W} \\\\ $$\nIt can be shown that $W$ is the first $(k - 1)$ eigen vectors of $S_{W}^{-1}S_{B}$.\nPCA vs. LDA Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques that are commonly used for dimensionality reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores” class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised” and computes the directions (“linear discriminants”) that will represent the axes that that maximize the separation between multiple classes.\nAlthough it might sound intuitive that LDA is superior to PCA for a multi-class classification task where the class labels are known, this might not always the case.\n  PCA 实质上是在寻找一个子空间。而这个子空间是协方差矩阵的特征空间(特征向量对应的空间)，选取特征值最大的 k 个特征向量组成的特征子空间(相当于这个子空间有 k 维，每一维代表一个特征，这 ｋ 个特征基本可以涵盖 90% 以上的信息)。Fisher判别 和 PCA 是在做类似的一件事，都是在找子空间。不同的是, PCA 是找一个低维的子空间，样本投影在这个空间基本不丢失信息。而 Fisher是寻找这样的一个空间，样本投影在这个空间上，类内距离最小，类间距离最大。\n两者的 相同点 ：\n 两者均可以对数据进行降维。 两者在降维时均使用了矩阵特征分解的思想。 两者都假设数据符合高斯分布。  两者的 不同点 ：\n LDA是有监督的降维方法，而PCA是无监督的降维方法 LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 LDA除了可以用于降维，还可以用于分类。 LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。   Reference [1] Raschka, S. (2014, August 3). Linear Discriminant Analysis. Dr. Sebastian Raschka. https://sebastianraschka.com/Articles/2014_python_lda.html.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/","summary":"Fisher\u0026rsquo;s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u0026ldquo;curse of dimensionality\u0026rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes.","title":"Fisher’s Linear Discriminant Analysis"},{"content":"Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\nThe goal is to preserve as much of the variance in the original data as possible in the new coordinate systems. Give data on $d$ variables, the hope is that the data points will lie mainly in a linear subspace of dimension lower than $d$. In practice, the data will usually not lie precisely in some lower dimensional subspace. The new variables that form a new coordinate system are called principal components (PCs).\n 主成分分析(principal components analysis,PCA) 是一种简化数据集的技术。它是一个线性变换。这个变换把数据变换到一个新的坐标系统中，其中，第一个新坐标轴选择是原始数据中 方差最大 的方向，第二个新坐标轴选取是与 第一个坐标轴正交的平面中 使得方差最大的，第三个轴是与第 1,2 个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面 $k$个坐标轴中，后面的坐标轴所含的方差几乎为 0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴. 这相当于只保留包含绝大部分方差的维度特征，而 忽略包含方差几乎为 0 的特征维度 ，实现对数据特征的 降维处理 。\n Principal Component Analysis  PCs are denoted by $u_{1},u_{2},...,u_{d}.$ The principal components form a basis for the data. Since PCs are orthogonal linear transformations of the original variables there is at most $d$ PCs. Normally, not all of the $d$ PCs are used but rather a subset of $p$ PCs, $u_{1},u_{2},...,u_{p}.$ In order to approximate the space spanned by the original data points $x = \\begin{bmatrix} x_{1} \\\\ \\cdots \\\\ x_{d} \\\\ \\end{bmatrix}_{\\ dx1}$ We can choose $p$ based on what percentage of the variance of the original data we would like to maintain.  The first PC, $u_{1}$ is called first principal component and has the maximum variance, thus it accounts for the most significant variance in the data.\nThe second PC, $u_{2}$ is called second principal component and has the second highest variance and so on until PC ud which has the minimum variance.\n In order to capture as much of the variability as possible, let us choose the first principal component, denoted by $u_{1}$, to capture the maximum variance. Suppose that all centred observations are stacked into the columns of a $d \\times n$ matrix $X = \\begin{bmatrix} x_{1} ... x_{n} \\\\ \\end{bmatrix}_{d \\times n}$, where each column corresponds to a $d$-dimensional observation and there are $n$ observations. The projection of $n$, $d$-dimensional observations on the first principal component $u_{1}$ is $u_{1}^{T}X$. We want projection on this first dimension to have maximum variance.\n$$ \\frac{1}{2N}\\sum_{n=1}^{N}(u_{1}^{T}x_{n}-u_{1}^{T}\\bar{x}_{n})^{2} = Var(u_{1}^{T}X)= u_{1}^{T}Su_{1} \\\\ $$\nWhere $S$ is the $d \\times d$ sample covariance matrix of $X$.\nClearly $Var(u_{1}^{T}X)$ can be made arbitrarily large by increasing the magnitude of $u_{1}$. This means that the variance stated above has no upper limit and so we can not find the maximum. To solve this problem, we choose $u_{1}$ to maximize $u_{1}^{T}Su_{1}$ while constraining $u_{1}$ to have unit length. Therefore, we can rewrite the above optimization problem as:\n$$ \\mathrm{max} \\ \\ u_{1}^{T}Su_{1} \\\\ \\mathrm{Subject \\ \\ to} \\ \\ u_{1}^{T}u_{1} = 1 \\\\ $$\nTo solve this optimization problem a Lagrange multiplier $\\lambda$ is introduced:\n$$ L(u_{1}, \\lambda) = u_{1}^{T}Su_{1}-\\lambda(u_{1}^{T}u_{1}-1) \\\\ $$\nLagrange Multiplier for PCA Lagrange multipliers are used to find the maximum or minimum of a function $f (x, y)$ subject to constraint $g(x, y) = c$. we define a new constant $\\lambda$ called a Lagrange Multiplier and we form the Lagrangian,\n$$ L(x,y,\\lambda) = f(x,y)-\\lambda g(x,y) \\\\ $$\nIf $f(x^{*},y^{*})$ is the max of $f(x, y)$ , there exists $\\lambda^{*}$ such that $(x^{*},y^{*},\\lambda^{*})$ is a stationary point of $L$ (partial derivatives are 0). In addition $(x^{*},y^{*})$ is a point in which functions $f$ and $g$ touch but do not cross. At this point, the tangents of $f$ and $g$ are parallel or gradients of $f$ and $g$ are parallel, such that:\n$$ \\nabla_{x,y}f = \\lambda\\nabla_{x,y}g \\\\ $$\n$$ \\begin{align*} \\mathrm{Where}, \\ \\nabla_{x,y}f \u0026amp;= (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}) \\mathrm{\\to the \\ gradient \\ of} \\ f \\\\ \\nabla_{x,y}g \u0026amp;= (\\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y}) \\mathrm{\\to the \\ gradient \\ of} \\ g \\\\ \\end{align*} $$\nDifferentiating with respect to $u_{1}$ gives $d$ equations,\n$$ \\frac{\\partial L(u_{1}, \\lambda)}{\\partial u_{1}} = 2S u_{1} - 2\\lambda u_{1} = 0 \\\\ S u_{1} = \\lambda u_{1} \\\\ $$\nPremultiplying both sides by $u_{1}^{T}$ we have:\n$$ u_{1}^{T}S u_{1} = \\lambda u_{1}^{T} u_{1} = \\lambda \\\\ $$\n$ u_{1}^{T}S u_{1}$ is maximized if $\\lambda$ is the largest eigenvalue of $S$. Clearly $\\lambda$ and $u_{1}$ are an eigenvalue and an eigenvector of $S$. Differentiating $L(u_{1}, \\lambda)$ with respect to the Lagrange multiplier $\\lambda$ gives us back the constraint:\n$$ u_{1}^{T}u_{1} = 1 \\\\ $$\nThis shows that the first principal component is given by the eigenvector with the largest associated eigenvalue of the sample covariance matrix $S$. A similar argument can show that the $p$ dominant eigenvectors of covariance matrix $S$ determine the first $p$ principal components.\nNote that the PCs decompose the total variance in the data in the following way :\n$$ \\sum_{i=1}^{d}Var(u_{i}^{T}X) = \\sum_{i=1}^{d}u_{1}^{T}Su_{1} = \\sum_{i=1}^{d}(\\lambda_{i}) = Tr(S) = Var(X) \\\\ $$\n$Var(u_{1}^{T}X)$ is maximized if $u_{1}$ is the eigenvector of $S$ with the corresponding maximum eigenvalue. Each successive PC can be generated in the above manner by taking the eigenvectors of $S$ that correspond to the eigenvalues:\n$$ \\lambda_{1} \\geq ... \\geq \\lambda_{d} \\\\ $$\nSuch that\n$$ Var(u_{1}^{T}X) \\geq ... \\geq Var(u_{d}^{T}X) \\\\ $$\nDirect PCA Algorithm  Normalize the data: Set $X = X - \\bar{X}$ Recover basis (PCs): Calculate $XX^{T}=\\sum_{i=1}^{n}x_{i}x_{i}^{T}$ and let $U=$ eigenvectors of $XX^{T}$ corresponding to the top $p$ eigenvalues. Encode training data: $Y=U^{T}X$ Where $Y$ is a $p \\times n$ matrix of encodings of the original data. Reconstruct training data: $\\hat{X} = UY=UU^{T}X$. Encode test example: $y=U^{T}x$ where $y$ is a $p$-dimensional encoding of x. Reconstruct test example: $\\hat{x} = Uy = UU^{T}x$.  $$ U^{T}_{p \\times d} \\cdot (X=\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\\\ \\end{bmatrix}_{\\ d \\times n})\\longrightarrow (Y=\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\cdots \\\\ y_{p} \\\\ \\end{bmatrix}_{\\ p \\times n}) \\\\ $$\n 基于特征值分解(Eigendecomposition)协方差矩阵实现PCA算法 : 对数据去中心化后，计算协方差矩阵(Covariance Matrix) $XX^{T}$。用 特征值分解方法 求协方差矩阵的特征值(Eigenvalues)与特征向量(Eigenvectors)。对特征值从大到小排序，选择其中最大的 k 个。然后将其对应的 k 个特征向量分别作为行向量组成特征向量矩阵。最后将数据转换到 k 个特征向量构建的新空间中。\n A unique solution can be obtained by finding the singular value decomposition of $X$. For each rank $p$, $U$ consists of the first $p$ columns of $U$.\n$$ X=U\\Sigma V^{T} \\\\ $$\nThe columns of $U$ in the SVD contain the eigenvectors of $XX^{T}$.\n 基于SVD(Singular Value Decomposition)分解协方差矩阵实现PCA算法 : 对数据去中心化后，计算协方差矩阵(Covariance Matrix) $XX^{T}$。通过 SVD 计算协方差矩阵的特征值(Eigenvalues)与特征向量(Eigenvectors)。 对特征值从大到小排序，选择其中最大的 k 个。然后将其对应的 k 个特征向量分别作为行向量组成特征向量矩阵。最后将数据转换到 k 个特征向量构建的新空间中。\n  当样本数多、样本特征数也多的时候,先求出协方差矩阵 $XX^{T}$ 的计算是很大的。然而有一些SVD的实现算法可以先 不求出协方差矩阵也能求出右奇异矩阵 。也就是说，我们的 PCA 算法可以不用做特征分解而是通过 SVD 来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的 PCA 算法的背后真正的实现就是用的 SVD，而不是特征值分解\n Referencce [1] Raschka, S. (2014, April 13). Implementing a Principal Component Analysis (PCA). Dr. Sebastian Raschka. https://sebastianraschka.com/Articles/2014_pca_step_by_step.htmlprincipal-component-analysis-pca-vs-multiple-discriminant-analysis-mda.\n[2] 张洋. (n.d.). 数据的向量表示及降维问题. CodingLabs. http://blog.codinglabs.org/articles/pca-tutorial.html.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/","summary":"Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.","title":"Principal Component Analysis"},{"content":"Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.\nLinear Discriminant Analysis for Classification LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:\n Maximizing the distance between the means of two classes; Minimizing the variation between each category.  Suppose that $Y \\in \\{1, ..., K\\}$ is assigned a prior $\\hat{\\pi}_{k}$ such that $\\sum_{i=1}^k \\hat{\\pi}_{k} = 1$. According to Bayes’ rule, the posterior probability is\n$$ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{\\sum_{i=1}^{K}f_{i}(x)\\pi_{i}} \\\\ $$\nwhere $f_{k}(x)$ is the density of $X$ conditioned on $k$. The Bayes Classifier can be expessed as:\n$$ h^{*}(x) = \\arg\\max_{k}\\{P(Y=k|X=x)\\} = \\arg\\max_{k}\\delta_{k}(x) \\\\ $$\nFor we assume that the random variable $X$ is a vector $X=(X_1,X_2,...,X_k)$ which is drawn from a multivariate Gaussian with class-specific mean vector and a common covariance matrix $\\Sigma \\ (i.e. \\Sigma_{k} = \\Sigma, \\forall k)$. In other words the covariance matrix is common to all K classes: $Cov(X)=\\Sigma$ of shape $d \\times d$.\nSince $x$ follows a multivariate Gaussian distribution, the probability $P(X=x|Y=k)$ is given by: ($\\mu_k$ is the mean of inputs for category $k$)\n$$ f_k(x)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ $$\nThen we can find the posterior distribution as:\n$$ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ $$\nSince $P(X=x)$ does not depend on $k$ so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into $C,C^{'},C{''}$ etc..\n$$ \\begin{align*} p_{k}(x) = \u0026amp;P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \u0026amp;=C \\cdot \\pi_{k} \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \u0026amp;=C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \\end{align*} $$\nTake the log of both sides:\n$$ \\begin{align*} logp_{k}(x) \u0026amp;=log(C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k))) \\\\ \u0026amp;= logC^{'} + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k) \\\\ \u0026amp;= logC^{'} + log\\pi_k -\\frac{1}{2}[(x^{T}\\Sigma^{-1}x+\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}]+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \u0026amp;= C^{''} + log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\end{align*} $$\nAnd so the objective function, sometimes called the linear discriminant function or linear score function is:\n$$ \\delta_{k} = log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ $$\nWhich means that given an input $x$ we predict the class with the highest value of $\\delta_{k}(x)$.\nTo find the Dicision Boundary, we will set $P(Y=k|X=x) = P(Y=l|X=x)$ which is $\\delta_{k}(x) = \\delta_{l}(x)$:\n$$ log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} = log\\pi_l -\\frac{1}{2}\\mu_{l}^{T}\\Sigma^{-1}\\mu_{l}+x^{T}\\Sigma^{-1}\\mu_{l} \\\\ log\\frac{\\pi_{k}}{\\pi_{l}} -\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Constant}}+\\underbrace{x^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Linear \\ in} \\ x} = 0 \\\\ \\Rightarrow a^{T}x + b = 0 \\\\ $$\nWhich is a linear function in $x$ - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.\nQuadratic Discrimination Analysis for Classification LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all $K$ classes. Quadratic discriminant analysis provides an alternative approach by assuming that each class has its own covariance matrix $\\Sigma_{k}$.\nTo derive the quadratic score function, we return to the previous derivation, but now $\\Sigma_{k}$ is a function of $k$, so we cannot push it into the constant anymore.\n$$ p_{k}(x) = \\pi_{k}\\frac{1}{(2\\pi)^{d/2}|\\Sigma_{k}|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ $$\n$$ \\begin{align*} logp_{k}(x) \u0026amp;= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}|-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \u0026amp;= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x +x^{T}\\Sigma_{k}^{-1}\\mu_{k} -\\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} \\\\ \\end{align*} $$\nWhich is a quadratic function of $x$. Under this less restrictive assumption, the classifier assigns an observation $X=x$ to the class for which the quadratic score function is the largest:\n$$ \\delta_{k}(x) = log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ $$\nTo find the Quadratic Dicision Boundary, we will set $P(Y=k|X=x) = P(Y=l|X=x)$ which is $\\delta_{k}(x) = \\delta_{l}(x)$:\n$$ log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) = log\\pi_l- \\frac{1}{2}log|\\Sigma_{l}| -\\frac{1}{2}(x-\\mu_l)^{T}\\Sigma_{l}^{-1}(x-\\mu_l) \\\\ \\frac{1}{2}x^{T}\\underbrace{(\\Sigma_{l}-\\Sigma_{k})}_{A}x+\\underbrace{(\\mu_{k}^{T}\\Sigma_{k}^{-1}-\\mu_{l}^{T}\\Sigma_{l}^{-1})}_{b^{T}}x +\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l}) + log(\\frac{\\pi_{l}}{\\pi_{k}}) + log(\\frac{|\\Sigma_{k}|^{1/2}}{|\\Sigma_{l}|^{1/2}})}_{c} = 0 \\\\ \\Rightarrow x^{T}Ax + b^{T}x + c = 0 \\\\ $$\nCase 1 : When $\\Sigma_{k} = I$ We first concider the case that $\\Sigma_{k} = I, \\forall k$. This is the case where each distribution is spherical, around the mean point. Then we can have:\n$$ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}I(x-\\mu_k) \\\\ $$\nWhere $log(|I|) = log(1) = 0$ and $(x-\\mu_k)^{T}I(x-\\mu_k) = (x-\\mu_k)^{T}(x-\\mu_k)$ is the Squared Euclidean Distance between two points $x$ and $\\mu_{k}$.\nThus under this condition (i.e. $\\Sigma = I$) , a new point can be classified by its distance from the center of a class, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-class’s means.\nCase 2 : When $\\Sigma_{k} \\neq I$ Since $\\Sigma_{k}$ is a symmetric matrix $\\Sigma_{k} = \\Sigma_{k}^{T}$, by using the Singular Value Decomposition (SVD) of $\\Sigma_{k}$, we can get:\n$$ \\Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\\\ \\Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\\\ $$\nThen,\n$$ \\begin{align*} (x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \u0026amp;= (x-\\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\\mu_k) \\\\ \u0026amp;= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026amp;= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-\\frac{1}{2}}S_{k}^{-\\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026amp;= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}I(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \u0026amp;= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\end{align*} $$\nWhich is also known as the Mahalanobis distance.\nThink of $S_{k}^{-\\frac{1}{2}}U_{k}^{T}$ as a linear transformation that takes points in class $k$ and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified $\\delta_{k}$ values to calculate $h^{*}(x)$. After applying the singular value decomposition, $\\Sigma_{k}^{-1}$ is considered to be an identity matrix such that:\n$$ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ $$\nWhere $log(|I|) = log(1) = 0$\nThe difference between Case 1 and Case 2 (i.e. the difference between using the Euclidean and Mahalanobis distance) can be seen in the illustration below:\n LDA and QDA in practice In practice we don’t know the parameters of Gaussian and will need to estimate them using our training data.\n$$ \\hat{\\pi}_{k} = \\hat{P}(y=k) = \\frac{n_{k}}{n} \\\\ $$\nwhere $n_{k}$ is the number of class $k$ observations.\n$$ \\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i:y_{i}=k}x_{i} \\\\ $$\n$$ \\hat{\\Sigma}_{k} = \\frac{1}{n_{k}-k}\\sum_{i:y_{i}=k}(x_{i}-\\hat{\\mu}_{k})(x_{i}-\\hat{\\mu}_{k})^{T} \\\\ $$\nIf we wish to use LDA we must calculate a common covariance, so we average all the covariances, e.g.\n$$ \\Sigma = \\frac{\\sum_{r=1}^k(n_{r}\\Sigma_{r})}{\\sum_{r=1}^k n_{r}} \\\\ $$\nWhere:\n  $n_{r}$ is the number of data points in class $r$.\n  $\\Sigma_{r}$ is the covariance of class $r$ and $n$ is the total number of data points.\n  $k$ is the number of classes.\n  Reference [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. https://xavierbourretsicotte.github.io/LDA_QDA.html.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/","summary":"Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.","title":"LDA and QDA for Classification"},{"content":"Binary Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n  \u0026ldquo;分类\u0026quot;是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是\u0026quot;回归\u0026rdquo;. 通过逻辑回归模型, 我们得到的计算结果是0-1之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类. 例如, 可能性大于 0.5 即记为 1, 可能性小于 0.5 则记为 0.\n The classification problem is just like the regression problem, except that the values we now want to predict take on only a small number of discrete values. For now, we will focus on the binary classification problem in which $y$ can take on only two values, 0 and 1. (Most of what we say here will also generalize to the multiple-class case.)\n$$ log\\frac{p}{1-p} = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\beta_{3}x_{3} + \\cdot\\cdot\\cdot + \\beta_{n}x_{n} = \\beta^{T}x \\\\ $$\n$$ \\begin{align*} P(y = 1) \u0026amp;= p = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ P(y = 0) \u0026amp;= 1 - p = \\frac{1}{1+e^{\\beta^{T}x}} \\end{align*} $$\nWe could approach the classification problem ignoring the fact that $y$ is discrete-valued, and use our old linear regression algorithm to try to predict $y$ given $x$. However, it is easy to construct examples where this method performs very poorly. Intuitively, it also doesn’t make sense for $h_{\\theta}(x)$ to take values larger than 1 or smaller than 0 when we know that $y \\in {0, 1}$. To fix this, let’s change the form for our hypotheses $h_{\\theta}(x)$ to satisfy $0 \\leq h_{\\theta}(x) \\leq 1$ This is accomplished by plugging $\\theta^{T}x$ into the Logistic Function. Our new form uses the \u0026ldquo;Sigmoid Function,\u0026rdquo; also called the \u0026ldquo;Logistic Function\u0026quot;:\n$$ f(x) = \\frac{1}{1+e^{-(x)}} \\\\ $$\n Logistic Regression First we need to define a Probability Mass Function:\n$$ \\begin{align*} \u0026amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=1|X=x) = \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} \\\\ \u0026amp;\\ \\ \\ \\ \\ \\ \\ \\ \\ \\ P(Y=0|X=x) = 1 - \\frac{e^{\\beta^{T}x}}{1+e^{\\beta^{T}x}} = \\frac{1}{1+e^{\\beta^{T}x}} \\\\ \u0026amp;\\Rightarrow \\ \\ \\ \\ P(Y \\ |X=x_{i}) = (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ \\end{align*} $$\nNaturally, we want to maximize the right-hand-side of the above statement. We will use Maximun Likelihood Estimation(MLE) to find $\\beta$:\n$$ \\hat{\\beta}_{MLE}= \\arg\\max_{\\beta} L(\\beta) \\\\ $$\n$$ L(\\beta) = \\prod_{i=1}^n P(Y=y_{i} |x_{i}) = \\prod_{i=1}^n (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})^{y_{i}} (\\frac{1}{1+e^{\\beta^{T}x_{i}}})^{1-y_{i}} \\\\ $$\n$$ \\begin{align*} l(\\beta) = log\\ L(\\beta) \u0026amp;= \\sum_{i=1}^n y_{i}\\left[\\beta^{T}x_{i} - log(1+e^{\\beta^{T}x_{i}})] + (1-y_{i})[-log(1+e^{\\beta^{T}x_{i}})\\right] \\\\ \u0026amp;=\\sum_{i=1}^n y_{i}\\beta^{T}x_{i}- log(1+e^{\\beta^{T}x_{i}}) \\\\ \\end{align*} $$\nNewton‐Raphson Method for Binary Logistic Regression Newton’s Method is an iterative equation solver: it is an algorithm to find the roots of a convex function. Equivalently, the method solves for root of the derivative of the convex function. The idea behind this method is ro use a quadratic approximate of the convex function and solve for its minimum at each step. For a convex function $f(x)$, the step taken in each iteration is $-(\\nabla^{2}f(x))^{-1}\\nabla f(x)$ . while $\\lVert\\nabla f(\\beta)\\rVert \u0026gt; \\varepsilon$:\n$$ \\beta^{new} = \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ $$\nWhere $\\nabla f(x)$ is the Gradient of $f(x)$ and $\\nabla^{2} f(x)$ is the Hessian Matrix of $f(x)$.\n$$ \\begin{align*} \\nabla f(x) = \\frac{\\partial l}{\\partial \\beta} \u0026amp;= \\sum_{i=1}^n y_{i}x_{i}- (\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}})\\cdot x_{i}^{T} \\\\ \u0026amp;= \\sum_{i=1}^n (y_{i}- \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}})\\cdot x_{i}^{T} = X(y-p) \\\\ \\end{align*} $$\n$$ \\nabla^{2}f(x) = \\frac{\\partial^{2} l}{\\partial \\beta \\partial \\beta^{T}} = \\sum_{i=1}^n - \\underbrace{\\frac{e^{\\beta^{T}x_{i}}}{1+e^{\\beta^{T}x_{i}}}}_{p_{i}}\\cdot \\underbrace{\\frac{1}{1+e^{\\beta^{T}x_{i}}}}_{1-p_{i}}x_{i} \\cdot x_{i}^{T} = -XWX^{T} \\\\ $$\nWhere $W$ is a diagonal $(n,n)$ matrix with the $i^{th}$ diagonal element defined as\n$$ W = \\begin{bmatrix} p_{i}(1-p_{i}) \u0026amp; \u0026amp; \\\\ \u0026amp; \\ddots \u0026amp; \u0026amp; \\\\ \u0026amp; \u0026amp; \u0026amp; \\\\ \\end{bmatrix}_{\\ n x n} \\\\ $$\nThe Newton‐Raphson algorithm can now be expressed as:\n$$ \\begin{align*} \\beta^{new} \u0026amp;= \\beta^{old}-(\\nabla^{2}f(x))^{-1}\\nabla f(x) \\\\ \u0026amp;= \\beta^{old}+ (XWX^{T})^{-1}X(y-p) \\\\ \u0026amp;= \\beta^{old}+ (XWX^{T})^{-1}[XWX^{T}\\beta^{t}+ X(y-p)] \\\\ \u0026amp;= \\beta^{old}+ (XWX^{T})^{-1}XWZ \\\\ \\end{align*} $$\nWhere $Z$ can be expressed as: $Z = X^{T}\\beta^{t}+ W^{-1}(y-p) $. This algorithm is also known as Iteratively Reweighted Least Squares(IRLS).\n$$ \\beta^{t+1} = \\arg\\min_{\\beta}(Z - X\\beta)^{T}W(Z-X\\beta) \\\\ $$\nOther types of Logistic Regression Multinomial Logistic Regression Three or more categories without ordering. Example: Predicting which food is preferred more (Veg, Non-Veg, Vegan).\nOrdinal Logistic Regression Three or more categories with ordering. Example: Movie rating from 1 to 5.\nReferences [1] K, D. (2021, April 8). Logistic Regression in Python using Scikit-learn. Medium. https://heartbeat.fritz.ai/logistic-regression-in-python-using-scikit-learn-d34e882eebb1.\n[2] Li, S. (2019, February 27). Building A Logistic Regression in Python, Step by Step. Medium. https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8.\n[3] Christian. (2020, September 17). Plotting the decision boundary of a logistic regression model. https://scipython.com/blog/plotting-the-decision-boundary-of-a-logistic-regression-model/.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/03_logistic_regression/","summary":"Binary Logistic Regression is one of the most simple and commonly used Machine Learning algorithms for two-class classification. It is easy to implement and can be used as the baseline for any binary classification problem. Its basic fundamental concepts are also constructive in deep learning. Logistic regression describes and estimates the relationship between one dependent binary variable and independent variables.\n  \u0026ldquo;分类\u0026quot;是应用 逻辑回归(Logistic Regression) 的目的和结果, 但中间过程依旧是\u0026quot;回归\u0026rdquo;. 通过逻辑回归模型, 我们得到的计算结果是0-1之间的连续数字, 可以把它称为\u0026quot;可能性\u0026quot;（概率）. 然后, 给这个可能性加一个阈值, 就成了分类.","title":"Logistic Regression"},{"content":"Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.\n Linear Regression Linear regression with multiple variables is also known as \u0026ldquo;multivariate linear regression\u0026rdquo;. The multivariable form of the hypothesis function accommodating these multiple features is as follows:\n$$ \\begin{align*} \u0026amp;\\mathrm{Hypothesis}: h_{\\theta}(x) = \\theta_{0} + \\theta_{1}x_{1} + \\theta_{2}x_{2} + \\theta_{3}x_{3} + \\cdot\\cdot\\cdot + \\theta_{n}x_{n} \\\\ \u0026amp;\\mathrm{Cost \\ Function}: J(\\theta) = \\frac{1}{2m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})^2 \\\\ \u0026amp;\\mathrm{Goal}: \\min_{\\theta}J(\\theta) \\\\ \\end{align*} $$\nUsing the definition of matrix multiplication, our multivariable hypothesis function can be concisely represented as:\n$$ h_{\\theta}(x) = \\begin{bmatrix} \\theta_{0} \u0026amp; \\theta_{1} \u0026amp; \\cdot\\cdot\\cdot \u0026amp; \\theta_{n} \\end{bmatrix} \\begin{bmatrix} x_{0} \\\\ x_{1} \\\\ \\cdot\\cdot\\cdot \\\\ x_{n} \\end{bmatrix} = \\theta^{T}x \\\\ $$\nGradient Descent for Linear Regression Gradient descent is a generic optimization algorithm used in many machine learning algorithms. It iteratively tweaks the parameters of the model in order to minimize the cost function. We do this is by taking the derivative (the tangential line to a function) of our cost function. The slope of the tangent is the derivative at that point and it will give us a direction to move towards. We make steps down the cost function in the direction with the steepest descent. The size of each step is determined by the parameter $\\alpha$, which is called the learning rate. The gradient descent algorithm can be represented as:\n$$\\begin{align*} \u0026amp;\\frac{\\partial J(\\theta)}{\\partial \\theta_{0}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)}) \\\\ \u0026amp;\\frac{\\partial J(\\theta)}{\\partial \\theta_{j}}= \\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)} \\\\ \\end{align*} $$\n$$ \\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix}=\\frac{1}{m}x^{T}(h_{\\theta}(x)-y) \\\\ $$\n$$ \\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}=\\begin{bmatrix} \\theta_{0} \\\\ \\theta_{1} \\\\ \\cdot\\cdot\\cdot \\\\ \\theta_{n} \\end{bmatrix}-\\alpha\\begin{bmatrix} \\frac{\\partial J(\\theta)}{\\partial \\theta_{0}} \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{1}} \\\\ \\cdot\\cdot\\cdot \\\\ \\frac{\\partial J(\\theta)}{\\partial \\theta_{n}} \\end{bmatrix} \\\\ $$\n$$\\begin{align*} \u0026amp;\\mathrm{Repect\\ until \\ convergence} \\{ \\\\ \u0026amp;\\ \\ \\ \\ \\theta_{j}^{new} :=\\theta_{j}^{old} - \\alpha\\frac{1}{m}\\sum_{i=1}^{m}(h_{\\theta}(x^{(i)})-y^{(i)})x_{j}^{(i)}) \\ \\ \\ \\ \\mathrm{for} \\ j = 0 \\cdot\\cdot\\cdot n \\\\ \u0026amp;\\} \\\\ \\end{align*} $$\nTo demonstrate the gradient descent algorithm, we initialize the model parameters with 0. The equation becomes $h_{\\theta}(x) = 0$. Gradient descent algorithm now tries to update the value of the parameters so that we arrive at the best fit line.\nWe should adjust our parameter $\\alpha$ to ensure that the gradient descent algorithm converges in a reasonable time. If $\\alpha$ is too small, gradient descent can be slow. If $\\alpha$ is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge. Failure to converge or too much time to obtain the minimum value imply that our step size is wrong.\n We can speed up gradient descent by having each of our input values in roughly the same range. This is because $\\theta$ will descend quickly on small ranges and slowly on large ranges, and so will oscillate inefficiently down to the optimum when the variables are very uneven.\nTwo techniques to help with this are feature scaling and mean normalization. Feature scaling involves dividing the input values by the range (i.e. the maximum value minus the minimum value) of the input variable, resulting in a new range of just 1. Mean normalization involves subtracting the average value for an input variable from the values for that input variable resulting in a new average value for the input variable of just zero. To implement both of these techniques, adjust your input values as shown in this formula:\n$$ x_{i} := \\frac{(x_{i}-\\mu_{i})}{s_{i}} \\\\ $$\nWhere $\\mu_{i}$ is the average of all the values for feature (i) and $s_{i}$ is the range of values (max - min), or $s_{i}$ is the standard deviation. Note that dividing by the range, or dividing by the standard deviation, give different results.\nNormal Equation for Linear Regression Gradient descent gives one way of minimizing J. In the \u0026ldquo;Normal Equation\u0026rdquo; method, we will minimize J by explicitly taking its derivatives with respect to the $\\theta_{j}$ ’s, and setting them to zero. This allows us to find the optimum theta without iteration. The normal equation formula is given below:\n$$ \\theta = (X^{T}X)^{-1}X^{T}y \\\\ $$\nThe following is a comparison of gradient descent and the normal equation:\n   Gradient Descent Normal Equation     Need to choose alpha No need to choose alpha   Needs many iterations No need to iterate   $O(kn^{2})$ $O(n^{3})$, need to calculate inverse of $X^{T}X$   Works well when n is large Slow if n is very large    With the normal equation, computing the inversion has complexity $O(n^{3})$. So if we have a very large number of features, the normal equation will be slow. In practice, when n exceeds 10,000 it might be a good time to go from a normal solution to an iterative process.\nIf $X^{T}X$ is noninvertible, the common causes might be having :\n Redundant features, where two features are very closely related (i.e. they are linearly dependent) Too many features (e.g. $m ≤ n$). In this case, delete some features or use \u0026ldquo;regularization\u0026rdquo;.  Solutions to the above problems include deleting a feature that is linearly dependent with another or deleting one or more features when there are too many features.\nEvaluating the performance of the Linear Regression Model We will be using Root mean squared error(RMSE) and Coefficient of Determination(R² score) to evaluate our model. RMSE is the square root of the average of the sum of the squares of residuals. RMSE is defined by:\n$$ RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2} \\\\ $$\n$R^{2}$ score or the coefficient of determination explains how much the total variance of the dependent variable can be reduced by using the least square regression. It can be defined by:\n$$ R^{2} = 1- \\frac{SS_{r}}{SS{t}} \\\\ $$\nWhere $SS_{t}$ is the total sum of errors if we take the mean of the observed values as the predicted value and $SS_{r}$ is the sum of the square of residuals.\n$$\\begin{align*} SS_{t} \u0026amp;= \\sum_{i=1}^{m}(y^{(i)}-\\bar{y})^2 \\\\ SS_{r} \u0026amp;= \\sum_{i=1}^{m}(h(x^{(i)})-y^{(i)})^2 \\\\ \\end{align*}$$\nReferences: [1] Agarwal, A. (2018, November 14). Linear Regression using Python. Medium. https://towardsdatascience.com/linear-regression-using-python-b136c91bf0a2.\n[2] Ng, A. (n.d.). Machine Learning. Coursera. https://www.coursera.org/learn/machine-learning.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/02_linear_regression/","summary":"Regression is a method of modelling a target value based on independent predictors. This method is mostly used for forecasting and finding out cause and effect relationship between variables. Regression techniques mostly differ based on the number of independent variables and the type of relationship between the independent and dependent variables. Regression problems usually have one continuous and unbounded dependent variable. The inputs, however, can be continuous, discrete, or even categorical data such as gender, nationality, brand, and so on.","title":"Linear Regression"},{"content":" Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.\n  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data. It is popular in machine learning and artificial intelligence textbooks to first consider the learning styles that an algorithm can adopt.\nThis taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.\nSupervised Learning Algorithms（监督学习）  In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\nSupervised learning problems are categorized into \u0026ldquo;Regression\u0026rdquo; and \u0026ldquo;Classification\u0026rdquo; problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.\nExamples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.\nUnsupervised Learning Algorithms （无监督学习）  In unsupervised learning, the data has no labels. Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don\u0026rsquo;t necessarily know the effect of the variables. We can derive this structure by Clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.\nExamples of Unsupervised Learning: Apriori algorithm, K-means.\nReinforcement Learning Algorithms （强化学习）  Lastly, we have reinforcement learning, the latest frontier of machine learning. A reinforcement algorithm learns by trial and error to achieve a clear objective. It tries out lots of different things and is rewarded or penalized depending on whether its behaviors help or hinder it from reaching its objective. This is like giving and withholding treats when teaching a dog a new trick. Reinforcement learning is the basis of Google’s AlphaGo, the program that famously beat the best human players in the complex game of Go.\nExample of Reinforcement Learning: Markov Decision Process\nRepresentation Algorithms Grouped By Similarity Algorithms are often grouped by similarity in terms of their function (how they work). For example, tree-based methods, and neural network inspired methods. This is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describe the problem and the class of algorithm such as Regression and Clustering.\nRegression Algorithms（回归算法） Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model. Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.\nThe most popular regression algorithms are:\n   Ordinary Least Squares Regression (OLSR) Linear Regression Logistic Regression Stepwise Regression Multivariate Adaptive Regression Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS)  Instance-based Algorithms（基于核的算法） Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.\nSuch methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.\nThe most popular instance-based algorithms are:\n   K-Nearest Neighbor (KNN) Learning Vector Quantization (LVQ) Self-Organizing Map (SOM) Locally Weighted Learning (LWL) Support Vector Machines (SVM)  Decision Tree Algorithms（决策树算法） Decision tree methods construct a model of decisions made based on actual values of attributes in the data. Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.\nThe most popular regularization algorithms are:\n   Classification and Regression Tree (CART) Iterative Dichotomiser 3 (ID3) C4.5 and C5.0 (different versions of a powerful approach) Chi-squared Automatic Interaction Detection (CHAID) Decision Stump M5 Conditional Decision Trees  Regularization Algorithms（正则化算法） An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing. I have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.\n  The most popular regularization algorithms are:\n Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Elastic Net Least-Angle Regression (LARS)  Bayesian Algorithms（贝叶斯算法） Bayesian methods are those that explicitly apply Bayes’ Theorem for problems such as classification and regression.\nThe most popular Bayesian algorithms are:\n   Naive Bayes Gaussian Naive Bayes Multinomial Naive Bayes Averaged One-Dependence Estimators (AODE) Bayesian Belief Network (BBN) Bayesian Network (BN)  Clustering Algorithms（聚类算法） Clustering, like regression, describes the class of problem and the class of methods. Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.\n  The most popular clustering algorithms are:\n K-Means K-Medians Expectation Maximisation (EM) Hierarchical Clustering  Association Rule Learning Algorithms（关联规则学习算法） Association rule learning methods extract rules that best explain observed relationships between variables in data. These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.\n  The most popular association rule learning algorithms are:\n Apriori algorithm Eclat algorithm  Dimensionality Reduction Algorithms（降维算法） Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarize or describe data using less information. This can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.\nThe most popular dimensionality reduction algorithms are:\n   Principal Component Analysis (PCA) Principal Component Regression (PCR) Partial Least Squares Regression (PLSR) Sammon Mapping Multidimensional Scaling (MDS) Projection Pursuit Linear Discriminant Analysis (LDA) Mixture Discriminant Analysis (MDA) Quadratic Discriminant Analysis (QDA) Flexible Discriminant Analysis (FDA)  Artificial Neural Network Algorithms（人工神经网络算法） Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks. They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.\nNote that Deep Learning have been separated out from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.\nThe most popular artificial neural network algorithms are:\n   Perceptron Multilayer Perceptrons (MLP) Back-Propagation Stochastic Gradient Descent Hopfield Network Radial Basis Function Network (RBFN)  Deep Learning Algorithms（深度学习算法） Deep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation. They are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with very large datasets of labelled analog data, such as image, text. audio, and video.\nThe most popular deep learning algorithms are:\n   Convolutional Neural Network (CNN) Recurrent Neural Networks (RNNs) Long Short-Term Memory Networks (LSTMs) Stacked Auto-Encoders Deep Boltzmann Machine (DBM) Deep Belief Networks (DBN)  Ensemble Algorithms（集成算法） Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction. Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.\nThe most popular ensemble algorithms are:\n   Boosting Bootstrapped Aggregation (Bagging) AdaBoost Weighted Average (Blending) Stacked Generalization (Stacking) Gradient Boosting Machines (GBM) Gradient Boosted Regression Trees (GBRT) Random Forest  Other Machine Learning Algorithms（其他机器学习算法） Algorithms from specialty subfields of machine learning, such as:\n Computational intelligence (evolutionary algorithms, etc.) Computer Vision (CV) Natural Language Processing (NLP) Recommender Systems Reinforcement Learning Graphical Models And more…  Feature Selection Algorithms When building a machine learning model in real-life, it’s almost rare that all the variables in the dataset are useful to build a model. Adding redundant variables reduces the generalization capability of the model and may also reduce the overall accuracy of a classifier. Furthermore adding more and more variables to a model increases the overall complexity of the model. The goal of feature selection in machine learning is to find the best set of features that allows one to build useful models of studied phenomena. The most popular feature selection algorithms are:\n Chi-square Test Fisher’s Score Correlation Coefficient Forward Feature Selection Backward Feature Elimination L1 Regularization  Performance Measures Evaluating the machine learning algorithm is an essential part of any project. Most of the times we use classification accuracy to measure the performance of our model, however it is not enough to truly judge our model. The metrics that you choose to evaluate your machine learning model is very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. The most popular evaluation performance measures are:\n Classification Accuracy Logarithmic Loss Confusion Matrix Area under Curve F1 Score Mean Absolute Error Mean Squared Error  Optimization Algorithms Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation. The most common type of optimization problems encountered in machine learning are continuous function optimization, where the input arguments to the function are real-valued numeric values, e.g. floating point values. The output from the function is also a real-valued evaluation of the input values. The most popular optimization algorithms are:\n Greedy Search Beam Search Gradient decent Conjugate gradient Momentum Adagrad RMSProp Adam  References [1] Brownlee, J. (2020, August 14). A Tour of Machine Learning Algorithms. Machine Learning Mastery. https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/.\n[2] Contact Centric. (2021, March 26). Machine Learning: A Quick Introduction and Five Core Steps. Centric Consulting. https://centricconsulting.com/blog/machine-learning-a-quick-introduction-and-five-core-steps/.\n[3] Brownlee, J. (2020, August 20). How to Choose a Feature Selection Method For Machine Learning. Machine Learning Mastery. https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/.\n","permalink":"https://followb1ind1y.github.io/posts/machine_learning/01_what_is-_machine_learning_machine_learning/","summary":"Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.\n  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data.","title":"What is Machine Learning"}]