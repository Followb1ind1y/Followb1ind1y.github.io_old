<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>What is Machine Learning | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning" />
<meta name="description" content="Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.
  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/01_what_is-_machine_learning_machine_learning/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="What is Machine Learning" />
<meta property="og:description" content="Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.
  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/01_what_is-_machine_learning_machine_learning/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-06T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-06T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="What is Machine Learning"/>
<meta name="twitter:description" content="Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.
  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.
  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "What is Machine Learning",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/01_what_is-_machine_learning_machine_learning/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "What is Machine Learning",
  "name": "What is Machine Learning",
  "description": "Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.\n  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data.",
  "keywords": [
    "Machine Learning"
  ],
  "articleBody": " Arthur Samuel (1959). Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned.\n  Tom Mitchell (1998). Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\n  Representation Algorithms Grouped By Learning Style There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data. It is popular in machine learning and artificial intelligence textbooks to first consider the learning styles that an algorithm can adopt.\nThis taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.\nSupervised Learning Algorithms（监督学习）  In supervised learning, we are given a data set and already know what our correct output should look like, having the idea that there is a relationship between the input and the output.\nSupervised learning problems are categorized into “Regression” and “Classification” problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.\nExamples of Supervised Learning: Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.\nUnsupervised Learning Algorithms （无监督学习）  In unsupervised learning, the data has no labels. Unsupervised learning allows us to approach problems with little or no idea what our results should look like. We can derive structure from data where we don’t necessarily know the effect of the variables. We can derive this structure by Clustering the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.\nExamples of Unsupervised Learning: Apriori algorithm, K-means.\nReinforcement Learning Algorithms （强化学习）  Lastly, we have reinforcement learning, the latest frontier of machine learning. A reinforcement algorithm learns by trial and error to achieve a clear objective. It tries out lots of different things and is rewarded or penalized depending on whether its behaviors help or hinder it from reaching its objective. This is like giving and withholding treats when teaching a dog a new trick. Reinforcement learning is the basis of Google’s AlphaGo, the program that famously beat the best human players in the complex game of Go.\nExample of Reinforcement Learning: Markov Decision Process\nRepresentation Algorithms Grouped By Similarity Algorithms are often grouped by similarity in terms of their function (how they work). For example, tree-based methods, and neural network inspired methods. This is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describe the problem and the class of algorithm such as Regression and Clustering.\nRegression Algorithms（回归算法） Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model. Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.\nThe most popular regression algorithms are:\n   Ordinary Least Squares Regression (OLSR) Linear Regression Logistic Regression Stepwise Regression Multivariate Adaptive Regression Splines (MARS) Locally Estimated Scatterplot Smoothing (LOESS)  Instance-based Algorithms（基于核的算法） Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.\nSuch methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called winner-take-all methods and memory-based learning. Focus is put on the representation of the stored instances and similarity measures used between instances.\nThe most popular instance-based algorithms are:\n   K-Nearest Neighbor (KNN) Learning Vector Quantization (LVQ) Self-Organizing Map (SOM) Locally Weighted Learning (LWL) Support Vector Machines (SVM)  Decision Tree Algorithms（决策树算法） Decision tree methods construct a model of decisions made based on actual values of attributes in the data. Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.\nThe most popular regularization algorithms are:\n   Classification and Regression Tree (CART) Iterative Dichotomiser 3 (ID3) C4.5 and C5.0 (different versions of a powerful approach) Chi-squared Automatic Interaction Detection (CHAID) Decision Stump M5 Conditional Decision Trees  Regularization Algorithms（正则化算法） An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing. I have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.\n  The most popular regularization algorithms are:\n Ridge Regression Least Absolute Shrinkage and Selection Operator (LASSO) Elastic Net Least-Angle Regression (LARS)  Bayesian Algorithms（贝叶斯算法） Bayesian methods are those that explicitly apply Bayes’ Theorem for problems such as classification and regression.\nThe most popular Bayesian algorithms are:\n   Naive Bayes Gaussian Naive Bayes Multinomial Naive Bayes Averaged One-Dependence Estimators (AODE) Bayesian Belief Network (BBN) Bayesian Network (BN)  Clustering Algorithms（聚类算法） Clustering, like regression, describes the class of problem and the class of methods. Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.\n  The most popular clustering algorithms are:\n K-Means K-Medians Expectation Maximisation (EM) Hierarchical Clustering  Association Rule Learning Algorithms（关联规则学习算法） Association rule learning methods extract rules that best explain observed relationships between variables in data. These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.\n  The most popular association rule learning algorithms are:\n Apriori algorithm Eclat algorithm  Dimensionality Reduction Algorithms（降维算法） Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarize or describe data using less information. This can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.\nThe most popular dimensionality reduction algorithms are:\n   Principal Component Analysis (PCA) Principal Component Regression (PCR) Partial Least Squares Regression (PLSR) Sammon Mapping Multidimensional Scaling (MDS) Projection Pursuit Linear Discriminant Analysis (LDA) Mixture Discriminant Analysis (MDA) Quadratic Discriminant Analysis (QDA) Flexible Discriminant Analysis (FDA)  Artificial Neural Network Algorithms（人工神经网络算法） Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks. They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.\nNote that Deep Learning have been separated out from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.\nThe most popular artificial neural network algorithms are:\n   Perceptron Multilayer Perceptrons (MLP) Back-Propagation Stochastic Gradient Descent Hopfield Network Radial Basis Function Network (RBFN)  Deep Learning Algorithms（深度学习算法） Deep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation. They are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with very large datasets of labelled analog data, such as image, text. audio, and video.\nThe most popular deep learning algorithms are:\n   Convolutional Neural Network (CNN) Recurrent Neural Networks (RNNs) Long Short-Term Memory Networks (LSTMs) Stacked Auto-Encoders Deep Boltzmann Machine (DBM) Deep Belief Networks (DBN)  Ensemble Algorithms（集成算法） Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction. Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.\nThe most popular ensemble algorithms are:\n   Boosting Bootstrapped Aggregation (Bagging) AdaBoost Weighted Average (Blending) Stacked Generalization (Stacking) Gradient Boosting Machines (GBM) Gradient Boosted Regression Trees (GBRT) Random Forest  Other Machine Learning Algorithms（其他机器学习算法） Algorithms from specialty subfields of machine learning, such as:\n Computational intelligence (evolutionary algorithms, etc.) Computer Vision (CV) Natural Language Processing (NLP) Recommender Systems Reinforcement Learning Graphical Models And more…  Feature Selection Algorithms When building a machine learning model in real-life, it’s almost rare that all the variables in the dataset are useful to build a model. Adding redundant variables reduces the generalization capability of the model and may also reduce the overall accuracy of a classifier. Furthermore adding more and more variables to a model increases the overall complexity of the model. The goal of feature selection in machine learning is to find the best set of features that allows one to build useful models of studied phenomena. The most popular feature selection algorithms are:\n Chi-square Test Fisher’s Score Correlation Coefficient Forward Feature Selection Backward Feature Elimination L1 Regularization  Performance Measures Evaluating the machine learning algorithm is an essential part of any project. Most of the times we use classification accuracy to measure the performance of our model, however it is not enough to truly judge our model. The metrics that you choose to evaluate your machine learning model is very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. The most popular evaluation performance measures are:\n Classification Accuracy Logarithmic Loss Confusion Matrix Area under Curve F1 Score Mean Absolute Error Mean Squared Error  Optimization Algorithms Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation. The most common type of optimization problems encountered in machine learning are continuous function optimization, where the input arguments to the function are real-valued numeric values, e.g. floating point values. The output from the function is also a real-valued evaluation of the input values. The most popular optimization algorithms are:\n Greedy Search Beam Search Gradient decent Conjugate gradient Momentum Adagrad RMSProp Adam  References [1] Brownlee, J. (2020, August 14). A Tour of Machine Learning Algorithms. Machine Learning Mastery. https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/.\n[2] Contact Centric. (2021, March 26). Machine Learning: A Quick Introduction and Five Core Steps. Centric Consulting. https://centricconsulting.com/blog/machine-learning-a-quick-introduction-and-five-core-steps/.\n[3] Brownlee, J. (2020, August 20). How to Choose a Feature Selection Method For Machine Learning. Machine Learning Mastery. https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/.\n",
  "wordCount" : "1847",
  "inLanguage": "en",
  "datePublished": "2021-05-06T00:00:00Z",
  "dateModified": "2021-05-06T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/01_what_is-_machine_learning_machine_learning/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      What is Machine Learning
    </h1>
    <div class="post-meta">May 6, 2021&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#representation-algorithms-grouped-by-learning-style" aria-label="Representation Algorithms Grouped By Learning Style">Representation Algorithms Grouped By Learning Style</a><ul>
                        
                <li>
                    <a href="#supervised-learning-algorithms%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0" aria-label="Supervised Learning Algorithms（监督学习）">Supervised Learning Algorithms（监督学习）</a></li>
                <li>
                    <a href="#unsupervised-learning-algorithms-%e6%97%a0%e7%9b%91%e7%9d%a3%e5%ad%a6%e4%b9%a0" aria-label="Unsupervised Learning Algorithms （无监督学习）">Unsupervised Learning Algorithms （无监督学习）</a></li>
                <li>
                    <a href="#reinforcement-learning-algorithms-%e5%bc%ba%e5%8c%96%e5%ad%a6%e4%b9%a0" aria-label="Reinforcement Learning Algorithms （强化学习）">Reinforcement Learning Algorithms （强化学习）</a></li></ul>
                </li>
                <li>
                    <a href="#representation-algorithms-grouped-by-similarity" aria-label="Representation Algorithms Grouped By Similarity">Representation Algorithms Grouped By Similarity</a><ul>
                        
                <li>
                    <a href="#regression-algorithms%e5%9b%9e%e5%bd%92%e7%ae%97%e6%b3%95" aria-label="Regression Algorithms（回归算法）">Regression Algorithms（回归算法）</a></li>
                <li>
                    <a href="#instance-based-algorithms%e5%9f%ba%e4%ba%8e%e6%a0%b8%e7%9a%84%e7%ae%97%e6%b3%95" aria-label="Instance-based Algorithms（基于核的算法）">Instance-based Algorithms（基于核的算法）</a></li>
                <li>
                    <a href="#decision-tree-algorithms%e5%86%b3%e7%ad%96%e6%a0%91%e7%ae%97%e6%b3%95" aria-label="Decision Tree Algorithms（决策树算法）">Decision Tree Algorithms（决策树算法）</a></li>
                <li>
                    <a href="#regularization-algorithms%e6%ad%a3%e5%88%99%e5%8c%96%e7%ae%97%e6%b3%95" aria-label="Regularization Algorithms（正则化算法）">Regularization Algorithms（正则化算法）</a></li>
                <li>
                    <a href="#bayesian-algorithms%e8%b4%9d%e5%8f%b6%e6%96%af%e7%ae%97%e6%b3%95" aria-label="Bayesian Algorithms（贝叶斯算法）">Bayesian Algorithms（贝叶斯算法）</a></li>
                <li>
                    <a href="#clustering-algorithms%e8%81%9a%e7%b1%bb%e7%ae%97%e6%b3%95" aria-label="Clustering Algorithms（聚类算法）">Clustering Algorithms（聚类算法）</a></li>
                <li>
                    <a href="#association-rule-learning-algorithms%e5%85%b3%e8%81%94%e8%a7%84%e5%88%99%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95" aria-label="Association Rule Learning Algorithms（关联规则学习算法）">Association Rule Learning Algorithms（关联规则学习算法）</a></li>
                <li>
                    <a href="#dimensionality-reduction-algorithms%e9%99%8d%e7%bb%b4%e7%ae%97%e6%b3%95" aria-label="Dimensionality Reduction Algorithms（降维算法）">Dimensionality Reduction Algorithms（降维算法）</a></li>
                <li>
                    <a href="#artificial-neural-network-algorithms%e4%ba%ba%e5%b7%a5%e7%a5%9e%e7%bb%8f%e7%bd%91%e7%bb%9c%e7%ae%97%e6%b3%95" aria-label="Artificial Neural Network Algorithms（人工神经网络算法）">Artificial Neural Network Algorithms（人工神经网络算法）</a></li>
                <li>
                    <a href="#deep-learning-algorithms%e6%b7%b1%e5%ba%a6%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95" aria-label="Deep Learning Algorithms（深度学习算法）">Deep Learning Algorithms（深度学习算法）</a></li>
                <li>
                    <a href="#ensemble-algorithms%e9%9b%86%e6%88%90%e7%ae%97%e6%b3%95" aria-label="Ensemble Algorithms（集成算法）">Ensemble Algorithms（集成算法）</a></li>
                <li>
                    <a href="#other-machine-learning-algorithms%e5%85%b6%e4%bb%96%e6%9c%ba%e5%99%a8%e5%ad%a6%e4%b9%a0%e7%ae%97%e6%b3%95" aria-label="Other Machine Learning Algorithms（其他机器学习算法）">Other Machine Learning Algorithms（其他机器学习算法）</a></li></ul>
                </li>
                <li>
                    <a href="#feature-selection-algorithms" aria-label="Feature Selection Algorithms">Feature Selection Algorithms</a></li>
                <li>
                    <a href="#performance-measures" aria-label="Performance Measures">Performance Measures</a></li>
                <li>
                    <a href="#optimization-algorithms" aria-label="Optimization Algorithms">Optimization Algorithms</a></li>
                <li>
                    <a href="#references" aria-label="References">References</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><blockquote>
<p><strong>Arthur Samuel (1959)</strong>. <em>Machine Learning: The field of study that gives computers the ability to learn without being explicitly learned</em>.</p>
</blockquote>
<blockquote>
<p><strong>Tom Mitchell (1998)</strong>. <em>Well-posed Learning Problem: a computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E</em>.</p>
</blockquote>
 <div align="center">
<img src="/img_ML/1_Five_Steps.PNG" width=650px />
 </div>
 <br>
<h2 id="representation-algorithms-grouped-by-learning-style">Representation Algorithms Grouped By Learning Style<a hidden class="anchor" aria-hidden="true" href="#representation-algorithms-grouped-by-learning-style">#</a></h2>
<p>There are different ways an algorithm can model a problem based on its interaction with the experience or environment or whatever we want to call the input data. It is popular in machine learning and artificial intelligence textbooks to first consider the learning styles that an algorithm can adopt.</p>
<p>This taxonomy or way of organizing machine learning algorithms is useful because it forces you to think about the roles of the input data and the model preparation process and select one that is the most appropriate for your problem in order to get the best result.</p>
<h3 id="supervised-learning-algorithms监督学习">Supervised Learning Algorithms（监督学习）<a hidden class="anchor" aria-hidden="true" href="#supervised-learning-algorithms监督学习">#</a></h3>
<div align="center">
<img src="/img_ML/1_Supervised_Learning.PNG" width=600px/>
</div>
<br>
<p>In supervised learning, we are given a data set and <strong>already know what our correct output</strong> should look like, having the idea that there is a relationship between the input and the output.</p>
<p>Supervised learning problems are categorized into <strong>&ldquo;Regression&rdquo;</strong> and <strong>&ldquo;Classification&rdquo;</strong> problems. In a regression problem, we are trying to predict results within a continuous output, meaning that we are trying to map input variables to some continuous function. In a classification problem, we are instead trying to predict results in a discrete output. In other words, we are trying to map input variables into discrete categories.</p>
<p><strong>Examples of Supervised Learning:</strong>  Regression, Decision Tree, Random Forest, KNN, Logistic Regression etc.</p>
<h3 id="unsupervised-learning-algorithms-无监督学习">Unsupervised Learning Algorithms （无监督学习）<a hidden class="anchor" aria-hidden="true" href="#unsupervised-learning-algorithms-无监督学习">#</a></h3>
<div align="center">
<img src="/img_ML/1_Unsupervised_Learning.PNG" width=600px/>
</div>
<br>
<p>In unsupervised learning, the data has no labels. Unsupervised learning allows us to approach problems with <strong>little or no idea</strong> what our results should look like. We can derive structure from data where we don&rsquo;t necessarily know the effect of the variables. We can derive this structure by <strong>Clustering</strong> the data based on relationships among the variables in the data. With unsupervised learning there is no feedback based on the prediction results.</p>
<p><strong>Examples of Unsupervised Learning:</strong>  Apriori algorithm, K-means.</p>
<h3 id="reinforcement-learning-algorithms-强化学习">Reinforcement Learning Algorithms （强化学习）<a hidden class="anchor" aria-hidden="true" href="#reinforcement-learning-algorithms-强化学习">#</a></h3>
<div align="center">
<img src="/img_ML/1_Reinforcement_Learning.PNG" width=600px/>
</div>
<br>
<p>Lastly, we have reinforcement learning, the latest frontier of machine learning. A reinforcement algorithm learns by trial and error to achieve a clear objective. It tries out lots of different things and is <strong>rewarded</strong> or <strong>penalized</strong> depending on whether its behaviors help or hinder it from reaching its objective. This is like giving and withholding treats when teaching a dog a new trick. Reinforcement learning is the basis of Google’s AlphaGo, the program that famously beat the best human players in the complex game of Go.</p>
<p><strong>Example of Reinforcement Learning:</strong> Markov Decision Process</p>
<h2 id="representation-algorithms-grouped-by-similarity">Representation Algorithms Grouped By Similarity<a hidden class="anchor" aria-hidden="true" href="#representation-algorithms-grouped-by-similarity">#</a></h2>
<p>Algorithms are often grouped by similarity in terms of their function (how they work). For example, tree-based methods, and neural network inspired methods. This is a useful grouping method, but it is not perfect. There are still algorithms that could just as easily fit into multiple categories like Learning Vector Quantization that is both a neural network inspired method and an instance-based method. There are also categories that have the same name that describe the problem and the class of algorithm such as Regression and Clustering.</p>
<br>
<h3 id="regression-algorithms回归算法">Regression Algorithms（回归算法）<a hidden class="anchor" aria-hidden="true" href="#regression-algorithms回归算法">#</a></h3>
<p>Regression is concerned with modeling the relationship between variables that is iteratively refined using a measure of error in the predictions made by the model. Regression methods are a workhorse of statistics and have been co-opted into statistical machine learning. This may be confusing because we can use regression to refer to the class of problem and the class of algorithm. Really, regression is a process.</p>
<p>The most popular regression algorithms are:</p>
<div>
<img src="/img_ML/1_Regression.PNG" width=135px align='right'/>
</div>
<ul>
<li>Ordinary Least Squares Regression (OLSR)</li>
<li><strong>Linear Regression</strong></li>
<li><strong>Logistic Regression</strong></li>
<li>Stepwise Regression</li>
<li>Multivariate Adaptive Regression Splines (MARS)</li>
<li>Locally Estimated Scatterplot Smoothing (LOESS)</li>
</ul>
<br>
<h3 id="instance-based-algorithms基于核的算法">Instance-based Algorithms（基于核的算法）<a hidden class="anchor" aria-hidden="true" href="#instance-based-algorithms基于核的算法">#</a></h3>
<p>Instance-based learning model is a decision problem with instances or examples of training data that are deemed important or required to the model.</p>
<p>Such methods typically build up a database of example data and compare new data to the database using a similarity measure in order to find the best match and make a prediction. For this reason, instance-based methods are also called <strong>winner-take-all methods</strong> and <strong>memory-based learning</strong>. Focus is put on the representation of the stored instances and similarity measures used between instances.</p>
<p>The most popular instance-based algorithms are:</p>
<div>
<img src="/img_ML/1_Instance_Base.PNG" width=135px align='right'/>
</div>
<ul>
<li><strong>K-Nearest Neighbor (KNN)</strong></li>
<li>Learning Vector Quantization (LVQ)</li>
<li>Self-Organizing Map (SOM)</li>
<li>Locally Weighted Learning (LWL)</li>
<li><strong>Support Vector Machines (SVM)</strong></li>
</ul>
<br>
<h3 id="decision-tree-algorithms决策树算法">Decision Tree Algorithms（决策树算法）<a hidden class="anchor" aria-hidden="true" href="#decision-tree-algorithms决策树算法">#</a></h3>
<p>Decision tree methods construct a model of decisions made based on actual values of attributes in the data. Decisions fork in tree structures until a prediction decision is made for a given record. Decision trees are trained on data for classification and regression problems. Decision trees are often fast and accurate and a big favorite in machine learning.</p>
<p>The most popular regularization algorithms are:</p>
<div>
<img src="/img_ML/1_Decision_Tree.PNG" width=135px align='right'/>
</div>
<ul>
<li>Classification and Regression Tree (CART)</li>
<li>Iterative Dichotomiser 3 (ID3)</li>
<li>C4.5 and C5.0 (different versions of a powerful approach)</li>
<li>Chi-squared Automatic Interaction Detection (CHAID)</li>
<li>Decision Stump</li>
<li>M5</li>
<li>Conditional Decision Trees</li>
</ul>
<br>
<h3 id="regularization-algorithms正则化算法">Regularization Algorithms（正则化算法）<a hidden class="anchor" aria-hidden="true" href="#regularization-algorithms正则化算法">#</a></h3>
<p>An extension made to another method (typically regression methods) that penalizes models based on their complexity, favoring simpler models that are also better at generalizing. I have listed regularization algorithms separately here because they are popular, powerful and generally simple modifications made to other methods.</p>
<div>
<img src="/img_ML/1_Regularzation.PNG" width=135px align='right'/>
</div>
<p>The most popular regularization algorithms are:</p>
<ul>
<li>Ridge Regression</li>
<li>Least Absolute Shrinkage and Selection Operator (LASSO)</li>
<li>Elastic Net</li>
<li>Least-Angle Regression (LARS)</li>
</ul>
<br>
<h3 id="bayesian-algorithms贝叶斯算法">Bayesian Algorithms（贝叶斯算法）<a hidden class="anchor" aria-hidden="true" href="#bayesian-algorithms贝叶斯算法">#</a></h3>
<p>Bayesian methods are those that explicitly apply Bayes’ Theorem for problems such as classification and regression.</p>
<p>The most popular Bayesian algorithms are:</p>
<div>
<img src="/img_ML/1_Bayesian.PNG" width=135px align='right'/>
</div>
<ul>
<li><strong>Naive Bayes</strong></li>
<li><strong>Gaussian Naive Bayes</strong></li>
<li>Multinomial Naive Bayes</li>
<li>Averaged One-Dependence Estimators (AODE)</li>
<li>Bayesian Belief Network (BBN)</li>
<li>Bayesian Network (BN)</li>
</ul>
<br>
<h3 id="clustering-algorithms聚类算法">Clustering Algorithms（聚类算法）<a hidden class="anchor" aria-hidden="true" href="#clustering-algorithms聚类算法">#</a></h3>
<p>Clustering, like regression, describes the class of problem and the class of methods. Clustering methods are typically organized by the modeling approaches such as centroid-based and hierarchal. All methods are concerned with using the inherent structures in the data to best organize the data into groups of maximum commonality.</p>
<div>
<img src="/img_ML/1_Clustering.PNG" width=135px align='right'/>
</div>
<p>The most popular clustering algorithms are:</p>
<ul>
<li><strong>K-Means</strong></li>
<li>K-Medians</li>
<li>Expectation Maximisation (EM)</li>
<li><strong>Hierarchical Clustering</strong></li>
</ul>
<br>
<h3 id="association-rule-learning-algorithms关联规则学习算法">Association Rule Learning Algorithms（关联规则学习算法）<a hidden class="anchor" aria-hidden="true" href="#association-rule-learning-algorithms关联规则学习算法">#</a></h3>
<p>Association rule learning methods extract rules that best explain observed relationships between variables in data. These rules can discover important and commercially useful associations in large multidimensional datasets that can be exploited by an organization.</p>
<div>
<img src="/img_ML/1_Association_Rule.PNG" width=135px align='right'/>
</div>
<p>The most popular association rule learning algorithms are:</p>
<ul>
<li>Apriori algorithm</li>
<li>Eclat algorithm</li>
</ul>
<br>
<h3 id="dimensionality-reduction-algorithms降维算法">Dimensionality Reduction Algorithms（降维算法）<a hidden class="anchor" aria-hidden="true" href="#dimensionality-reduction-algorithms降维算法">#</a></h3>
<p>Like clustering methods, dimensionality reduction seek and exploit the inherent structure in the data, but in this case in an unsupervised manner or order to summarize or describe data using less information. This can be useful to visualize dimensional data or to simplify data which can then be used in a supervised learning method. Many of these methods can be adapted for use in classification and regression.</p>
<p>The most popular dimensionality reduction algorithms are:</p>
<div>
<img src="/img_ML/1_Dimensionality_Reduction.PNG" width=135px align='right'/>
</div>
<ul>
<li><strong>Principal Component Analysis (PCA)</strong></li>
<li>Principal Component Regression (PCR)</li>
<li>Partial Least Squares Regression (PLSR)</li>
<li>Sammon Mapping</li>
<li>Multidimensional Scaling (MDS)</li>
<li>Projection Pursuit</li>
<li><strong>Linear Discriminant Analysis (LDA)</strong></li>
<li>Mixture Discriminant Analysis (MDA)</li>
<li><strong>Quadratic Discriminant Analysis (QDA)</strong></li>
<li><strong>Flexible Discriminant Analysis (FDA)</strong></li>
</ul>
<br>
<h3 id="artificial-neural-network-algorithms人工神经网络算法">Artificial Neural Network Algorithms（人工神经网络算法）<a hidden class="anchor" aria-hidden="true" href="#artificial-neural-network-algorithms人工神经网络算法">#</a></h3>
<p>Artificial Neural Networks are models that are inspired by the structure and/or function of biological neural networks. They are a class of pattern matching that are commonly used for regression and classification problems but are really an enormous subfield comprised of hundreds of algorithms and variations for all manner of problem types.</p>
<p>Note that Deep Learning have been separated out from neural networks because of the massive growth and popularity in the field. Here we are concerned with the more classical methods.</p>
<p>The most popular artificial neural network algorithms are:</p>
<div>
<img src="/img_ML/1_Artificial_Neural_Network.PNG" width=135px align='right'/>
</div>
<ul>
<li><strong>Perceptron</strong></li>
<li>Multilayer Perceptrons (MLP)</li>
<li><strong>Back-Propagation</strong></li>
<li>Stochastic Gradient Descent</li>
<li>Hopfield Network</li>
<li><strong>Radial Basis Function Network (RBFN)</strong></li>
</ul>
<br>
<h3 id="deep-learning-algorithms深度学习算法">Deep Learning Algorithms（深度学习算法）<a hidden class="anchor" aria-hidden="true" href="#deep-learning-algorithms深度学习算法">#</a></h3>
<p>Deep Learning methods are a modern update to Artificial Neural Networks that exploit abundant cheap computation. They are concerned with building much larger and more complex neural networks and, as commented on above, many methods are concerned with very large datasets of labelled analog data, such as image, text. audio, and video.</p>
<p>The most popular deep learning algorithms are:</p>
<div>
<img src="/img_ML/1_Deep_Learning.PNG" width=135px align='right'/>
</div>
<ul>
<li><strong>Convolutional Neural Network (CNN)</strong></li>
<li><strong>Recurrent Neural Networks (RNNs)</strong></li>
<li>Long Short-Term Memory Networks (LSTMs)</li>
<li>Stacked Auto-Encoders</li>
<li>Deep Boltzmann Machine (DBM)</li>
<li>Deep Belief Networks (DBN)</li>
</ul>
<br>
<h3 id="ensemble-algorithms集成算法">Ensemble Algorithms（集成算法）<a hidden class="anchor" aria-hidden="true" href="#ensemble-algorithms集成算法">#</a></h3>
<p>Ensemble methods are models composed of multiple weaker models that are independently trained and whose predictions are combined in some way to make the overall prediction. Much effort is put into what types of weak learners to combine and the ways in which to combine them. This is a very powerful class of techniques and as such is very popular.</p>
<p>The most popular ensemble algorithms are:</p>
<div>
<img src="/img_ML/1_Ensemble.PNG" width=135px align='right'/>
</div>
<ul>
<li><strong>Boosting</strong></li>
<li>Bootstrapped Aggregation (Bagging)</li>
<li><strong>AdaBoost</strong></li>
<li>Weighted Average (Blending)</li>
<li>Stacked Generalization (Stacking)</li>
<li>Gradient Boosting Machines (GBM)</li>
<li>Gradient Boosted Regression Trees (GBRT)</li>
<li><strong>Random Forest</strong></li>
</ul>
<br>
<h3 id="other-machine-learning-algorithms其他机器学习算法">Other Machine Learning Algorithms（其他机器学习算法）<a hidden class="anchor" aria-hidden="true" href="#other-machine-learning-algorithms其他机器学习算法">#</a></h3>
<p>Algorithms from specialty subfields of machine learning, such as:</p>
<ul>
<li>Computational intelligence (evolutionary algorithms, etc.)</li>
<li>Computer Vision (CV)</li>
<li>Natural Language Processing (NLP)</li>
<li>Recommender Systems</li>
<li>Reinforcement Learning</li>
<li>Graphical Models</li>
<li>And more…</li>
</ul>
<br>
<h2 id="feature-selection-algorithms">Feature Selection Algorithms<a hidden class="anchor" aria-hidden="true" href="#feature-selection-algorithms">#</a></h2>
<p>When building a machine learning model in real-life, it’s almost rare that all the variables in the dataset are useful to build a model. Adding redundant variables reduces the generalization capability of the model and may also reduce the overall accuracy of a classifier. Furthermore adding more and more variables to a model increases the overall complexity of the model. The goal of feature selection in machine learning is to find the best set of features that allows one to build useful models of studied phenomena. The most popular feature selection algorithms are:</p>
<ul>
<li><strong>Chi-square Test</strong></li>
<li><strong>Fisher’s Score</strong></li>
<li><strong>Correlation Coefficient</strong></li>
<li>Forward Feature Selection</li>
<li>Backward Feature Elimination</li>
<li><strong>L1 Regularization</strong></li>
</ul>
<br>
<h2 id="performance-measures">Performance Measures<a hidden class="anchor" aria-hidden="true" href="#performance-measures">#</a></h2>
<p>Evaluating the machine learning algorithm is an essential part of any project. Most of the times we use classification accuracy to measure the performance of our model, however it is not enough to truly judge our model. The metrics that you choose to evaluate your machine learning model is very important. Choice of metrics influences how the performance of machine learning algorithms is measured and compared. The most popular evaluation performance measures are:</p>
<ul>
<li>Classification Accuracy</li>
<li>Logarithmic Loss</li>
<li>Confusion Matrix</li>
<li>Area under Curve</li>
<li>F1 Score</li>
<li>Mean Absolute Error</li>
<li>Mean Squared Error</li>
</ul>
<br>
<h2 id="optimization-algorithms">Optimization Algorithms<a hidden class="anchor" aria-hidden="true" href="#optimization-algorithms">#</a></h2>
<p>Optimization is the problem of finding a set of inputs to an objective function that results in a maximum or minimum function evaluation. The most common type of optimization problems encountered in machine learning are continuous function optimization, where the input arguments to the function are real-valued numeric values, e.g. floating point values. The output from the function is also a real-valued evaluation of the input values. The most popular optimization algorithms are:</p>
<ul>
<li>Greedy Search</li>
<li>Beam Search</li>
<li>Gradient decent</li>
<li>Conjugate gradient</li>
<li>Momentum</li>
<li>Adagrad</li>
<li>RMSProp</li>
<li>Adam</li>
</ul>
<br>
<h2 id="references">References<a hidden class="anchor" aria-hidden="true" href="#references">#</a></h2>
<p>[1]  Brownlee, J. (2020, August 14). A Tour of Machine Learning Algorithms. Machine Learning Mastery. <a href="https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/">https://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/</a>.</p>
<p>[2]  Contact Centric. (2021, March 26). Machine Learning: A Quick Introduction and Five Core Steps. Centric Consulting. <a href="https://centricconsulting.com/blog/machine-learning-a-quick-introduction-and-five-core-steps/">https://centricconsulting.com/blog/machine-learning-a-quick-introduction-and-five-core-steps/</a>.</p>
<p>[3]  Brownlee, J. (2020, August 20). How to Choose a Feature Selection Method For Machine Learning. Machine Learning Mastery. <a href="https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/">https://machinelearningmastery.com/feature-selection-with-real-and-categorical-data/</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/02_linear_regression/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Linear Regression</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Machine Learning on twitter"
        href="https://twitter.com/intent/tweet/?text=What%20is%20Machine%20Learning&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f&amp;hashtags=MachineLearning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Machine Learning on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f&amp;title=What%20is%20Machine%20Learning&amp;summary=What%20is%20Machine%20Learning&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Machine Learning on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f&title=What%20is%20Machine%20Learning">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Machine Learning on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Machine Learning on whatsapp"
        href="https://api.whatsapp.com/send?text=What%20is%20Machine%20Learning%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share What is Machine Learning on telegram"
        href="https://telegram.me/share/url?text=What%20is%20Machine%20Learning&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f01_what_is-_machine_learning_machine_learning%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
