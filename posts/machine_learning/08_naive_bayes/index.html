<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Naive Bayes | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning, Naive Bayes" />
<meta name="description" content="Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes&#39; theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Naive Bayes" />
<meta property="og:description" content="Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes&#39; theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-19T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-19T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Naive Bayes"/>
<meta name="twitter:description" content="Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes&#39; theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Naive Bayes",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Naive Bayes",
  "name": "Naive Bayes",
  "description": "Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes' theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.",
  "keywords": [
    "Machine Learning", "Naive Bayes"
  ],
  "articleBody": "Naive Bayes classifiers are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on Bayes' theorem, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.\nBeing relatively robust, easy to implement, fast, and accurate, naive Bayes classifiers are used in many different fields. Some examples include the diagnosis of diseases and making decisions about treatment processes, the classification of RNA sequences in taxonomic studies, and spam filtering in e-mail clients.\nHowever, strong violations of the independence assumptions and non-linear classification problems can lead to very poor performances of naive Bayes classifiers. We have to keep in mind that the type of data and the type problem to be solved dictate which classification model we want to choose. In practice, it is always recommended to compare different classification models on the particular dataset and consider the prediction performances as well as computational efficiency.\n 朴素贝叶斯(Naive Bayes) 的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。朴素 : 之所以称为朴素，是因为它假定数据集中的所有要素都是相互独立的。贝叶斯 : 它基于贝叶斯定理。\n Naive Bayes From Bayes Rule, we have:\n$$ P(Y=k|X=x) = \\frac{p(X=x|Y=k)P(Y=k)}{P(X=x)} = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^{K}\\pi_{l}f_{l}(x)} \\\\ $$\nWhere density is $f_{k}(X) = P(X=x|Y=k)$ and prior probability of class $k$ is $\\pi_{k} = P(Y=k)$.\nConditional on class $k$, assume the variables $x_{j}$ are independent:\n$$ f_{k}(x)= \\prod_{j=1}^{p}f_{kj}(x_{j}) \\\\ $$\nWhere p is the number of x‐variables.\n Independence assumption: Conditional on the outcome, there is no multicollinearity This assumption is almost always wrong; but extremely convenient. This method is also called “Idiot’s Bayes”  Plugging the density into Bayes rule, we obtain:\n$$ P(Y=k|X=x) = \\frac{\\pi_{k}f_{k}(x)}{\\sum_{l=1}^{K}\\pi_{l}f_{l}(x)}=\\frac{\\pi_{k}\\prod_{j=1}^{p}f_{kj}(x_{j})}{\\sum_{l=1}^{K}\\pi_{l}\\prod_{j=1}^{p}f_{lj}(x_{j})} \\\\ $$\nThe denominator does not depend on class $k$. It is a constant. To find the class that maximizes the posterior probability, we can ignore the denominator:\n$$ P(Y=k|X=x) \\propto \\pi_{k}\\prod_{j=1}^{p}f_{kj}(x_{j}) \\\\ $$\nwhere $j=1,...,p$ indexes x‐variables\nPredict the class $k$ that maximizes the posterior probability(Decision rule):\n$$ h(x) = \\arg\\max_{k}\\left(P(Y=k)\\prod_{j=1}^{p}P(X_{j}=x_{j}|Y=k)\\right) = \\arg\\max_{k}\\left(\\pi_{k}\\prod_{j=1}^{p}f_{kj}(x_{j})\\right) \\\\ $$\nWhen there are many x‐variables, multiplying many small probabilities may result in an “underflow”. Numerically, all posterior probabilities are 0. It is unclear which 0 is “largest”. We can take the log to avoid this problem. Because it is a monotone function, taking the log does not change which class $k$ gives the maximum posterior probability:\n$$ h(x) = \\arg\\max_{k}(log(\\pi_{k}) + \\sum_{j=1}^{p}log[f_{kj}(x_{j})]) \\\\ $$\nTypically, estimate the prior probability as the fraction of time the class occurs in the training data:\n$$ \\pi_{k} = P(Y=k) = \\frac{n_{k}}{n} \\\\ $$\nEstimate the probability as:\n$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \\frac{n_{kj}}{n_{k}} \\\\ $$\n where $n_{k}$ is the number of obs in class $k$ Where $n_{kj}$ is the number of obs in class $k$ taking the value $x_{j}$  LaPlace Smoothing For a given x‐variable, LaPlace smoothing adds one observation to each x‐category\n$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \\frac{n_{kj}+1}{n_{k}+d_{j}} \\\\ $$\n where $d_{j}$ is the number of categories of the corresponding x‐variable  Smoothing in general  The probability estimates “shrink” away from the extremes Instead of adding just one observation, we can add an arbitrary number of observations, $L$, that controls the amount of shrinking:  $$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \\frac{n_{kj}+L}{n_{k}+L*d_{j}} \\\\ $$\n In the limit, for very large L,  $$ P(X_{j}=x_{j}|Y=k) \\to \\frac{L}{L \\times d_{j}} = \\frac{1}{d_{j}} \\\\ $$\n Gaussian Naive Bayes Gaussian Naive Bayes classifier assumes that the likelihoods are Gaussian:\n$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) =\\frac{1}{\\sqrt{2\\pi\\sigma_{jk}^{2}}}exp[\\frac{-(x_{j}-\\mu_{jk})^{2}}{2\\sigma_{jk}}] \\\\ $$\n Gaussian Naive Bayes is not as common as the case where all x‐ variables are categorical. Maximum likelihood estimate of parameters are:  $$ \\begin{align*} \\mu_{jk} \u0026= \\frac{\\sum_{n=1}^{N}I[Y^{(n)}=k]\\cdot x_{j}^{(n)}}{\\sum_{n=1}^{N}I[Y^{(n)}=k]} \\\\ \\sigma_{jk} \u0026= \\frac{\\sum_{n=1}^{N}I[Y^{(n)}=k]\\cdot (x_{j}^{(n)}-\\mu_{jk})^{2}}{\\sum_{n=1}^{N}I[Y^{(n)}=k]} \\\\ \\end{align*} $$\nLet’s consider a simple example using the Iris data set:\n There are 3 class labels: Setosa, Versicolor, Virginica which we label as $y\\in\\{0,1,2\\}$ There are two explanatory variables (features): $X_{1}$: sepal length and $X_{2}$: sepal width.  For each feature, we calculate the estimated class mean, class variance and prior probability\n Mean: $\\mu_{x_{1}|Y=0}$, $\\mu_{x_{1}|Y=1}$, $\\mu_{x_{1}|Y=2}$ and $\\mu_{x_{2}|Y=0}$, $\\mu_{x_{2}|Y=1}$, $\\mu_{x_{2}|Y=2}$ Variance: $\\sigma_{x_{1}|Y=0}^{2}$, $\\sigma_{x_{1}|Y=1}^{2}$, $\\sigma_{x_{1}|Y=2}^{2}$ and $\\sigma_{x_{2}|Y=0}^{2}$, $\\sigma_{x_{2}|Y=1}^{2}$, $\\sigma_{x_{2}|Y=2}^{2}$ Prior: $P(Y=0)$, $P(Y=1)$, $P(Y=2)$  For any point $(x1,x2)$ we compute the Gaussian Naive Bayes objective function (i.e. the one we are trying to maximize) for each class :\n$$ \\begin{align*} h(x) \u0026= \\arg\\max_{k}[P(Y=k)\\prod_{j=1}^{2}P(X_{j}=x_{j}|Y=k)] \\\\ \u0026= \\arg\\max_{k}[P(X_{1}=x_{1}|Y=k)P(k)\\cdot P(X_{2}=x_{2}|Y=k)P(k)] \\\\ \u0026= \\arg\\max_{k}[\\phi(x_{1}|\\mu_{x_{1}|k},\\sigma_{x_{1}|k}^{2})P(k)\\cdot \\phi(x_{2}|\\mu_{x_{2}|k},\\sigma_{x_{2}|k}^{2})P(k)] \\\\ \\end{align*} $$\n where $\\phi(x_{1}|\\mu_{x_{1}|k},\\sigma_{x_{1}|k}^{2})$ is the PDF of a Gaussian univariate distribution with parameters $\\mu_{x_{1}|k}$, $\\sigma_{x_{1}|k}^{2}$. Repeat this calculation for each class, and then predict the class which has the highest value.  Reference [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Gaussian Naive Bayes Classifier: Iris data set - Data Blog. https://xavierbourretsicotte.github.io/Naive_Bayes_Classifier.html.\n",
  "wordCount" : "784",
  "inLanguage": "en",
  "datePublished": "2021-05-19T00:00:00Z",
  "dateModified": "2021-05-19T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Naive Bayes
    </h1>
    <div class="post-meta">May 19, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#naive-bayes" aria-label="Naive Bayes">Naive Bayes</a><ul>
                        
                <li>
                    <a href="#laplace-smoothing" aria-label="LaPlace Smoothing">LaPlace Smoothing</a></li>
                <li>
                    <a href="#smoothing-in-general" aria-label="Smoothing in general">Smoothing in general</a></li></ul>
                </li>
                <li>
                    <a href="#gaussian-naive-bayes" aria-label="Gaussian Naive Bayes">Gaussian Naive Bayes</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Naive Bayes classifiers</strong> are linear classifiers that are known for being simple yet very efficient. The probabilistic model of naive Bayes classifiers is based on <strong>Bayes' theorem</strong>, and the adjective naive comes from the assumption that the features in a dataset are mutually independent. In practice, the independence assumption is often violated, but naive Bayes classifiers still tend to perform very well under this unrealistic assumption. Especially for small sample sizes, naive Bayes classifiers can outperform the more powerful alternatives.</p>
<p>Being relatively robust, easy to implement, fast, and accurate, naive Bayes classifiers are used in many different fields. Some examples include the diagnosis of diseases and making decisions about treatment processes, the classification of RNA sequences in taxonomic studies, and spam filtering in e-mail clients.</p>
<p>However, <strong>strong violations of the independence assumptions</strong> and <strong>non-linear classification problems</strong> can lead to very poor performances of naive Bayes classifiers. We have to keep in mind that the type of data and the type problem to be solved dictate which classification model we want to choose. In practice, it is always recommended to compare different classification models on the particular dataset and consider the prediction performances as well as computational efficiency.</p>
<br>
<blockquote>
<p><strong>朴素贝叶斯(Naive Bayes)</strong> 的思想基础是：对于给出的待分类项，求解在此项出现的条件下各个类别出现的概率，哪个最大，就认为此待分类项属于哪个类别。<strong>朴素</strong> : 之所以称为朴素，是因为它假定数据集中的所有要素都是相互独立的。<strong>贝叶斯</strong> : 它基于贝叶斯定理。</p>
</blockquote>
<h2 id="naive-bayes">Naive Bayes<a hidden class="anchor" aria-hidden="true" href="#naive-bayes">#</a></h2>
<p>From Bayes Rule, we have:</p>
<p><code>$$ P(Y=k|X=x) = \frac{p(X=x|Y=k)P(Y=k)}{P(X=x)} = \frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}\pi_{l}f_{l}(x)} \\ $$</code></p>
<p>Where density is <code>$f_{k}(X) = P(X=x|Y=k)$</code> and prior probability of class <code>$k$</code> is <code>$\pi_{k} = P(Y=k)$</code>.</p>
<p>Conditional on class <code>$k$</code>, assume the variables <code>$x_{j}$</code> are independent:</p>
<p><code>$$ f_{k}(x)= \prod_{j=1}^{p}f_{kj}(x_{j}) \\ $$</code></p>
<p>Where p is the number of x‐variables.</p>
<ul>
<li>Independence assumption: Conditional on the outcome, there is no multicollinearity</li>
<li>This assumption is almost always wrong; but extremely convenient.</li>
<li>This method is also called &ldquo;Idiot&rsquo;s Bayes&rdquo;</li>
</ul>
<p>Plugging the density into Bayes rule, we obtain:</p>
<p><code>$$ P(Y=k|X=x) = \frac{\pi_{k}f_{k}(x)}{\sum_{l=1}^{K}\pi_{l}f_{l}(x)}=\frac{\pi_{k}\prod_{j=1}^{p}f_{kj}(x_{j})}{\sum_{l=1}^{K}\pi_{l}\prod_{j=1}^{p}f_{lj}(x_{j})} \\ $$</code></p>
<p>The denominator does not depend on class <code>$k$</code>. It is a constant. To find the class that maximizes the posterior probability, we can ignore the denominator:</p>
<p><code>$$ P(Y=k|X=x) \propto \pi_{k}\prod_{j=1}^{p}f_{kj}(x_{j}) \\ $$</code></p>
<p>where <code>$j=1,...,p$</code> indexes x‐variables</p>
<p>Predict the class <code>$k$</code> that maximizes the posterior probability(Decision rule):</p>
<p><code>$$ h(x) = \arg\max_{k}\left(P(Y=k)\prod_{j=1}^{p}P(X_{j}=x_{j}|Y=k)\right) = \arg\max_{k}\left(\pi_{k}\prod_{j=1}^{p}f_{kj}(x_{j})\right) \\ $$</code></p>
<p>When there are many x‐variables, multiplying many small probabilities may result in an &ldquo;underflow&rdquo;. Numerically, all posterior probabilities are 0. It is unclear which 0 is &ldquo;largest&rdquo;. We can take the log to avoid this problem. Because it is a monotone function, taking the log does not change which class <code>$k$</code> gives the maximum posterior probability:</p>
<p><code>$$ h(x) = \arg\max_{k}(log(\pi_{k}) + \sum_{j=1}^{p}log[f_{kj}(x_{j})]) \\ $$</code></p>
<p>Typically, estimate the prior probability as the fraction of time the class occurs in the training data:</p>
<p><code>$$ \pi_{k} = P(Y=k) = \frac{n_{k}}{n} \\ $$</code></p>
<p>Estimate the probability as:</p>
<p><code>$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \frac{n_{kj}}{n_{k}} \\ $$</code></p>
<ul>
<li>where <code>$n_{k}$</code> is the number of obs in class <code>$k$</code></li>
<li>Where <code>$n_{kj}$</code> is the number of obs in class <code>$k$</code> taking the value <code>$x_{j}$</code></li>
</ul>
<h3 id="laplace-smoothing">LaPlace Smoothing<a hidden class="anchor" aria-hidden="true" href="#laplace-smoothing">#</a></h3>
<p>For a given x‐variable, <strong>LaPlace smoothing</strong> adds one observation to each x‐category</p>
<p><code>$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \frac{n_{kj}+1}{n_{k}+d_{j}} \\ $$</code></p>
<ul>
<li>where <code>$d_{j}$</code> is the number of categories of the corresponding x‐variable</li>
</ul>
<h3 id="smoothing-in-general">Smoothing in general<a hidden class="anchor" aria-hidden="true" href="#smoothing-in-general">#</a></h3>
<ul>
<li>The probability estimates “shrink” away from the extremes</li>
<li>Instead of adding just one observation, we can add an arbitrary number of observations, <code>$L$</code>, that controls the amount of shrinking:</li>
</ul>
<p><code>$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) = \frac{n_{kj}+L}{n_{k}+L*d_{j}} \\ $$</code></p>
<ul>
<li>In the limit, for very large L,</li>
</ul>
<p><code>$$ P(X_{j}=x_{j}|Y=k) \to \frac{L}{L \times d_{j}} = \frac{1}{d_{j}} \\ $$</code></p>
<div align="center">
<img src="/img_ML/8_LaPlace.PNG" width=800px/>
</div>
<br>
<h2 id="gaussian-naive-bayes">Gaussian Naive Bayes<a hidden class="anchor" aria-hidden="true" href="#gaussian-naive-bayes">#</a></h2>
<p>Gaussian Naive Bayes classifier assumes that the likelihoods are Gaussian:</p>
<p><code>$$ f_{kj}(x_{j}) = P(X_{j}=x_{j}|Y=k) =\frac{1}{\sqrt{2\pi\sigma_{jk}^{2}}}exp[\frac{-(x_{j}-\mu_{jk})^{2}}{2\sigma_{jk}}] \\ $$</code></p>
<ul>
<li>Gaussian Naive Bayes is not as common as the case where all x‐ variables are categorical.</li>
<li>Maximum likelihood estimate of parameters are:</li>
</ul>
<p><code>$$ \begin{align*} \mu_{jk} &amp;= \frac{\sum_{n=1}^{N}I[Y^{(n)}=k]\cdot x_{j}^{(n)}}{\sum_{n=1}^{N}I[Y^{(n)}=k]} \\ \sigma_{jk} &amp;= \frac{\sum_{n=1}^{N}I[Y^{(n)}=k]\cdot (x_{j}^{(n)}-\mu_{jk})^{2}}{\sum_{n=1}^{N}I[Y^{(n)}=k]} \\ \end{align*} $$</code></p>
<p>Let&rsquo;s consider a simple example using the Iris data set:</p>
<ul>
<li>There are 3 class labels: Setosa, Versicolor, Virginica which we label as <code>$y\in\{0,1,2\}$</code></li>
<li>There are two explanatory variables (features): <code>$X_{1}$</code>: sepal length and <code>$X_{2}$</code>: sepal width.</li>
</ul>
<p>For each feature, we calculate the estimated class mean, class variance and prior probability</p>
<ul>
<li><strong>Mean:</strong> <code>$\mu_{x_{1}|Y=0}$</code>, <code>$\mu_{x_{1}|Y=1}$</code>, <code>$\mu_{x_{1}|Y=2}$</code> and <code>$\mu_{x_{2}|Y=0}$</code>, <code>$\mu_{x_{2}|Y=1}$</code>, <code>$\mu_{x_{2}|Y=2}$</code></li>
<li><strong>Variance:</strong> <code>$\sigma_{x_{1}|Y=0}^{2}$</code>, <code>$\sigma_{x_{1}|Y=1}^{2}$</code>, <code>$\sigma_{x_{1}|Y=2}^{2}$</code> and <code>$\sigma_{x_{2}|Y=0}^{2}$</code>, <code>$\sigma_{x_{2}|Y=1}^{2}$</code>, <code>$\sigma_{x_{2}|Y=2}^{2}$</code></li>
<li><strong>Prior:</strong> <code>$P(Y=0)$</code>, <code>$P(Y=1)$</code>, <code>$P(Y=2)$</code></li>
</ul>
<p>For any point <code>$(x1,x2)$</code> we compute the Gaussian Naive Bayes objective function (i.e. the one we are trying to maximize) for each class :</p>
<p><code>$$ \begin{align*} h(x) &amp;= \arg\max_{k}[P(Y=k)\prod_{j=1}^{2}P(X_{j}=x_{j}|Y=k)] \\ &amp;= \arg\max_{k}[P(X_{1}=x_{1}|Y=k)P(k)\cdot P(X_{2}=x_{2}|Y=k)P(k)] \\ &amp;= \arg\max_{k}[\phi(x_{1}|\mu_{x_{1}|k},\sigma_{x_{1}|k}^{2})P(k)\cdot \phi(x_{2}|\mu_{x_{2}|k},\sigma_{x_{2}|k}^{2})P(k)] \\ \end{align*} $$</code></p>
<ul>
<li>where <code>$\phi(x_{1}|\mu_{x_{1}|k},\sigma_{x_{1}|k}^{2})$</code> is the PDF of a Gaussian univariate distribution with parameters <code>$\mu_{x_{1}|k}$</code>, <code>$\sigma_{x_{1}|k}^{2}$</code>. Repeat this calculation for each class, and then predict the class which has the highest value.</li>
</ul>
<br>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Gaussian Naive Bayes Classifier: Iris data set - Data Blog. <a href="https://xavierbourretsicotte.github.io/Naive_Bayes_Classifier.html">https://xavierbourretsicotte.github.io/Naive_Bayes_Classifier.html</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/naive-bayes/">Naive Bayes</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/09_support_vector_machine/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Support Vector Machine</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/">
    <span class="title">Next Page »</span>
    <br>
    <span>K-Nearest Neighbors</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Naive Bayes on twitter"
        href="https://twitter.com/intent/tweet/?text=Naive%20Bayes&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f&amp;hashtags=MachineLearning%2cNaiveBayes">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Naive Bayes on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f&amp;title=Naive%20Bayes&amp;summary=Naive%20Bayes&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Naive Bayes on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f&title=Naive%20Bayes">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Naive Bayes on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Naive Bayes on whatsapp"
        href="https://api.whatsapp.com/send?text=Naive%20Bayes%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Naive Bayes on telegram"
        href="https://telegram.me/share/url?text=Naive%20Bayes&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f08_naive_bayes%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
