<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>K-Nearest Neighbors | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning, K-Nearest Neighbors" />
<meta name="description" content="K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.
We say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="K-Nearest Neighbors" />
<meta property="og:description" content="K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.
We say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-18T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-18T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="K-Nearest Neighbors"/>
<meta name="twitter:description" content="K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.
We say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "K-Nearest Neighbors",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "K-Nearest Neighbors",
  "name": "K-Nearest Neighbors",
  "description": "K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.\nWe say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset.",
  "keywords": [
    "Machine Learning", "K-Nearest Neighbors"
  ],
  "articleBody": "K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both classification and regression problems.\nWe say that KNN is a non-parametric and lazy learning algorithm. Non-parametric means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. Lazy algorithm means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.\n KNN(K Nearest Neighbor) 是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的 k 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中 K 通常是不大于 20 的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。\n K-Nearest Neighbors   Training example in Euclidean space: $x\\in\\mathbf{R}^{d}$\n  Idea: The value of the target function for a new query is estimated from the known value(s) of the nearest training example(s)\n  Distance typically defined to be Euclidean:\n  $$ \\parallel x^{(a)}-x^{(b)} \\parallel_{2} = \\sqrt{\\sum_{j=1}^{d}(x_{j}^{(a)} - x_{j}^{(b)})^{2}} \\\\ $$\nkNN Algorithom  Load the training and test data Choose the value of K For each point in test data:   find the Euclidean distance to all training data points store the Euclidean distances in a list and sort it choose the first k points assign a class to the test point based on the majority of classes present in the chosen points  End    KNN算法的思想大致为: 在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的 前 K 个数据 ，则该测试数据对应的类别就是 K 个数据中 出现次数最多的那个分类 。如下图，绿色圆要被决定赋予哪个类。如果 K=3，由于红色三角形所占比例为 2/3，绿色圆将被赋予红色三角形那个类，如果 K=5，由于蓝色四方形比例为 3/5，因此绿色圆被赋予蓝色四方形类。\n  Choice of K  $k=1$ can be unstable, particularly if the data are noisy Better to choose an odd number to avoid ties, e.g. $k=3$ or $k=5$ Larger $k$ may lead to better performance. But if we set $k$ too large we may end up looking at samples that are not neighbors (are far away from the query) Rule of thumb is $k , where $n$ is the number of training examples Choose $k$ that yields the smallest error on the test data  Pros and Cons of K-Nearest Neighbors Pros:  It is extremely easy to implement It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc. Since the algorithm requires no training before making predictions, new data can be added seamlessly. There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)  Cons:  Accuracy depends on the quality of the data. Poor at classifying data points in a boundary where they can be classified one way or another. Doesn’t work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension. Has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher. Doesn’t work well with categorical features since it is difficult to find the distance between dimensions with categorical features.  Issues \u0026 Remedies   If some attributes (coordinates of x) have larger ranges, they are treated as more important\n Normalize scale  Simple option: Linearly scale the range of each feature to be, e.g., in range $[0,1]$ Linearly scale each dimension to have 0 mean and variance 1 (compute mean $\\mu$ and variance $\\sigma^{2}$ for an attribute $x_{j}$ and scale: $\\frac{(x_{j} - m)}{\\sigma}$)   Be careful: sometimes scale matters    Irrelevant, correlated attributes add noise to distance measure\n eliminate some attributes or vary and possibly adapt weight of attributes    Non-metric attributes (symbols)\n Hamming distance    Expensive at test time: To find one nearest neighbor of a query point $x$, we must compute the distance to all $N$ training examples. Complexity: $O(kdN)$ for kNN\n Use subset of dimensions Pre-sort training examples into fast data structures (e.g., kd-trees) Compute only an approximate distance (e.g., LSH) Remove redundant data (e.g., condensing)    Storage Requirements: Must store all training data\n Remove redundant data (e.g., condensing) Pre-sorting often increases the storage requirements    High Dimensional Data: “Curse of Dimensionality”\n Required amount of training data increases exponentially with dimension Computational cost also increases    Reference [1] Zemel, R., Urtasun, R., \u0026 Fidler, S. (n.d.). CSC 411: Lecture 05: Nearest Neighbors. https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/05_nn.pdf.\n[2] Sanjay.M. (2018, November 2). KNN using scikit-learn. Medium. https://towardsdatascience.com/knn-using-scikit-learn-c6bed765be75.\n",
  "wordCount" : "769",
  "inLanguage": "en",
  "datePublished": "2021-05-18T00:00:00Z",
  "dateModified": "2021-05-18T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      K-Nearest Neighbors
    </h1>
    <div class="post-meta">May 18, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#k-nearest-neighbors" aria-label="K-Nearest Neighbors">K-Nearest Neighbors</a><ul>
                        
                <li>
                    <a href="#knn-algorithom" aria-label="kNN Algorithom">kNN Algorithom</a></li>
                <li>
                    <a href="#choice-of-k" aria-label="Choice of K">Choice of K</a></li></ul>
                </li>
                <li>
                    <a href="#pros-and-cons-of-k-nearest-neighbors" aria-label="Pros and Cons of K-Nearest Neighbors">Pros and Cons of K-Nearest Neighbors</a><ul>
                        
                <li>
                    <a href="#pros" aria-label="Pros:">Pros:</a></li>
                <li>
                    <a href="#cons" aria-label="Cons:">Cons:</a></li>
                <li>
                    <a href="#issues--remedies" aria-label="Issues &amp;amp; Remedies">Issues &amp; Remedies</a></li></ul>
                </li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>K Nearest Neighbor(KNN) is a very simple, easy to understand, versatile and one of the topmost machine learning algorithms. KNN used in the variety of applications such as finance, healthcare, political science, handwriting detection, image recognition and video recognition. KNN algorithm used for both <strong>classification</strong> and <strong>regression</strong> problems.</p>
<p>We say that KNN is a non-parametric and lazy learning algorithm. <strong>Non-parametric</strong> means there is no assumption for underlying data distribution. In other words, the model structure determined from the dataset. This will be very helpful in practice where most of the real world datasets do not follow mathematical theoretical assumptions. <strong>Lazy algorithm</strong> means it does not need any training data points for model generation. All training data used in the testing phase. This makes training faster and testing phase slower and costlier. Costly testing phase means time and memory. In the worst case, KNN needs more time to scan all data points and scanning all data points will require more memory for storing training data.</p>
<br>
<blockquote>
<p><strong>KNN(K Nearest Neighbor)</strong> 是通过测量不同特征值之间的距离进行分类。它的思路是：如果一个样本在特征空间中的 k 个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别，则该样本也属于这个类别，其中 K 通常是不大于 20 的整数。KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
</blockquote>
<h2 id="k-nearest-neighbors">K-Nearest Neighbors<a hidden class="anchor" aria-hidden="true" href="#k-nearest-neighbors">#</a></h2>
<ul>
<li>
<p>Training example in Euclidean space: <code>$x\in\mathbf{R}^{d}$</code></p>
</li>
<li>
<p><strong>Idea:</strong> The value of the target function for a new query is estimated from the known value(s) of the nearest training example(s)</p>
</li>
<li>
<p>Distance typically defined to be Euclidean:</p>
</li>
</ul>
<p><code>$$ \parallel x^{(a)}-x^{(b)} \parallel_{2} = \sqrt{\sum_{j=1}^{d}(x_{j}^{(a)} - x_{j}^{(b)})^{2}} \\ $$</code></p>
<h3 id="knn-algorithom">kNN Algorithom<a hidden class="anchor" aria-hidden="true" href="#knn-algorithom">#</a></h3>
<ol>
<li>Load the training and test data</li>
<li>Choose the value of K</li>
<li>For each point in test data:</li>
</ol>
<ul>
<li>find the Euclidean distance to all training data points</li>
<li>store the Euclidean distances in a list and sort it</li>
<li>choose the first k points</li>
<li>assign a class to the test point based on the majority of classes present in the chosen points</li>
</ul>
<ol start="4">
<li>End</li>
</ol>
<div align="center">
<img src="/img_ML/7_knn_algorithom.PNG" width=800px/>
</div>
<br>
<blockquote>
<p>KNN算法的思想大致为: 在训练集中数据和标签已知的情况下，输入测试数据，将测试数据的特征与训练集中对应的特征进行相互比较，找到训练集中与之最为相似的 <strong>前 K 个数据</strong> ，则该测试数据对应的类别就是 K 个数据中 <strong>出现次数最多的那个分类</strong> 。如下图，绿色圆要被决定赋予哪个类。如果 K=3，由于红色三角形所占比例为 2/3，绿色圆将被赋予红色三角形那个类，如果 K=5，由于蓝色四方形比例为 3/5，因此绿色圆被赋予蓝色四方形类。</p>
</blockquote>
<br>
<div align="center">
<img src="/img_ML/7_knn_cercle.PNG" width=300px/>
</div>
<br>
<h3 id="choice-of-k">Choice of K<a hidden class="anchor" aria-hidden="true" href="#choice-of-k">#</a></h3>
<ul>
<li><code>$k=1$</code> can be unstable, particularly if the data are noisy</li>
<li>Better to choose an odd number to avoid ties, e.g. <code>$k=3$</code> or <code>$k=5$</code></li>
<li>Larger <code>$k$</code> may lead to better performance. But if we set <code>$k$</code> too large we may end up looking at samples that are not neighbors (are far away from the query)</li>
<li>Rule of thumb is <code>$k &lt; \mathrm{Sqrt}(n)$</code>, where <code>$n$</code> is the number of training examples</li>
<li>Choose <code>$k$</code> that yields the smallest error on the test data</li>
</ul>
<br>
<h2 id="pros-and-cons-of-k-nearest-neighbors">Pros and Cons of K-Nearest Neighbors<a hidden class="anchor" aria-hidden="true" href="#pros-and-cons-of-k-nearest-neighbors">#</a></h2>
<h3 id="pros">Pros:<a hidden class="anchor" aria-hidden="true" href="#pros">#</a></h3>
<ul>
<li>It is extremely easy to implement</li>
<li>It is lazy learning algorithm and therefore requires no training prior to making real time predictions. This makes the KNN algorithm much faster than other algorithms that require training e.g SVM, linear regression, etc.</li>
<li>Since the algorithm requires no training before making predictions, new data can be added seamlessly.</li>
<li>There are only two parameters required to implement KNN i.e. the value of K and the distance function (e.g. Euclidean or Manhattan etc.)</li>
</ul>
<h3 id="cons">Cons:<a hidden class="anchor" aria-hidden="true" href="#cons">#</a></h3>
<ul>
<li>Accuracy depends on the quality of the data.</li>
<li>Poor at classifying data points in a boundary where they can be classified one way or another.</li>
<li>Doesn&rsquo;t work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension.</li>
<li>Has a high prediction cost for large datasets. This is because in large datasets the cost of calculating distance between new point and each existing point becomes higher.</li>
<li>Doesn&rsquo;t work well with categorical features since it is difficult to find the distance between dimensions with categorical features.</li>
</ul>
<h3 id="issues--remedies">Issues &amp; Remedies<a hidden class="anchor" aria-hidden="true" href="#issues--remedies">#</a></h3>
<ul>
<li>
<p>If some attributes (coordinates of x) have <strong>larger ranges</strong>, they are treated as more important</p>
<ul>
<li>Normalize scale
<ul>
<li>Simple option: Linearly scale the range of each feature to be, e.g., in range <code>$[0,1]$</code></li>
<li>Linearly scale each dimension to have 0 mean and variance 1 (compute mean <code>$\mu$</code> and variance <code>$\sigma^{2}$</code> for an attribute <code>$x_{j}$</code> and scale: <code>$\frac{(x_{j} - m)}{\sigma}$</code>)</li>
</ul>
</li>
<li>Be careful: sometimes scale matters</li>
</ul>
</li>
<li>
<p><strong>Irrelevant</strong>, <strong>correlated</strong> attributes add noise to distance measure</p>
<ul>
<li>eliminate some attributes</li>
<li>or vary and possibly adapt weight of attributes</li>
</ul>
</li>
<li>
<p><strong>Non-metric</strong> attributes (symbols)</p>
<ul>
<li>Hamming distance</li>
</ul>
</li>
<li>
<p><strong>Expensive at test time:</strong> To find one nearest neighbor of a query point <code>$x$</code>, we must compute the distance to all <code>$N$</code> training examples. Complexity: <code>$O(kdN)$</code> for kNN</p>
<ul>
<li>Use subset of dimensions</li>
<li>Pre-sort training examples into fast data structures (e.g., kd-trees)</li>
<li>Compute only an approximate distance (e.g., LSH)</li>
<li>Remove redundant data (e.g., condensing)</li>
</ul>
</li>
<li>
<p><strong>Storage Requirements:</strong> Must store all training data</p>
<ul>
<li>Remove redundant data (e.g., condensing)</li>
<li>Pre-sorting often increases the storage requirements</li>
</ul>
</li>
<li>
<p><strong>High Dimensional Data:</strong> “Curse of Dimensionality”</p>
<ul>
<li>Required amount of training data increases exponentially with dimension</li>
<li>Computational cost also increases</li>
</ul>
</li>
</ul>
<br>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] Zemel, R., Urtasun, R., &amp; Fidler, S. (n.d.). CSC 411: Lecture 05: Nearest Neighbors. <a href="https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/05_nn.pdf">https://www.cs.toronto.edu/~urtasun/courses/CSC411_Fall16/05_nn.pdf</a>.</p>
<p>[2] Sanjay.M. (2018, November 2). KNN using scikit-learn. Medium. <a href="https://towardsdatascience.com/knn-using-scikit-learn-c6bed765be75">https://towardsdatascience.com/knn-using-scikit-learn-c6bed765be75</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/k-nearest-neighbors/">K-Nearest Neighbors</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/08_naive_bayes/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Naive Bayes</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/">
    <span class="title">Next Page »</span>
    <br>
    <span>Fisher’s Linear Discriminant Analysis</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share K-Nearest Neighbors on twitter"
        href="https://twitter.com/intent/tweet/?text=K-Nearest%20Neighbors&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f&amp;hashtags=MachineLearning%2cK-NearestNeighbors">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share K-Nearest Neighbors on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f&amp;title=K-Nearest%20Neighbors&amp;summary=K-Nearest%20Neighbors&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share K-Nearest Neighbors on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f&title=K-Nearest%20Neighbors">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share K-Nearest Neighbors on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share K-Nearest Neighbors on whatsapp"
        href="https://api.whatsapp.com/send?text=K-Nearest%20Neighbors%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share K-Nearest Neighbors on telegram"
        href="https://telegram.me/share/url?text=K-Nearest%20Neighbors&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f07_k_nearest_neighbors%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
