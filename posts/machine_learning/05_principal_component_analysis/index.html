<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Principal Component Analysis | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning, PCA" />
<meta name="description" content="Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.
Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Principal Component Analysis" />
<meta property="og:description" content="Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.
Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-16T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-16T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Principal Component Analysis"/>
<meta name="twitter:description" content="Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.
Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Principal Component Analysis",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Principal Component Analysis",
  "name": "Principal Component Analysis",
  "description": "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity.",
  "keywords": [
    "Machine Learning", "PCA"
  ],
  "articleBody": "Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\nReducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.\nThe goal is to preserve as much of the variance in the original data as possible in the new coordinate systems. Give data on $d$ variables, the hope is that the data points will lie mainly in a linear subspace of dimension lower than $d$. In practice, the data will usually not lie precisely in some lower dimensional subspace. The new variables that form a new coordinate system are called principal components (PCs).\n 主成分分析(principal components analysis,PCA) 是一种简化数据集的技术。它是一个线性变换。这个变换把数据变换到一个新的坐标系统中，其中，第一个新坐标轴选择是原始数据中 方差最大 的方向，第二个新坐标轴选取是与 第一个坐标轴正交的平面中 使得方差最大的，第三个轴是与第 1,2 个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面 $k$个坐标轴中，后面的坐标轴所含的方差几乎为 0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴. 这相当于只保留包含绝大部分方差的维度特征，而 忽略包含方差几乎为 0 的特征维度 ，实现对数据特征的 降维处理 。\n Principal Component Analysis  PCs are denoted by $u_{1},u_{2},...,u_{d}.$ The principal components form a basis for the data. Since PCs are orthogonal linear transformations of the original variables there is at most $d$ PCs. Normally, not all of the $d$ PCs are used but rather a subset of $p$ PCs, $u_{1},u_{2},...,u_{p}.$ In order to approximate the space spanned by the original data points $x = \\begin{bmatrix} x_{1} \\\\ \\cdots \\\\ x_{d} \\\\ \\end{bmatrix}_{\\ dx1}$ We can choose $p$ based on what percentage of the variance of the original data we would like to maintain.  The first PC, $u_{1}$ is called first principal component and has the maximum variance, thus it accounts for the most significant variance in the data.\nThe second PC, $u_{2}$ is called second principal component and has the second highest variance and so on until PC ud which has the minimum variance.\n In order to capture as much of the variability as possible, let us choose the first principal component, denoted by $u_{1}$, to capture the maximum variance. Suppose that all centred observations are stacked into the columns of a $d \\times n$ matrix $X = \\begin{bmatrix} x_{1} ... x_{n} \\\\ \\end{bmatrix}_{d \\times n}$, where each column corresponds to a $d$-dimensional observation and there are $n$ observations. The projection of $n$, $d$-dimensional observations on the first principal component $u_{1}$ is $u_{1}^{T}X$. We want projection on this first dimension to have maximum variance.\n$$ \\frac{1}{2N}\\sum_{n=1}^{N}(u_{1}^{T}x_{n}-u_{1}^{T}\\bar{x}_{n})^{2} = Var(u_{1}^{T}X)= u_{1}^{T}Su_{1} \\\\ $$\nWhere $S$ is the $d \\times d$ sample covariance matrix of $X$.\nClearly $Var(u_{1}^{T}X)$ can be made arbitrarily large by increasing the magnitude of $u_{1}$. This means that the variance stated above has no upper limit and so we can not find the maximum. To solve this problem, we choose $u_{1}$ to maximize $u_{1}^{T}Su_{1}$ while constraining $u_{1}$ to have unit length. Therefore, we can rewrite the above optimization problem as:\n$$ \\mathrm{max} \\ \\ u_{1}^{T}Su_{1} \\\\ \\mathrm{Subject \\ \\ to} \\ \\ u_{1}^{T}u_{1} = 1 \\\\ $$\nTo solve this optimization problem a Lagrange multiplier $\\lambda$ is introduced:\n$$ L(u_{1}, \\lambda) = u_{1}^{T}Su_{1}-\\lambda(u_{1}^{T}u_{1}-1) \\\\ $$\nLagrange Multiplier for PCA Lagrange multipliers are used to find the maximum or minimum of a function $f (x, y)$ subject to constraint $g(x, y) = c$. we define a new constant $\\lambda$ called a Lagrange Multiplier and we form the Lagrangian,\n$$ L(x,y,\\lambda) = f(x,y)-\\lambda g(x,y) \\\\ $$\nIf $f(x^{*},y^{*})$ is the max of $f(x, y)$ , there exists $\\lambda^{*}$ such that $(x^{*},y^{*},\\lambda^{*})$ is a stationary point of $L$ (partial derivatives are 0). In addition $(x^{*},y^{*})$ is a point in which functions $f$ and $g$ touch but do not cross. At this point, the tangents of $f$ and $g$ are parallel or gradients of $f$ and $g$ are parallel, such that:\n$$ \\nabla_{x,y}f = \\lambda\\nabla_{x,y}g \\\\ $$\n$$ \\begin{align*} \\mathrm{Where}, \\ \\nabla_{x,y}f \u0026= (\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}) \\mathrm{\\to the \\ gradient \\ of} \\ f \\\\ \\nabla_{x,y}g \u0026= (\\frac{\\partial g}{\\partial x}, \\frac{\\partial g}{\\partial y}) \\mathrm{\\to the \\ gradient \\ of} \\ g \\\\ \\end{align*} $$\nDifferentiating with respect to $u_{1}$ gives $d$ equations,\n$$ \\frac{\\partial L(u_{1}, \\lambda)}{\\partial u_{1}} = 2S u_{1} - 2\\lambda u_{1} = 0 \\\\ S u_{1} = \\lambda u_{1} \\\\ $$\nPremultiplying both sides by $u_{1}^{T}$ we have:\n$$ u_{1}^{T}S u_{1} = \\lambda u_{1}^{T} u_{1} = \\lambda \\\\ $$\n$ u_{1}^{T}S u_{1}$ is maximized if $\\lambda$ is the largest eigenvalue of $S$. Clearly $\\lambda$ and $u_{1}$ are an eigenvalue and an eigenvector of $S$. Differentiating $L(u_{1}, \\lambda)$ with respect to the Lagrange multiplier $\\lambda$ gives us back the constraint:\n$$ u_{1}^{T}u_{1} = 1 \\\\ $$\nThis shows that the first principal component is given by the eigenvector with the largest associated eigenvalue of the sample covariance matrix $S$. A similar argument can show that the $p$ dominant eigenvectors of covariance matrix $S$ determine the first $p$ principal components.\nNote that the PCs decompose the total variance in the data in the following way :\n$$ \\sum_{i=1}^{d}Var(u_{i}^{T}X) = \\sum_{i=1}^{d}u_{1}^{T}Su_{1} = \\sum_{i=1}^{d}(\\lambda_{i}) = Tr(S) = Var(X) \\\\ $$\n$Var(u_{1}^{T}X)$ is maximized if $u_{1}$ is the eigenvector of $S$ with the corresponding maximum eigenvalue. Each successive PC can be generated in the above manner by taking the eigenvectors of $S$ that correspond to the eigenvalues:\n$$ \\lambda_{1} \\geq ... \\geq \\lambda_{d} \\\\ $$\nSuch that\n$$ Var(u_{1}^{T}X) \\geq ... \\geq Var(u_{d}^{T}X) \\\\ $$\nDirect PCA Algorithm  Normalize the data: Set $X = X - \\bar{X}$ Recover basis (PCs): Calculate $XX^{T}=\\sum_{i=1}^{n}x_{i}x_{i}^{T}$ and let $U=$ eigenvectors of $XX^{T}$ corresponding to the top $p$ eigenvalues. Encode training data: $Y=U^{T}X$ Where $Y$ is a $p \\times n$ matrix of encodings of the original data. Reconstruct training data: $\\hat{X} = UY=UU^{T}X$. Encode test example: $y=U^{T}x$ where $y$ is a $p$-dimensional encoding of x. Reconstruct test example: $\\hat{x} = Uy = UU^{T}x$.  $$ U^{T}_{p \\times d} \\cdot (X=\\begin{bmatrix} x_{1} \\\\ x_{2} \\\\ \\vdots \\\\ x_{n} \\\\ \\end{bmatrix}_{\\ d \\times n})\\longrightarrow (Y=\\begin{bmatrix} y_{1} \\\\ y_{2} \\\\ \\cdots \\\\ y_{p} \\\\ \\end{bmatrix}_{\\ p \\times n}) \\\\ $$\n 基于特征值分解(Eigendecomposition)协方差矩阵实现PCA算法 : 对数据去中心化后，计算协方差矩阵(Covariance Matrix) $XX^{T}$。用 特征值分解方法 求协方差矩阵的特征值(Eigenvalues)与特征向量(Eigenvectors)。对特征值从大到小排序，选择其中最大的 k 个。然后将其对应的 k 个特征向量分别作为行向量组成特征向量矩阵。最后将数据转换到 k 个特征向量构建的新空间中。\n A unique solution can be obtained by finding the singular value decomposition of $X$. For each rank $p$, $U$ consists of the first $p$ columns of $U$.\n$$ X=U\\Sigma V^{T} \\\\ $$\nThe columns of $U$ in the SVD contain the eigenvectors of $XX^{T}$.\n 基于SVD(Singular Value Decomposition)分解协方差矩阵实现PCA算法 : 对数据去中心化后，计算协方差矩阵(Covariance Matrix) $XX^{T}$。通过 SVD 计算协方差矩阵的特征值(Eigenvalues)与特征向量(Eigenvectors)。 对特征值从大到小排序，选择其中最大的 k 个。然后将其对应的 k 个特征向量分别作为行向量组成特征向量矩阵。最后将数据转换到 k 个特征向量构建的新空间中。\n  当样本数多、样本特征数也多的时候,先求出协方差矩阵 $XX^{T}$ 的计算是很大的。然而有一些SVD的实现算法可以先 不求出协方差矩阵也能求出右奇异矩阵 。也就是说，我们的 PCA 算法可以不用做特征分解而是通过 SVD 来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的 PCA 算法的背后真正的实现就是用的 SVD，而不是特征值分解\n Referencce [1] Raschka, S. (2014, April 13). Implementing a Principal Component Analysis (PCA). Dr. Sebastian Raschka. https://sebastianraschka.com/Articles/2014_pca_step_by_step.htmlprincipal-component-analysis-pca-vs-multiple-discriminant-analysis-mda.\n[2] 张洋. (n.d.). 数据的向量表示及降维问题. CodingLabs. http://blog.codinglabs.org/articles/pca-tutorial.html.\n",
  "wordCount" : "1128",
  "inLanguage": "en",
  "datePublished": "2021-05-16T00:00:00Z",
  "dateModified": "2021-05-16T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Principal Component Analysis
    </h1>
    <div class="post-meta">May 16, 2021&nbsp;·&nbsp;6 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#principal-component-analysis" aria-label="Principal Component Analysis">Principal Component Analysis</a><ul>
                        
                <li>
                    <a href="#lagrange-multiplier-for-pca" aria-label="Lagrange Multiplier for PCA">Lagrange Multiplier for PCA</a></li>
                <li>
                    <a href="#direct-pca-algorithm" aria-label="Direct PCA Algorithm">Direct PCA Algorithm</a></li></ul>
                </li>
                <li>
                    <a href="#referencce" aria-label="Referencce">Referencce</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.</p>
<p>Reducing the number of variables of a data set naturally comes at the expense of accuracy, but the trick in dimensionality reduction is to trade a little accuracy for simplicity. Because smaller data sets are easier to explore and visualize and make analyzing data much easier and faster for machine learning algorithms without extraneous variables to process.</p>
<p>The goal is to preserve as much of the variance in the original data as possible in the new coordinate systems.
Give data on <code>$d$</code> variables, the hope is that the data points will lie mainly in a linear subspace of dimension lower than <code>$d$</code>. In practice, the data will usually not lie precisely in some lower dimensional subspace. The new variables that form a new coordinate system are called <strong>principal components</strong> (PCs).</p>
<blockquote>
<p><strong>主成分分析(principal components analysis,PCA)</strong> 是一种简化数据集的技术。它是一个线性变换。这个变换把数据变换到一个新的坐标系统中，其中，第一个新坐标轴选择是原始数据中 <strong>方差最大</strong> 的方向，第二个新坐标轴选取是与 <strong>第一个坐标轴正交的平面中</strong> 使得方差最大的，第三个轴是与第 1,2 个轴正交的平面中方差最大的。依次类推，可以得到n个这样的坐标轴。通过这种方式获得的新的坐标轴，我们发现，大部分方差都包含在前面 <code>$k$</code>个坐标轴中，后面的坐标轴所含的方差几乎为 0。于是，我们可以忽略余下的坐标轴，只保留前面k个含有绝大部分方差的坐标轴. 这相当于只保留包含绝大部分方差的维度特征，而 <strong>忽略包含方差几乎为 0 的特征维度</strong> ，实现对数据特征的 <strong>降维处理</strong> 。</p>
</blockquote>
<br>
<h2 id="principal-component-analysis">Principal Component Analysis<a hidden class="anchor" aria-hidden="true" href="#principal-component-analysis">#</a></h2>
<ul>
<li>PCs are denoted by <code>$u_{1},u_{2},...,u_{d}.$</code></li>
<li>The principal components form a basis for the data.</li>
<li>Since PCs are orthogonal linear transformations of the original variables there is at most <code>$d$</code> PCs.</li>
<li>Normally, not all of the <code>$d$</code> PCs are used but rather a subset of <code>$p$</code> PCs, <code>$u_{1},u_{2},...,u_{p}.$</code></li>
<li>In order to approximate the space spanned by the original data points
<code>$x = \begin{bmatrix} x_{1}    \\ \cdots   \\ x_{d}   \\ \end{bmatrix}_{\ dx1}$</code>
We can choose <code>$p$</code> based on what percentage of the variance of the original data we would like to maintain.</li>
</ul>
<p>The first PC, <code>$u_{1}$</code> is called <strong>first principal component</strong> and has the maximum variance, thus it accounts for the most
significant variance in the data.</p>
<p>The second PC, <code>$u_{2}$</code> is called <strong>second principal component</strong> and has the second highest variance and so on until PC ud which has the minimum variance.</p>
<div align="center">
<img src="/img_ML/5_PC12.PNG" width=400px/>
</div>
<br>
<p>In order to capture as much of the variability as possible, let us choose the first principal component, denoted by
<code>$u_{1}$</code>, to capture the maximum variance. Suppose that all centred observations are stacked into the columns of a <code>$d \times n$</code> matrix
<code>$X = \begin{bmatrix} x_{1} ... x_{n}  \\ \end{bmatrix}_{d \times n}$</code>,
where each column corresponds to a <code>$d$</code>-dimensional observation and there are <code>$n$</code> observations. The projection of <code>$n$</code>, <code>$d$</code>-dimensional observations on the first principal component <code>$u_{1}$</code> is <code>$u_{1}^{T}X$</code>. We want projection on this first dimension to have maximum variance.</p>
<p><code>$$ \frac{1}{2N}\sum_{n=1}^{N}(u_{1}^{T}x_{n}-u_{1}^{T}\bar{x}_{n})^{2} = Var(u_{1}^{T}X)= u_{1}^{T}Su_{1} \\ $$</code></p>
<p>Where <code>$S$</code> is the <code>$d \times d$</code> sample covariance matrix of <code>$X$</code>.</p>
<p>Clearly <code>$Var(u_{1}^{T}X)$</code> can be made arbitrarily large by increasing the magnitude of <code>$u_{1}$</code>. This means that the variance stated above has no upper limit and so we can not find the maximum. To solve this problem, we choose <code>$u_{1}$</code> to maximize <code>$u_{1}^{T}Su_{1}$</code> while constraining <code>$u_{1}$</code> to have unit length. Therefore, we can rewrite the above optimization problem as:</p>
<p><code>$$ \mathrm{max} \ \ u_{1}^{T}Su_{1} \\ \mathrm{Subject \ \ to} \ \ u_{1}^{T}u_{1} = 1 \\ $$</code></p>
<p>To solve this optimization problem a Lagrange multiplier <code>$\lambda$</code> is introduced:</p>
<p><code>$$ L(u_{1}, \lambda) = u_{1}^{T}Su_{1}-\lambda(u_{1}^{T}u_{1}-1) \\ $$</code></p>
<h3 id="lagrange-multiplier-for-pca">Lagrange Multiplier for PCA<a hidden class="anchor" aria-hidden="true" href="#lagrange-multiplier-for-pca">#</a></h3>
<p>Lagrange multipliers are used to find the maximum or minimum of a function <code>$f (x, y)$</code> subject to constraint
<code>$g(x, y) = c$</code>. we define a new constant <code>$\lambda$</code> called a <strong>Lagrange Multiplier</strong> and we form the Lagrangian,</p>
<p><code>$$ L(x,y,\lambda) = f(x,y)-\lambda g(x,y) \\ $$</code></p>
<p>If <code>$f(x^{*},y^{*})$</code> is the max of <code>$f(x, y)$</code> , there exists <code>$\lambda^{*}$</code> such that <code>$(x^{*},y^{*},\lambda^{*})$</code> is a stationary point of <code>$L$</code> (partial derivatives are 0). In addition <code>$(x^{*},y^{*})$</code> is a point in which functions <code>$f$</code> and <code>$g$</code> touch but do not cross. At this point, the tangents of <code>$f$</code> and <code>$g$</code> are parallel or gradients of <code>$f$</code> and <code>$g$</code> are parallel, such that:</p>
<p><code>$$ \nabla_{x,y}f = \lambda\nabla_{x,y}g \\ $$</code></p>
<p><code>$$ \begin{align*} \mathrm{Where}, \ \nabla_{x,y}f &amp;= (\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}) \mathrm{\to the \ gradient \ of} \ f \\ \nabla_{x,y}g &amp;= (\frac{\partial g}{\partial x}, \frac{\partial g}{\partial y}) \mathrm{\to the \ gradient \ of} \ g \\ \end{align*} $$</code></p>
<p>Differentiating with respect to <code>$u_{1}$</code> gives <code>$d$</code> equations,</p>
<p><code>$$ \frac{\partial L(u_{1}, \lambda)}{\partial u_{1}} = 2S u_{1} - 2\lambda u_{1} = 0 \\ S u_{1} = \lambda u_{1} \\ $$</code></p>
<p>Premultiplying both sides by <code>$u_{1}^{T}$</code> we have:</p>
<p><code>$$ u_{1}^{T}S u_{1} = \lambda u_{1}^{T} u_{1} = \lambda \\ $$</code></p>
<p><code>$ u_{1}^{T}S u_{1}$</code> is maximized if <code>$\lambda$</code> is the largest eigenvalue of <code>$S$</code>. Clearly <code>$\lambda$</code> and <code>$u_{1}$</code> are an eigenvalue and an eigenvector of <code>$S$</code>. Differentiating <code>$L(u_{1}, \lambda)$</code> with respect to the Lagrange multiplier <code>$\lambda$</code> gives us back the constraint:</p>
<p><code>$$ u_{1}^{T}u_{1} = 1 \\ $$</code></p>
<p>This shows that the first principal component is given by the eigenvector with the largest associated eigenvalue of the sample covariance matrix <code>$S$</code>. A similar argument can show that the <code>$p$</code> dominant eigenvectors of covariance matrix <code>$S$</code> determine the first <code>$p$</code> principal components.</p>
<p>Note that the PCs decompose the total variance in the data in the following way :</p>
<p><code>$$ \sum_{i=1}^{d}Var(u_{i}^{T}X) = \sum_{i=1}^{d}u_{1}^{T}Su_{1} = \sum_{i=1}^{d}(\lambda_{i}) = Tr(S) = Var(X) \\ $$</code></p>
<p><code>$Var(u_{1}^{T}X)$</code> is maximized if <code>$u_{1}$</code> is the eigenvector of <code>$S$</code> with the corresponding maximum eigenvalue. Each successive PC can be generated in the above manner by taking the eigenvectors of <code>$S$</code> that correspond to the eigenvalues:</p>
<p><code>$$ \lambda_{1} \geq ... \geq \lambda_{d} \\ $$</code></p>
<p>Such that</p>
<p><code>$$ Var(u_{1}^{T}X) \geq ... \geq Var(u_{d}^{T}X) \\ $$</code></p>
<h3 id="direct-pca-algorithm">Direct PCA Algorithm<a hidden class="anchor" aria-hidden="true" href="#direct-pca-algorithm">#</a></h3>
<ul>
<li><strong>Normalize the data:</strong> Set <code>$X = X - \bar{X}$</code></li>
<li><strong>Recover basis (PCs):</strong> Calculate <code>$XX^{T}=\sum_{i=1}^{n}x_{i}x_{i}^{T}$</code> and let <code>$U=$</code> eigenvectors of <code>$XX^{T}$</code> corresponding to the top <code>$p$</code> eigenvalues.</li>
<li><strong>Encode training data:</strong> <code>$Y=U^{T}X$</code> Where <code>$Y$</code> is a <code>$p \times n$</code> matrix of encodings of the original data.</li>
<li><strong>Reconstruct training data:</strong> <code>$\hat{X} = UY=UU^{T}X$</code>.</li>
<li><strong>Encode test example:</strong> <code>$y=U^{T}x$</code> where <code>$y$</code> is a <code>$p$</code>-dimensional encoding of x.</li>
<li><strong>Reconstruct test example:</strong> <code>$\hat{x} = Uy = UU^{T}x$</code>.</li>
</ul>
<p><code>$$ U^{T}_{p \times d} \cdot (X=\begin{bmatrix} x_{1}    \\ x_{2}    \\ \vdots        \\ x_{n}    \\ \end{bmatrix}_{\ d \times n})\longrightarrow (Y=\begin{bmatrix} y_{1}    \\ y_{2}    \\ \cdots        \\ y_{p}    \\ \end{bmatrix}_{\ p \times n}) \\ $$</code></p>
<blockquote>
<p><strong>基于特征值分解(Eigendecomposition)协方差矩阵实现PCA算法</strong> : 对数据去中心化后，计算协方差矩阵(Covariance Matrix) <code>$XX^{T}$</code>。用 <strong>特征值分解方法</strong> 求协方差矩阵的特征值(Eigenvalues)与特征向量(Eigenvectors)。对特征值从大到小排序，选择其中最大的 k 个。然后将其对应的 k 个特征向量分别作为行向量组成特征向量矩阵。最后将数据转换到 k 个特征向量构建的新空间中。</p>
</blockquote>
<p>A unique solution can be obtained by finding the <strong>singular value decomposition</strong> of <code>$X$</code>. For each rank <code>$p$</code>, <code>$U$</code> consists of the first <code>$p$</code> columns of <code>$U$</code>.</p>
<p><code>$$ X=U\Sigma V^{T} \\ $$</code></p>
<p><strong>The columns of <code>$U$</code> in the SVD contain the eigenvectors of <code>$XX^{T}$</code>.</strong></p>
<blockquote>
<p><strong>基于SVD(Singular Value Decomposition)分解协方差矩阵实现PCA算法</strong> : 对数据去中心化后，计算协方差矩阵(Covariance Matrix) <code>$XX^{T}$</code>。通过 <strong>SVD</strong> 计算协方差矩阵的特征值(Eigenvalues)与特征向量(Eigenvectors)。 对特征值从大到小排序，选择其中最大的 k 个。然后将其对应的 k 个特征向量分别作为行向量组成特征向量矩阵。最后将数据转换到 k 个特征向量构建的新空间中。</p>
</blockquote>
<blockquote>
<p>当样本数多、样本特征数也多的时候,先求出协方差矩阵 <code>$XX^{T}$</code> 的计算是很大的。然而有一些SVD的实现算法可以先 <strong>不求出协方差矩阵也能求出右奇异矩阵</strong> 。也就是说，我们的 PCA 算法可以不用做特征分解而是通过 SVD 来完成，这个方法在样本量很大的时候很有效。实际上，scikit-learn的 PCA 算法的背后真正的实现就是用的 SVD，而不是特征值分解</p>
</blockquote>
<br>
<h2 id="referencce">Referencce<a hidden class="anchor" aria-hidden="true" href="#referencce">#</a></h2>
<p>[1] Raschka, S. (2014, April 13). Implementing a Principal Component Analysis (PCA). Dr. Sebastian Raschka. <a href="https://sebastianraschka.com/Articles/2014_pca_step_by_step.htmlprincipal-component-analysis-pca-vs-multiple-discriminant-analysis-mda">https://sebastianraschka.com/Articles/2014_pca_step_by_step.htmlprincipal-component-analysis-pca-vs-multiple-discriminant-analysis-mda</a>.</p>
<p>[2] 张洋. (n.d.). 数据的向量表示及降维问题. CodingLabs. <a href="http://blog.codinglabs.org/articles/pca-tutorial.html">http://blog.codinglabs.org/articles/pca-tutorial.html</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/pca/">PCA</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Fisher’s Linear Discriminant Analysis</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/">
    <span class="title">Next Page »</span>
    <br>
    <span>LDA and QDA for Classification</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on twitter"
        href="https://twitter.com/intent/tweet/?text=Principal%20Component%20Analysis&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f&amp;hashtags=MachineLearning%2cPCA">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f&amp;title=Principal%20Component%20Analysis&amp;summary=Principal%20Component%20Analysis&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f&title=Principal%20Component%20Analysis">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on whatsapp"
        href="https://api.whatsapp.com/send?text=Principal%20Component%20Analysis%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Principal Component Analysis on telegram"
        href="https://telegram.me/share/url?text=Principal%20Component%20Analysis&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f05_principal_component_analysis%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
