<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>LDA and QDA for Classification | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning, LDA, QDA" />
<meta name="description" content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="LDA and QDA for Classification" />
<meta property="og:description" content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-13T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-13T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="LDA and QDA for Classification"/>
<meta name="twitter:description" content="Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "LDA and QDA for Classification",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "LDA and QDA for Classification",
  "name": "LDA and QDA for Classification",
  "description": "Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.",
  "keywords": [
    "Machine Learning", "LDA", "QDA"
  ],
  "articleBody": "Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. Linear discriminant analysis (LDA) is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. Quadratic discriminant analysis (QDA) is a variant of LDA that allows for non-linear separation of data.\nLinear Discriminant Analysis for Classification LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:\n Maximizing the distance between the means of two classes; Minimizing the variation between each category.  Suppose that $Y \\in \\{1, ..., K\\}$ is assigned a prior $\\hat{\\pi}_{k}$ such that $\\sum_{i=1}^k \\hat{\\pi}_{k} = 1$. According to Bayesâ€™ rule, the posterior probability is\n$$ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{\\sum_{i=1}^{K}f_{i}(x)\\pi_{i}} \\\\ $$\nwhere $f_{k}(x)$ is the density of $X$ conditioned on $k$. The Bayes Classifier can be expessed as:\n$$ h^{*}(x) = \\arg\\max_{k}\\{P(Y=k|X=x)\\} = \\arg\\max_{k}\\delta_{k}(x) \\\\ $$\nFor we assume that the random variable $X$ is a vector $X=(X_1,X_2,...,X_k)$ which is drawn from a multivariate Gaussian with class-specific mean vector and a common covariance matrix $\\Sigma \\ (i.e. \\Sigma_{k} = \\Sigma, \\forall k)$. In other words the covariance matrix is common to all K classes: $Cov(X)=\\Sigma$ of shape $d \\times d$.\nSince $x$ follows a multivariate Gaussian distribution, the probability $P(X=x|Y=k)$ is given by: ($\\mu_k$ is the mean of inputs for category $k$)\n$$ f_k(x)=\\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ $$\nThen we can find the posterior distribution as:\n$$ P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ $$\nSince $P(X=x)$ does not depend on $k$ so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into $C,C^{'},C{''}$ etc..\n$$ \\begin{align*} p_{k}(x) = \u0026P(Y=k |X=x)=\\frac{f_{k}(x)\\pi_{k}}{P(X=x)} = C \\cdot f_{k}(x)\\pi_{k} \\\\ \u0026=C \\cdot \\pi_{k} \\frac{1}{(2\\pi)^{d/2}|\\Sigma|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \u0026=C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k)) \\\\ \\end{align*} $$\nTake the log of both sides:\n$$ \\begin{align*} logp_{k}(x) \u0026=log(C^{'} \\cdot \\pi_{k} exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k))) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma^{-1}(x-\\mu_k) \\\\ \u0026= logC^{'} + log\\pi_k -\\frac{1}{2}[(x^{T}\\Sigma^{-1}x+\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}]+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \u0026= C^{''} + log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ \\end{align*} $$\nAnd so the objective function, sometimes called the linear discriminant function or linear score function is:\n$$ \\delta_{k} = log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} \\\\ $$\nWhich means that given an input $x$ we predict the class with the highest value of $\\delta_{k}(x)$.\nTo find the Dicision Boundary, we will set $P(Y=k|X=x) = P(Y=l|X=x)$ which is $\\delta_{k}(x) = \\delta_{l}(x)$:\n$$ log\\pi_k -\\frac{1}{2}\\mu_{k}^{T}\\Sigma^{-1}\\mu_{k}+x^{T}\\Sigma^{-1}\\mu_{k} = log\\pi_l -\\frac{1}{2}\\mu_{l}^{T}\\Sigma^{-1}\\mu_{l}+x^{T}\\Sigma^{-1}\\mu_{l} \\\\ log\\frac{\\pi_{k}}{\\pi_{l}} -\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Constant}}+\\underbrace{x^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l})}_{\\mathrm{Linear \\ in} \\ x} = 0 \\\\ \\Rightarrow a^{T}x + b = 0 \\\\ $$\nWhich is a linear function in $x$ - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.\nQuadratic Discrimination Analysis for Classification LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all $K$ classes. Quadratic discriminant analysis provides an alternative approach by assuming that each class has its own covariance matrix $\\Sigma_{k}$.\nTo derive the quadratic score function, we return to the previous derivation, but now $\\Sigma_{k}$ is a function of $k$, so we cannot push it into the constant anymore.\n$$ p_{k}(x) = \\pi_{k}\\frac{1}{(2\\pi)^{d/2}|\\Sigma_{k}|^{1/2}}exp(-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k)) \\\\ $$\n$$ \\begin{align*} logp_{k}(x) \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}|-\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ \u0026= C +log\\pi_{k} - \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}x^{T}\\Sigma_{k}^{-1}x +x^{T}\\Sigma_{k}^{-1}\\mu_{k} -\\frac{1}{2}\\mu_{k}^{T}\\Sigma_{k}^{-1}\\mu_{k} \\\\ \\end{align*} $$\nWhich is a quadratic function of $x$. Under this less restrictive assumption, the classifier assigns an observation $X=x$ to the class for which the quadratic score function is the largest:\n$$ \\delta_{k}(x) = log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \\\\ $$\nTo find the Quadratic Dicision Boundary, we will set $P(Y=k|X=x) = P(Y=l|X=x)$ which is $\\delta_{k}(x) = \\delta_{l}(x)$:\n$$ log\\pi_k- \\frac{1}{2}log|\\Sigma_{k}| -\\frac{1}{2}(x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) = log\\pi_l- \\frac{1}{2}log|\\Sigma_{l}| -\\frac{1}{2}(x-\\mu_l)^{T}\\Sigma_{l}^{-1}(x-\\mu_l) \\\\ \\frac{1}{2}x^{T}\\underbrace{(\\Sigma_{l}-\\Sigma_{k})}_{A}x+\\underbrace{(\\mu_{k}^{T}\\Sigma_{k}^{-1}-\\mu_{l}^{T}\\Sigma_{l}^{-1})}_{b^{T}}x +\\underbrace{\\frac{1}{2}(\\mu_{k}-\\mu_{l})^{T}\\Sigma^{-1}(\\mu_{k}-\\mu_{l}) + log(\\frac{\\pi_{l}}{\\pi_{k}}) + log(\\frac{|\\Sigma_{k}|^{1/2}}{|\\Sigma_{l}|^{1/2}})}_{c} = 0 \\\\ \\Rightarrow x^{T}Ax + b^{T}x + c = 0 \\\\ $$\nCase 1 : When $\\Sigma_{k} = I$ We first concider the case that $\\Sigma_{k} = I, \\forall k$. This is the case where each distribution is spherical, around the mean point. Then we can have:\n$$ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(x-\\mu_k)^{T}I(x-\\mu_k) \\\\ $$\nWhere $log(|I|) = log(1) = 0$ and $(x-\\mu_k)^{T}I(x-\\mu_k) = (x-\\mu_k)^{T}(x-\\mu_k)$ is the Squared Euclidean Distance between two points $x$ and $\\mu_{k}$.\nThus under this condition (i.e. $\\Sigma = I$) , a new point can be classified by its distance from the center of a class, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-classâ€™s means.\nCase 2 : When $\\Sigma_{k} \\neq I$ Since $\\Sigma_{k}$ is a symmetric matrix $\\Sigma_{k} = \\Sigma_{k}^{T}$, by using the Singular Value Decomposition (SVD) of $\\Sigma_{k}$, we can get:\n$$ \\Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\\\ \\Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\\\ $$\nThen,\n$$ \\begin{align*} (x-\\mu_k)^{T}\\Sigma_{k}^{-1}(x-\\mu_k) \u0026= (x-\\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (U_{k}^{T}x-U_{k}^{T}\\mu_k)^{T}S_{k}^{-\\frac{1}{2}}S_{k}^{-\\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}I(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \u0026= (S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ \\end{align*} $$\nWhich is also known as the Mahalanobis distance.\nThink of $S_{k}^{-\\frac{1}{2}}U_{k}^{T}$ as a linear transformation that takes points in class $k$ and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified $\\delta_{k}$ values to calculate $h^{*}(x)$. After applying the singular value decomposition, $\\Sigma_{k}^{-1}$ is considered to be an identity matrix such that:\n$$ \\delta_{k}(x) = - \\frac{1}{2}log|I| + log\\pi_k -\\frac{1}{2}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k)^{T}(S_{k}^{-\\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\\frac{1}{2}}U_{k}^{T}\\mu_k) \\\\ $$\nWhere $log(|I|) = log(1) = 0$\nThe difference between Case 1 and Case 2 (i.e. the difference between using the Euclidean and Mahalanobis distance) can be seen in the illustration below:\n LDA and QDA in practice In practice we donâ€™t know the parameters of Gaussian and will need to estimate them using our training data.\n$$ \\hat{\\pi}_{k} = \\hat{P}(y=k) = \\frac{n_{k}}{n} \\\\ $$\nwhere $n_{k}$ is the number of class $k$ observations.\n$$ \\hat{\\mu}_{k} = \\frac{1}{n_{k}}\\sum_{i:y_{i}=k}x_{i} \\\\ $$\n$$ \\hat{\\Sigma}_{k} = \\frac{1}{n_{k}-k}\\sum_{i:y_{i}=k}(x_{i}-\\hat{\\mu}_{k})(x_{i}-\\hat{\\mu}_{k})^{T} \\\\ $$\nIf we wish to use LDA we must calculate a common covariance, so we average all the covariances, e.g.\n$$ \\Sigma = \\frac{\\sum_{r=1}^k(n_{r}\\Sigma_{r})}{\\sum_{r=1}^k n_{r}} \\\\ $$\nWhere:\n  $n_{r}$ is the number of data points in class $r$.\n  $\\Sigma_{r}$ is the covariance of class $r$ and $n$ is the total number of data points.\n  $k$ is the number of classes.\n  Reference [1] Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. https://xavierbourretsicotte.github.io/LDA_QDA.html.\n",
  "wordCount" : "1121",
  "inLanguage": "en",
  "datePublished": "2021-05-13T00:00:00Z",
  "dateModified": "2021-05-13T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/04_lda_and_qda_for_classification/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      LDA and QDA for Classification
    </h1>
    <div class="post-meta">May 13, 2021&nbsp;Â·&nbsp;6 min&nbsp;Â·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#linear-discriminant-analysis-for-classification" aria-label="Linear Discriminant Analysis for Classification">Linear Discriminant Analysis for Classification</a></li>
                <li>
                    <a href="#quadratic-discrimination-analysis-for-classification" aria-label="Quadratic Discrimination Analysis for Classification">Quadratic Discrimination Analysis for Classification</a><ul>
                        
                <li>
                    <a href="#case-1--when-sigma_k--i" aria-label="Case 1 : When $\Sigma_{k} = I$">Case 1 : When <code>$\Sigma_{k} = I$</code></a></li>
                <li>
                    <a href="#case-2--when-sigma_k-neq-i" aria-label="Case 2 : When $\Sigma_{k} \neq I$">Case 2 : When <code>$\Sigma_{k} \neq I$</code></a></li></ul>
                </li>
                <li>
                    <a href="#lda-and-qda-in-practice" aria-label="LDA and QDA in practice">LDA and QDA in practice</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>Discriminant analysis encompasses methods that can be used for both classification and dimensionality reduction. <strong>Linear discriminant analysis (LDA)</strong> is particularly popular because it is both a classifier and a dimensionality reduction technique. Despite its simplicity, LDA often produces robust, decent, and interpretable classification results. When tackling real-world classification problems, LDA is often the first and benchmarking method before other more complicated and flexible ones are employed. <strong>Quadratic discriminant analysis (QDA)</strong> is a variant of LDA that allows for non-linear separation of data.</p>
<br>
<h2 id="linear-discriminant-analysis-for-classification">Linear Discriminant Analysis for Classification<a hidden class="anchor" aria-hidden="true" href="#linear-discriminant-analysis-for-classification">#</a></h2>
<p>LDA assumes that all classes are linearly separable and according to this multiple linear discrimination function representing several hyperplanes in the feature space are created to distinguish the classes. If there are two classes then the LDA draws one hyperplane and projects the data onto this hyperplane in such a way as to maximize the separation of the two categories. This hyperplane is created according to the two criteria considered simultaneously:</p>
<ul>
<li>Maximizing the distance between the means of two classes;</li>
<li>Minimizing the variation between each category.</li>
</ul>
<p>Suppose that <code>$Y \in \{1, ..., K\}$</code> is assigned a prior <code>$\hat{\pi}_{k}$</code> such that <code>$\sum_{i=1}^k \hat{\pi}_{k} = 1$</code>. According to Bayesâ€™ rule, the posterior probability is</p>
<p><code>$$ P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{\sum_{i=1}^{K}f_{i}(x)\pi_{i}} \\ $$</code></p>
<p>where <code>$f_{k}(x)$</code> is the density of <code>$X$</code> conditioned on <code>$k$</code>. The Bayes Classifier can be expessed as:</p>
<p><code>$$ h^{*}(x) = \arg\max_{k}\{P(Y=k|X=x)\} = \arg\max_{k}\delta_{k}(x) \\ $$</code></p>
<p>For we assume that the random variable <code>$X$</code> is a vector <code>$X=(X_1,X_2,...,X_k)$</code> which is drawn from a <em>multivariate Gaussian</em> with class-specific mean vector and a common covariance matrix <code>$\Sigma \ (i.e. \Sigma_{k} = \Sigma, \forall k)$</code>. In other words the covariance matrix is common to all K classes: <code>$Cov(X)=\Sigma$</code> of shape <code>$d \times d$</code>.</p>
<p>Since <code>$x$</code> follows a multivariate Gaussian distribution, the probability <code>$P(X=x|Y=k)$</code> is given by: (<code>$\mu_k$</code> is the mean of inputs for category <code>$k$</code>)</p>
<p><code>$$ f_k(x)=\frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)) \\ $$</code></p>
<p>Then we can find the posterior distribution as:</p>
<p><code>$$ P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{P(X=x)} = C \cdot f_{k}(x)\pi_{k} \\ $$</code></p>
<p>Since <code>$P(X=x)$</code> does not depend on <code>$k$</code> so we write it as a constant. We will now proceed to expand and simplify the algebra, putting all constant terms into <code>$C,C^{'},C{''}$</code> etc..</p>
<p><code>$$ \begin{align*} p_{k}(x) = &amp;P(Y=k |X=x)=\frac{f_{k}(x)\pi_{k}}{P(X=x)} = C \cdot f_{k}(x)\pi_{k} \\ &amp;=C \cdot \pi_{k} \frac{1}{(2\pi)^{d/2}|\Sigma|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k)) \\ &amp;=C^{'} \cdot \pi_{k} exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k)) \\ \end{align*} $$</code></p>
<p>Take the log of both sides:</p>
<p><code>$$ \begin{align*} logp_{k}(x) &amp;=log(C^{'} \cdot \pi_{k} exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k))) \\ &amp;= logC^{'} + log\pi_k -\frac{1}{2}(x-\mu_k)^{T}\Sigma^{-1}(x-\mu_k) \\ &amp;= logC^{'} + log\pi_k -\frac{1}{2}[(x^{T}\Sigma^{-1}x+\mu_{k}^{T}\Sigma^{-1}\mu_{k}]+x^{T}\Sigma^{-1}\mu_{k} \\ &amp;= C^{''} + log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} \\ \end{align*} $$</code></p>
<p>And so the objective function, sometimes called the linear discriminant function or linear score function is:</p>
<p><code>$$ \delta_{k} = log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} \\ $$</code></p>
<p>Which means that given an input <code>$x$</code> we predict the class with the highest value of <code>$\delta_{k}(x)$</code>.</p>
<p>To find the Dicision Boundary, we will set <code>$P(Y=k|X=x) = P(Y=l|X=x)$</code> which is <code>$\delta_{k}(x) = \delta_{l}(x)$</code>:</p>
<p><code>$$ log\pi_k -\frac{1}{2}\mu_{k}^{T}\Sigma^{-1}\mu_{k}+x^{T}\Sigma^{-1}\mu_{k} = log\pi_l -\frac{1}{2}\mu_{l}^{T}\Sigma^{-1}\mu_{l}+x^{T}\Sigma^{-1}\mu_{l} \\ log\frac{\pi_{k}}{\pi_{l}} -\underbrace{\frac{1}{2}(\mu_{k}-\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})}_{\mathrm{Constant}}+\underbrace{x^{T}\Sigma^{-1}(\mu_{k}-\mu_{l})}_{\mathrm{Linear \ in} \ x} = 0 \\ \Rightarrow a^{T}x + b = 0 \\ $$</code></p>
<p>Which is a linear function in <code>$x$</code> - this explains why the decision boundaries are linear - hence the name Linear Discriminant Analysis.</p>
<br>
<h2 id="quadratic-discrimination-analysis-for-classification">Quadratic Discrimination Analysis for Classification<a hidden class="anchor" aria-hidden="true" href="#quadratic-discrimination-analysis-for-classification">#</a></h2>
<p>LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector, but a covariance matrix that is common to all <code>$K$</code> classes. <strong>Quadratic discriminant analysis</strong> provides an alternative approach by assuming that each class has its <strong>own covariance matrix <code>$\Sigma_{k}$</code></strong>.</p>
<p>To derive the quadratic score function, we return to the previous derivation, but now <code>$\Sigma_{k}$</code> is a function of <code>$k$</code>, so we cannot push it into the constant anymore.</p>
<p><code>$$ p_{k}(x) = \pi_{k}\frac{1}{(2\pi)^{d/2}|\Sigma_{k}|^{1/2}}exp(-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k)) \\ $$</code></p>
<p><code>$$ \begin{align*} logp_{k}(x) &amp;= C +log\pi_{k} - \frac{1}{2}log|\Sigma_{k}|-\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) \\ &amp;= C +log\pi_{k} - \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}x^{T}\Sigma_{k}^{-1}x +x^{T}\Sigma_{k}^{-1}\mu_{k} -\frac{1}{2}\mu_{k}^{T}\Sigma_{k}^{-1}\mu_{k} \\ \end{align*} $$</code></p>
<p>Which is a quadratic function of <code>$x$</code>. Under this less restrictive assumption, the classifier assigns an observation <code>$X=x$</code> to the class for which the quadratic score function is the largest:</p>
<p><code>$$ \delta_{k}(x) = log\pi_k- \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) \\ $$</code></p>
<p>To find the Quadratic Dicision Boundary, we will set <code>$P(Y=k|X=x) = P(Y=l|X=x)$</code> which is <code>$\delta_{k}(x) = \delta_{l}(x)$</code>:</p>
<p><code>$$ log\pi_k- \frac{1}{2}log|\Sigma_{k}| -\frac{1}{2}(x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) = log\pi_l- \frac{1}{2}log|\Sigma_{l}| -\frac{1}{2}(x-\mu_l)^{T}\Sigma_{l}^{-1}(x-\mu_l) \\ \frac{1}{2}x^{T}\underbrace{(\Sigma_{l}-\Sigma_{k})}_{A}x+\underbrace{(\mu_{k}^{T}\Sigma_{k}^{-1}-\mu_{l}^{T}\Sigma_{l}^{-1})}_{b^{T}}x +\underbrace{\frac{1}{2}(\mu_{k}-\mu_{l})^{T}\Sigma^{-1}(\mu_{k}-\mu_{l}) + log(\frac{\pi_{l}}{\pi_{k}}) + log(\frac{|\Sigma_{k}|^{1/2}}{|\Sigma_{l}|^{1/2}})}_{c} = 0 \\ \Rightarrow x^{T}Ax + b^{T}x + c = 0 \\ $$</code></p>
<h3 id="case-1--when-sigma_k--i">Case 1 : When <code>$\Sigma_{k} = I$</code><a hidden class="anchor" aria-hidden="true" href="#case-1--when-sigma_k--i">#</a></h3>
<p>We first concider the case that <code>$\Sigma_{k} = I, \forall k$</code>. This is the case where each distribution is spherical, around
the mean point. Then we can have:</p>
<p><code>$$ \delta_{k}(x) = - \frac{1}{2}log|I| + log\pi_k -\frac{1}{2}(x-\mu_k)^{T}I(x-\mu_k) \\ $$</code></p>
<p>Where <code>$log(|I|) = log(1) = 0$</code> and <code>$(x-\mu_k)^{T}I(x-\mu_k) = (x-\mu_k)^{T}(x-\mu_k)$</code> is the <strong>Squared Euclidean Distance</strong> between two points <code>$x$</code> and <code>$\mu_{k}$</code>.</p>
<p>Thus under this condition (i.e. <code>$\Sigma = I$</code>) , a new point can be classified by <strong>its distance from the center of a class</strong>, adjusted by some prior. Further, for two-class problem with equal prior, the discriminating function would be the perpendicular bisector of the 2-classâ€™s means.</p>
<h3 id="case-2--when-sigma_k-neq-i">Case 2 : When <code>$\Sigma_{k} \neq I$</code><a hidden class="anchor" aria-hidden="true" href="#case-2--when-sigma_k-neq-i">#</a></h3>
<p>Since <code>$\Sigma_{k}$</code> is a symmetric matrix <code>$\Sigma_{k} = \Sigma_{k}^{T}$</code>, by using the <strong>Singular Value Decomposition (SVD)</strong> of <code>$\Sigma_{k}$</code>, we can get:</p>
<p><code>$$ \Sigma_{k} = U_{k}S_{k}U_{k}^{T} \\ \Sigma_{k}^{-1} = (U_{k}S_{k}U_{k}^{T})^{-1} = U_{k}S_{k}^{-1}U_{k}^{T}\\ $$</code></p>
<p>Then,</p>
<p><code>$$ \begin{align*} (x-\mu_k)^{T}\Sigma_{k}^{-1}(x-\mu_k) &amp;= (x-\mu_k)^{T}U_{k}S_{k}^{-1}U_{k}^{T}(x-\mu_k) \\ &amp;= (U_{k}^{T}x-U_{k}^{T}\mu_k)^{T}S_{k}^{-1}(U_{k}^{T}x-U_{k}^{T}\mu_k) \\ &amp;= (U_{k}^{T}x-U_{k}^{T}\mu_k)^{T}S_{k}^{-\frac{1}{2}}S_{k}^{-\frac{1}{2}}(U_{k}^{T}x-U_{k}^{T}\mu_k) \\ &amp;= (S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}I(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\ &amp;= (S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\ \end{align*} $$</code></p>
<p>Which is also known as the <strong>Mahalanobis distance</strong>.</p>
<p>Think of <code>$S_{k}^{-\frac{1}{2}}U_{k}^{T}$</code> as a linear transformation that takes points in class <code>$k$</code> and distributes them spherically around a point, like in Case 1. Thus when we are given a new point, we can apply the modified <code>$\delta_{k}$</code> values to calculate <code>$h^{*}(x)$</code>. After applying the singular value decomposition, <code>$\Sigma_{k}^{-1}$</code> is considered to be an identity matrix such that:</p>
<p><code>$$ \delta_{k}(x) = - \frac{1}{2}log|I| + log\pi_k -\frac{1}{2}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k)^{T}(S_{k}^{-\frac{1}{2}}U_{k}^{T}x-S_{k}^{-\frac{1}{2}}U_{k}^{T}\mu_k) \\ $$</code></p>
<p>Where <code>$log(|I|) = log(1) = 0$</code></p>
<p>The difference between Case 1 and Case 2 (i.e. the difference between using the <strong>Euclidean and Mahalanobis distance</strong>) can be seen in the illustration below:</p>
<div align="center">
<img src="/img_ML/4_Euclidean_and_Mahalanobis_distance.gif" width=600px/>
</div>
<br>
<h2 id="lda-and-qda-in-practice">LDA and QDA in practice<a hidden class="anchor" aria-hidden="true" href="#lda-and-qda-in-practice">#</a></h2>
<p>In practice we donâ€™t know the parameters of Gaussian and will need to estimate them using our training data.</p>
<p><code>$$ \hat{\pi}_{k} = \hat{P}(y=k) = \frac{n_{k}}{n} \\ $$</code></p>
<p>where <code>$n_{k}$</code> is the number of class <code>$k$</code> observations.</p>
<p><code>$$ \hat{\mu}_{k} = \frac{1}{n_{k}}\sum_{i:y_{i}=k}x_{i} \\ $$</code></p>
<p><code>$$ \hat{\Sigma}_{k} = \frac{1}{n_{k}-k}\sum_{i:y_{i}=k}(x_{i}-\hat{\mu}_{k})(x_{i}-\hat{\mu}_{k})^{T} \\ $$</code></p>
<p>If we wish to use LDA we must calculate a <strong>common covariance</strong>, so we average all the covariances, e.g.</p>
<p><code>$$ \Sigma = \frac{\sum_{r=1}^k(n_{r}\Sigma_{r})}{\sum_{r=1}^k n_{r}} \\ $$</code></p>
<p>Where:</p>
<ul>
<li>
<p><code>$n_{r}$</code> is the number of data points in class <code>$r$</code>.</p>
</li>
<li>
<p><code>$\Sigma_{r}$</code> is the covariance of class <code>$r$</code> and <code>$n$</code> is the total number of data points.</p>
</li>
<li>
<p><code>$k$</code> is the number of classes.</p>
</li>
</ul>
<br>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1]  Sicotte, X. B. (2018, June 22). Xavier Bourret Sicotte. Linear and Quadratic Discriminant Analysis - Data Blog. <a href="https://xavierbourretsicotte.github.io/LDA_QDA.html">https://xavierbourretsicotte.github.io/LDA_QDA.html</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/lda/">LDA</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/qda/">QDA</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/">
    <span class="title">Â« Prev Page</span>
    <br>
    <span>Principal Component Analysis</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/machine_learning/03_logistic_regression/">
    <span class="title">Next Page Â»</span>
    <br>
    <span>Logistic Regression</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share LDA and QDA for Classification on twitter"
        href="https://twitter.com/intent/tweet/?text=LDA%20and%20QDA%20for%20Classification&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f&amp;hashtags=MachineLearning%2cLDA%2cQDA">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LDA and QDA for Classification on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f&amp;title=LDA%20and%20QDA%20for%20Classification&amp;summary=LDA%20and%20QDA%20for%20Classification&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LDA and QDA for Classification on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f&title=LDA%20and%20QDA%20for%20Classification">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LDA and QDA for Classification on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LDA and QDA for Classification on whatsapp"
        href="https://api.whatsapp.com/send?text=LDA%20and%20QDA%20for%20Classification%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share LDA and QDA for Classification on telegram"
        href="https://telegram.me/share/url?text=LDA%20and%20QDA%20for%20Classification&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f04_lda_and_qda_for_classification%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
