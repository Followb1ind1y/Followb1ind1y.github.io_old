<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Fisher’s Linear Discriminant Analysis | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning, FDA" />
<meta name="description" content="Fisher&rsquo;s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (&ldquo;curse of dimensionality&rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Fisher’s Linear Discriminant Analysis" />
<meta property="og:description" content="Fisher&rsquo;s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (&ldquo;curse of dimensionality&rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-17T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-17T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Fisher’s Linear Discriminant Analysis"/>
<meta name="twitter:description" content="Fisher&rsquo;s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (&ldquo;curse of dimensionality&rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Fisher’s Linear Discriminant Analysis",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Fisher’s Linear Discriminant Analysis",
  "name": "Fisher’s Linear Discriminant Analysis",
  "description": "Fisher\u0026rsquo;s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (\u0026ldquo;curse of dimensionality\u0026rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes.",
  "keywords": [
    "Machine Learning", "FDA"
  ],
  "articleBody": "Fisher’s Linear Discriminant Analysis (FDA) is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (“curse of dimensionality”) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes.\nFisher’s Linear Discriminant Analysis Assume we have only 2 classes. The idea behind Fisher’s Linear Discriminant Analysis is to reduce the dimensionality of the data to one dimension. That is, to take d-dimensional $x\\in \\mathbf{R}^{d}$ and map it to one dimension by finding $w^{T}x$ where:\n$$z = w^{T}x= \\begin{bmatrix} w_{1} ... w_{d} \\\\ \\end{bmatrix} \\begin{bmatrix} x_{1} \\\\ ... \\\\ x_{d} \\\\ \\end{bmatrix} = \\sum_{i=1}^{d}w_{i}x_{i} \\\\ $$\nThe one-dimensional $z$ is then used for classification.\nGoal: To find a direction such that projected data $w^{T}x$ are well separated.\nConsider the two-class problem:\n$$ \\mu_{0}=\\frac{1}{n_{0}}\\sum_{i:y_{i}=0}x_{i} \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\mu_{1}=\\frac{1}{n_{1}}\\sum_{i:y_{i}=0}x_{i} \\\\ $$\nWe want to:\n  Maximize the distance between projected class means.\n  Minimize the within class variance.\n   Fisher判别分析(Fisher’s Linear Discriminant Analysis)的目的：给定一个投影向量$w$，将$x$投影到$w$向量上，使得不同类别的$x$投影后的值 $y=w^{T}x$ 尽可能互相分开(far apart)。投影向量的选择很关键，当投影向量选择不合理时，不同类别的$x$投影后的$y$根本无法被分开。所以要找到最优的投影向量，使得投影后的值被最大限度地区分。(这里的前提是：原始数据是线性可分的)\n最大限度的划分投影后的值需要用到两个准则:\n 投影后的两类样本 均值 之间的距离 尽可能大 投影后两类样本各自的 方差尽可能小    The distance between projected class means is:\n$$ \\begin{align*} (w^{T}\\mu_{0} - w^{T}\\mu_{1})^{2} \u0026= (w^{T}\\mu_{0} - w^{T}\\mu_{1})^{T}(w^{T}\\mu_{0} - w^{T}\\mu_{1}) \\\\ \u0026= (\\mu_{0}-\\mu_{1})^{T}ww^{T}(\\mu_{0}-\\mu_{1}) \\\\ \u0026=w^{T}(\\mu_{0}-\\mu_{1})(\\mu_{0}-\\mu_{1})^{T}w \\\\ \u0026= w^{T}S_{B}w \\\\ \\end{align*} $$ where $S_{B}$ is the between-class variance (known).\nMinimizing the within-class variance is equivalent to minimizing the sum of all individual within-class variances. Thus the within class variance is:\n$$ \\begin{align*} w^{T}\\Sigma_{0}w+w^{T}\\Sigma_{1}w \u0026= w^{T}(\\Sigma_{0}+\\Sigma_{1})w \\\\ \u0026= w^{T}S_{W}w \\\\ \\end{align*} $$\nwhere $S_{W}$ is the within-class covariance (known).\nTo maximize the distance between projected class means and minimize the within-class variance, we can maximize the ratio:\n$$ \\max_{w} \\ \\frac{w^{T}S_{B}w}{w^{T}S_{W}w} \\\\ $$\nNote that the numerator is unbounded since we can make any arbitary $w^{T}$. But since we are only interested in direction, length is not important. Therefore we can fix the length of $w$ (i.e. unit length) and find the direction. This is equivalent to finding:\n$$ \\max_{w} \\ w^{T}S_{B}w \\\\ \\mathrm{Subject \\ to} \\ \\ w^{T}S_{W}w = 1 \\\\ $$\nTo turn this constraint optimization problem into a non-constranst optimization problem, we apply Lagrange multipliers:\n$$ L(w,\\lambda) = w^{T}S_{B}w - \\lambda(w^{T}S_{W}w-1) \\\\ $$\nDifferentiating with respect to $w$ we get:\n$$ \\frac{\\partial L}{\\partial w} = 2S_{B} w - \\lambda2S_{W} w = 0 \\\\ S_{B} w = \\lambda S_{W}w \\\\ $$\nThis is a generalized eigenvector problem that is equivalent to (if $S_{W}$ is not singular):\n$$ S_{W}^{-1}S_{B}w = \\lambda w \\\\ $$\nwhere $\\lambda$ and $w$ are the eigenvalues and eigenvectors of $S_{W}^{-1}S_{B}$respectively. $w$ is the eigenvector corresponding to the largest eigenvalue of $S_{W}^{-1}S_{B}$.\nIn fact, for two-classes problems, there exists a simpler solution. Recall that $S_{B}w = (\\mu_{0}-\\mu_{1})(\\mu_{0}-\\mu_{1})^{T}w$ where $(\\mu_{0}-\\mu_{1})^{T}w$ is a scalar. Therefore $S_{B}w\\propto(\\mu_{0}-\\mu_{1})$. That is, $S_{B}w$ is on the same direction as $(\\mu_{0}-\\mu_{1})$. Since $S_{W}^{-1}S_{B}w = \\lambda w$, we get:\n$$ S_{w}^{-1}(\\mu_{0}-\\mu_{1}) \\propto w \\\\ $$\nwhich gives us the direction.\nFisher’s Linear Discriminant Analysis For Multiple Classes  We have defined $\\varepsilon(w)=\\frac{w^{T}S_{B}w}{w^{T}S_{W}w}$ that needs to be maximized. $w$ is the largest eigen vectors of $S_{W}^{-1}S_{B}$. For two classes, $w \\propto S_{w}^{-1}(\\mu_{0}-\\mu_{1})$. For $k$-class problem, Fisher Discriminant Analysis involves $(k - 1)$ discriminant functions. Make $W_{d \\times (K-1)}$ where each column describes a discriminant. So now, we have to update the two notions we have defined for a $2$-class problem, $S_{B}$ and $S_{W}$ .\n$$ S_{w}=\\sum_{i=1}^{K}\\Sigma_{i} \\\\ $$\n$S_{B}$ generalization to multiple classes in not obvious. We will define the total variance $S_{T}$ as the sum of the within class variance and between classes variance.\n$$ S_{T} = S_{B}+S_{W} \\\\ $$\nWhere $S_{T}=\\frac{1}{n}\\sum_{i=1}^{n}(x_{i}-\\mu)(x_{i}-\\mu)^{T}$ and $\\mu=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$. So,\n$$ S_{B} = S_{T}-S_{W} \\\\ $$\nIt can be shown that $W$ is the first $(k - 1)$ eigen vectors of $S_{W}^{-1}S_{B}$.\nPCA vs. LDA Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques that are commonly used for dimensionality reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores” class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised” and computes the directions (“linear discriminants”) that will represent the axes that that maximize the separation between multiple classes.\nAlthough it might sound intuitive that LDA is superior to PCA for a multi-class classification task where the class labels are known, this might not always the case.\n  PCA 实质上是在寻找一个子空间。而这个子空间是协方差矩阵的特征空间(特征向量对应的空间)，选取特征值最大的 k 个特征向量组成的特征子空间(相当于这个子空间有 k 维，每一维代表一个特征，这 ｋ 个特征基本可以涵盖 90% 以上的信息)。Fisher判别 和 PCA 是在做类似的一件事，都是在找子空间。不同的是, PCA 是找一个低维的子空间，样本投影在这个空间基本不丢失信息。而 Fisher是寻找这样的一个空间，样本投影在这个空间上，类内距离最小，类间距离最大。\n两者的 相同点 ：\n 两者均可以对数据进行降维。 两者在降维时均使用了矩阵特征分解的思想。 两者都假设数据符合高斯分布。  两者的 不同点 ：\n LDA是有监督的降维方法，而PCA是无监督的降维方法 LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。 LDA除了可以用于降维，还可以用于分类。 LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。   Reference [1] Raschka, S. (2014, August 3). Linear Discriminant Analysis. Dr. Sebastian Raschka. https://sebastianraschka.com/Articles/2014_python_lda.html.\n",
  "wordCount" : "820",
  "inLanguage": "en",
  "datePublished": "2021-05-17T00:00:00Z",
  "dateModified": "2021-05-17T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/06_fishers_linear_discriminant_analysis/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Fisher’s Linear Discriminant Analysis
    </h1>
    <div class="post-meta">May 17, 2021&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#fishers-linear-discriminant-analysis" aria-label="Fisher’s Linear Discriminant Analysis">Fisher’s Linear Discriminant Analysis</a></li>
                <li>
                    <a href="#fishers-linear-discriminant-analysis-for-multiple-classes" aria-label="Fisher’s Linear Discriminant Analysis For Multiple Classes">Fisher’s Linear Discriminant Analysis For Multiple Classes</a></li>
                <li>
                    <a href="#pca-vs-lda" aria-label="PCA vs. LDA">PCA vs. LDA</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Fisher&rsquo;s Linear Discriminant Analysis (FDA)</strong> is most commonly used as dimensionality reduction technique in the pre-processing step for pattern-classification and machine learning applications. The goal is to project a dataset onto a lower-dimensional space with good class-separability in order avoid overfitting (&ldquo;curse of dimensionality&rdquo;) and also reduce computational costs. The general FDA approach is very similar to a Principal Component Analysis, but in addition to finding the component axes that maximize the variance of our data (PCA), we are additionally interested in the axes that maximize the separation between multiple classes.</p>
<br>
<h2 id="fishers-linear-discriminant-analysis">Fisher’s Linear Discriminant Analysis<a hidden class="anchor" aria-hidden="true" href="#fishers-linear-discriminant-analysis">#</a></h2>
<p>Assume we have only 2 classes. The idea behind Fisher’s Linear Discriminant Analysis is to reduce the dimensionality of the data to one dimension. That is, to take d-dimensional <code>$x\in \mathbf{R}^{d}$</code> and map it to one dimension by finding <code>$w^{T}x$</code> where:</p>
<p><code>$$z = w^{T}x= \begin{bmatrix} w_{1} ... w_{d}   \\ \end{bmatrix} \begin{bmatrix} x_{1}    \\ ...      \\ x_{d}    \\ \end{bmatrix} = \sum_{i=1}^{d}w_{i}x_{i} \\ $$</code></p>
<p>The one-dimensional <code>$z$</code> is then used for classification.</p>
<p><strong>Goal:</strong> To find a direction such that projected data <code>$w^{T}x$</code> are well separated.</p>
<p>Consider the two-class problem:</p>
<p><code>$$ \mu_{0}=\frac{1}{n_{0}}\sum_{i:y_{i}=0}x_{i} \ \ \ \ \ \ \ \ \ \ \ \ \mu_{1}=\frac{1}{n_{1}}\sum_{i:y_{i}=0}x_{i} \\ $$</code></p>
<p>We want to:</p>
<ol>
<li>
<p><strong>Maximize the distance between projected class means.</strong></p>
</li>
<li>
<p><strong>Minimize the within class variance.</strong></p>
</li>
</ol>
<blockquote>
<p><strong>Fisher判别分析(Fisher’s Linear Discriminant Analysis)的目的</strong>：给定一个投影向量<code>$w$</code>，将<code>$x$</code>投影到<code>$w$</code>向量上，使得不同类别的<code>$x$</code>投影后的值 <code>$y=w^{T}x$</code> 尽可能互相分开(far apart)。投影向量的选择很关键，当投影向量选择不合理时，不同类别的<code>$x$</code>投影后的<code>$y$</code>根本无法被分开。所以要找到最优的投影向量，使得投影后的值被最大限度地区分。(这里的前提是：原始数据是线性可分的)</p>
<p>最大限度的划分投影后的值需要用到两个准则:</p>
<ol>
<li>投影后的两类样本 <strong>均值</strong> 之间的距离 <strong>尽可能大</strong></li>
<li>投影后两类样本各自的 <strong>方差尽可能小</strong></li>
</ol>
</blockquote>
<div align="center">
<img src="/img_ML/6_PCA_goal.jpeg" width=500px/>
</div>
<br>
<p>The distance between projected class means is:</p>
<p><code>$$ \begin{align*} (w^{T}\mu_{0} - w^{T}\mu_{1})^{2} &amp;= (w^{T}\mu_{0} - w^{T}\mu_{1})^{T}(w^{T}\mu_{0} - w^{T}\mu_{1}) \\ &amp;= (\mu_{0}-\mu_{1})^{T}ww^{T}(\mu_{0}-\mu_{1}) \\ &amp;=w^{T}(\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}w \\ &amp;= w^{T}S_{B}w \\ \end{align*} $$</code>
where <code>$S_{B}$</code> is the between-class variance (known).</p>
<p>Minimizing the within-class variance is equivalent to minimizing the sum of all individual within-class variances.
Thus the within class variance is:</p>
<p><code>$$ \begin{align*} w^{T}\Sigma_{0}w+w^{T}\Sigma_{1}w &amp;= w^{T}(\Sigma_{0}+\Sigma_{1})w \\ &amp;= w^{T}S_{W}w \\ \end{align*} $$</code></p>
<p>where <code>$S_{W}$</code> is the within-class covariance (known).</p>
<p>To maximize the distance between projected class means and minimize the within-class variance, we can maximize the ratio:</p>
<p><code>$$ \max_{w} \ \frac{w^{T}S_{B}w}{w^{T}S_{W}w} \\ $$</code></p>
<p>Note that the numerator is unbounded since we can make any arbitary <code>$w^{T}$</code>. But since we are only interested in direction, length is not important. Therefore we can fix the length of <code>$w$</code> (i.e. unit length) and find the direction. This is equivalent to finding:</p>
<p><code>$$ \max_{w} \ w^{T}S_{B}w \\ \mathrm{Subject \  to} \ \ w^{T}S_{W}w = 1 \\ $$</code></p>
<p>To turn this constraint optimization problem into a non-constranst optimization problem, we apply <strong>Lagrange multipliers:</strong></p>
<p><code>$$ L(w,\lambda) = w^{T}S_{B}w - \lambda(w^{T}S_{W}w-1) \\ $$</code></p>
<p>Differentiating with respect to <code>$w$</code> we get:</p>
<p><code>$$ \frac{\partial L}{\partial w} = 2S_{B} w - \lambda2S_{W} w = 0 \\ S_{B} w = \lambda S_{W}w \\ $$</code></p>
<p>This is a generalized eigenvector problem that is equivalent to (if <code>$S_{W}$</code> is not singular):</p>
<p><code>$$ S_{W}^{-1}S_{B}w = \lambda w \\ $$</code></p>
<p>where <code>$\lambda$</code> and <code>$w$</code> are the eigenvalues and eigenvectors of <code>$S_{W}^{-1}S_{B}$</code>respectively. <code>$w$</code> is the eigenvector corresponding to the largest eigenvalue of <code>$S_{W}^{-1}S_{B}$</code>.</p>
<p>In fact, for two-classes problems, there exists a <strong>simpler solution</strong>. Recall that <code>$S_{B}w = (\mu_{0}-\mu_{1})(\mu_{0}-\mu_{1})^{T}w$</code> where <code>$(\mu_{0}-\mu_{1})^{T}w$</code> is a scalar. Therefore <code>$S_{B}w\propto(\mu_{0}-\mu_{1})$</code>. That is, <code>$S_{B}w$</code> is on the same direction as <code>$(\mu_{0}-\mu_{1})$</code>. Since <code>$S_{W}^{-1}S_{B}w = \lambda w$</code>, we get:</p>
<p><code>$$ S_{w}^{-1}(\mu_{0}-\mu_{1}) \propto w \\ $$</code></p>
<p>which gives us the direction.</p>
<br>
<h2 id="fishers-linear-discriminant-analysis-for-multiple-classes">Fisher’s Linear Discriminant Analysis For Multiple Classes<a hidden class="anchor" aria-hidden="true" href="#fishers-linear-discriminant-analysis-for-multiple-classes">#</a></h2>
<div align="center">
<img src="/img_ML/6_Multi_LDA.jpeg" width=600px/>
</div>
<br>
<p>We have defined <code>$\varepsilon(w)=\frac{w^{T}S_{B}w}{w^{T}S_{W}w}$</code> that needs to be maximized. <code>$w$</code> is the largest eigen vectors of <code>$S_{W}^{-1}S_{B}$</code>. For two classes, <code>$w \propto S_{w}^{-1}(\mu_{0}-\mu_{1})$</code>. For <code>$k$</code>-class problem, Fisher Discriminant Analysis involves <code>$(k - 1)$</code> discriminant functions. Make <code>$W_{d \times (K-1)}$</code> where each column describes a discriminant. So now, we have to update the two notions we have defined for a <code>$2$</code>-class problem, <code>$S_{B}$</code> and <code>$S_{W}$</code> .</p>
<p><code>$$ S_{w}=\sum_{i=1}^{K}\Sigma_{i} \\ $$</code></p>
<p><code>$S_{B}$</code> generalization to multiple classes in not obvious. We will define the total variance <code>$S_{T}$</code> as the sum of the within class variance and between classes variance.</p>
<p><code>$$ S_{T} = S_{B}+S_{W} \\ $$</code></p>
<p>Where <code>$S_{T}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\mu)(x_{i}-\mu)^{T}$</code> and <code>$\mu=\frac{1}{n}\sum_{i=1}^{n}x_{i}$</code>. So,</p>
<p><code>$$ S_{B} = S_{T}-S_{W} \\ $$</code></p>
<p>It can be shown that <code>$W$</code> is the first <code>$(k - 1)$</code> eigen vectors of <code>$S_{W}^{-1}S_{B}$</code>.</p>
<br>
<h2 id="pca-vs-lda">PCA vs. LDA<a hidden class="anchor" aria-hidden="true" href="#pca-vs-lda">#</a></h2>
<p>Both Linear Discriminant Analysis (LDA) and Principal Component Analysis (PCA) are linear transformation techniques that are commonly used for dimensionality reduction. PCA can be described as an “unsupervised” algorithm, since it “ignores” class labels and its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset. In contrast to PCA, LDA is “supervised” and computes the directions (“linear discriminants”) that will represent the axes that that maximize the separation between multiple classes.</p>
<p>Although it might sound intuitive that LDA is superior to PCA for a multi-class classification task where the class labels are known, this might not always the case.</p>
<div align="center">
<img src="/img_ML/6_PCA_vs_LDA.PNG" width=800px/>
</div>
<br>
<blockquote>
<p><strong>PCA</strong> 实质上是在寻找一个子空间。而这个子空间是协方差矩阵的特征空间(特征向量对应的空间)，选取特征值最大的 k 个特征向量组成的特征子空间(相当于这个子空间有 k 维，每一维代表一个特征，这 ｋ 个特征基本可以涵盖 90% 以上的信息)。<strong>Fisher判别</strong> 和 PCA 是在做类似的一件事，都是在找子空间。不同的是, PCA 是找一个低维的子空间，样本投影在这个空间基本不丢失信息。而 Fisher是寻找这样的一个空间，样本投影在这个空间上，类内距离最小，类间距离最大。</p>
<p>两者的 <strong>相同点</strong> ：</p>
<ul>
<li>两者均可以对数据进行降维。</li>
<li>两者在降维时均使用了矩阵特征分解的思想。</li>
<li>两者都假设数据符合高斯分布。</li>
</ul>
<p>两者的 <strong>不同点</strong> ：</p>
<ul>
<li>LDA是有监督的降维方法，而PCA是无监督的降维方法</li>
<li>LDA降维最多降到类别数k-1的维数，而PCA没有这个限制。</li>
<li>LDA除了可以用于降维，还可以用于分类。</li>
<li>LDA选择分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向。</li>
</ul>
</blockquote>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] Raschka, S. (2014, August 3). Linear Discriminant Analysis. Dr. Sebastian Raschka. <a href="https://sebastianraschka.com/Articles/2014_python_lda.html">https://sebastianraschka.com/Articles/2014_python_lda.html</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/fda/">FDA</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/07_k_nearest_neighbors/">
    <span class="title">« Prev Page</span>
    <br>
    <span>K-Nearest Neighbors</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/machine_learning/05_principal_component_analysis/">
    <span class="title">Next Page »</span>
    <br>
    <span>Principal Component Analysis</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fisher’s Linear Discriminant Analysis on twitter"
        href="https://twitter.com/intent/tweet/?text=Fisher%e2%80%99s%20Linear%20Discriminant%20Analysis&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f&amp;hashtags=MachineLearning%2cFDA">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fisher’s Linear Discriminant Analysis on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f&amp;title=Fisher%e2%80%99s%20Linear%20Discriminant%20Analysis&amp;summary=Fisher%e2%80%99s%20Linear%20Discriminant%20Analysis&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fisher’s Linear Discriminant Analysis on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f&title=Fisher%e2%80%99s%20Linear%20Discriminant%20Analysis">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fisher’s Linear Discriminant Analysis on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fisher’s Linear Discriminant Analysis on whatsapp"
        href="https://api.whatsapp.com/send?text=Fisher%e2%80%99s%20Linear%20Discriminant%20Analysis%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Fisher’s Linear Discriminant Analysis on telegram"
        href="https://telegram.me/share/url?text=Fisher%e2%80%99s%20Linear%20Discriminant%20Analysis&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f06_fishers_linear_discriminant_analysis%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
