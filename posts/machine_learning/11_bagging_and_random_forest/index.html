<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Bagging and Random Forest | Followb1ind1y</title>
<meta name="keywords" content="Machine Learning, Bagging, Random Forest" />
<meta name="description" content="Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.
An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/machine_learning/11_bagging_and_random_forest/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Bagging and Random Forest" />
<meta property="og:description" content="Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.
An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/machine_learning/11_bagging_and_random_forest/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-05-28T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-05-28T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Bagging and Random Forest"/>
<meta name="twitter:description" content="Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.
An ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Bagging and Random Forest",
      "item": "https://followb1ind1y.github.io/posts/machine_learning/11_bagging_and_random_forest/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Bagging and Random Forest",
  "name": "Bagging and Random Forest",
  "description": "Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.\nAn ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier.",
  "keywords": [
    "Machine Learning", "Bagging", "Random Forest"
  ],
  "articleBody": "Ensemble Learning Ensemble methods aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.\nAn ensemble is itself a supervised learning algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model.When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are noise, variance, and bias. Ensemble helps to reduce these factors (except noise, which is irreducible error). The noise-related error is mainly due to noise in the training data and can’t be removed. However, the errors due to bias and variance can be reduced. The total error can be expressed as follows:\n$$ \\mathrm{Total \\ Error} = \\mathrm{Bias} + \\mathrm{Variance} + \\mathrm{Irreducible \\ Error} $$\nA measure such as mean square error (MSE) captures all of these errors for a continuous target variable and can be represented as follows:\n$$ \\begin{align*} \\mathrm{MSE} \u0026= E[(Y-\\hat{f}(x))^{2}] \\\\ \u0026= \\underbrace{[E[\\hat{f}(x)]-f(x)]^{2}}_{\\mathrm{Bias}} + \\underbrace{E[(\\hat{f}(x)-E[\\hat{f}(x)])^{2}]}_{\\mathrm{Variance}} + \\underbrace{\\varepsilon}_{\\mathrm{Noise}} \\\\ \\end{align*} $$\nUsing techniques like Bagging and Boosting helps to decrease the variance and increase the robustness of the model. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.\nEnsemble Algorithm The goal of ensemble algorithms is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.\n There are two families of ensemble methods which are usually distinguished:\n Averaging methods. The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.   Examples: Bagging methods, Forests of randomized trees.  Boosting methods. Base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.   Examples: AdaBoost, Gradient Tree Boosting.  Advantages of Ensemble Algorithm  Ensemble is a proven method for improving the accuracy of the model and works in most of the cases. Ensemble makes the model more robust and stable thus ensuring decent performance on the test cases in most scenarios. You can use ensemble to capture linear and simple as well nonlinear complex relationships in the data. This can be done by using two different models and forming an ensemble of two.  Disadvantages of Ensemble Algorithm  Ensemble reduces the model interpret-ability and makes it very difficult to draw any crucial business insights at the end It is time-consuming and thus might not be the best idea for real-time applications The selection of models for creating an ensemble is an art which is really hard to master  Bootstrap Sampling Sampling is the process of selecting a subset of observations from the population with the purpose of estimating some parameters about the whole population. Resampling methods, on the other hand, are used to improve the estimates of the population parameters. In machine learning, the bootstrap method refers to random sampling with replacement. This sample is referred to as a resample. This allows the model or algorithm to get a better understanding of the various biases, variances and features that exist in the resample. Taking a sample of the data allows the resample to contain different characteristics then it might have contained as a whole.\nBootstrapping is also great for small size data sets that can have a tendency to overfit. In fact, we recommended this to one company who was concerned because their data sets were far from “Big Data”. Bootstrapping can be a solution in this case because algorithms that utilize bootstrapping can be more robust and handle new data sets depending on the methodology chosen(boosting or bagging). The reason behind using the bootstrap method is because it can test the stability of a solution. By using multiple sample data sets and then testing multiple models, it can increase robustness. Perhaps one sample data set has a larger mean than another, or a different standard deviation. This might break a model that was overfit, and not tested using data sets with different variations.\nOne of the many reasons bootstrapping has become very common is because of the increase in computing power. This allows for many times more permutations to be done with different resamples than previously. Bootstrapping is used in both Bagging and Boosting.\n Assume we have a sample of $n$ values (x) and we’d like to get an estimate of the mean of the sample.\n$$ \\mathrm{mean}(x) = \\frac{1}{n} \\times \\mathrm{sum}(x) \\\\ $$\nConsider a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample. We know that our sample is small and that the mean has an error in it. We can improve the estimate of our mean using the bootstrap procedure:\n Create many (e.g. 1000) random sub-samples of the data set with replacement (meaning we can select the same value multiple times). Calculate the mean of each sub-sample Calculate the average of all of our collected means and use that as our estimated mean for the data  Bagging Bootstrap AGgregation, also known as Bagging, is a powerful ensemble method that was proposed by Leo Breiman in 1994 to prevent overfitting. The concept behind bagging is to combine the predictions of several base learners to create a more accurate output. Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.\n  Suppose there are N observations and M features. A sample from observation is selected randomly with replacement (Bootstrapping). A subset of features are selected to create a model with sample of observations and subset of features. Feature from the subset is selected which gives the best split on the training data. This is repeated to create many models and every model is trained in parallel Prediction is given based on the aggregation of predictions from all the models.  This approach can be used with machine learning algorithms that have a high variance, such as decision trees. A separate model is trained on each bootstrap sample of data and the average output of those models used to make predictions. This technique is called bootstrap aggregation or bagging for short. Variance means that an algorithm’s performance is sensitive to the training data, with high variance suggesting that the more the training data is changed, the more the performance of the algorithm will vary.\nThe performance of high variance machine learning algorithms like unpruned decision trees can be improved by training many trees and taking the average of their predictions. Results are often better than a single decision tree. What Bagging does is help reduce variance from models that are might be very accurate, but only on the data they were trained on. This is also known as overfitting. Bagging gets around this by creating its own variance amongst the data by sampling and replacing data while it tests multiple hypothesis(models). In turn, this reduces the noise by utilizing multiple samples that would most likely be made up of data with various attributes(median, average, etc).\nOnce each model has developed a hypothesis. The models use voting for classification or averaging for regression. This is where the “Aggregating” in “Bootstrap Aggregating” comes into play. Each hypothesis has the same weight as all the others. Essentially, all these models run at the same time, and vote on which hypothesis is the most accurate. This helps to decrease variance i.e. reduce the overfit.\n Bagging 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。大部分情况下，经过 bagging 得到的结果方差（variance）更小。\n Baging for Classification Consider a classification problem with $K$ classes. When averaging over the $B$ bootstrap samples, there are two options and conflicting advice\n  Option 1: use the majority of votes (consensus estimate)\n This is the only option mentioned in James et al.    Option 2: average the proportions from the individual classifiers and choose class with greatest average proportion\n The bagged predictor is the class with the largest average proportion:    $$ \\hat{G}_{bag}(x) = \\arg\\max_{k} \\hat{f}_{k}(x) \\\\ $$\nWhere $\\hat{f}_{k}$ is the average proportion for class $k$\n$$ \\hat{f}_{k} = \\frac{1}{B}\\sum_{b=1}^{B}\\hat{p}_{b}(x) \\\\ $$ Where $p_{b}$ is the proportion in the corresponding terminal leaf for bag $b$.\nAdvantages of Bagging  Bagging takes advantage of ensemble learning wherein multiple weak learners outperform a single strong learner. It helps reduce variance and thus helps us avoid overfitting.  Disadvantages of Bagging  There is loss of interpretability of the model. There can possibly be a problem of high bias if not modeled properly. While bagging gives us more accuracy, it is computationally expensive and may not be desirable depending on the use case.  Random Forest Random forest is a supervised learning algorithm. The “forest” it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result.\n Bagging + 决策树(Decision Tree) = 随机森林(Random Forest)\n The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random:\n Random sampling of training data points when building trees Random subsets of features considered when splitting nodes   The basic steps involved in performing the random forest algorithm are mentioned below:\n Pick N random records from the dataset. Build a decision tree based on these N records. Choose the number of trees you want in your algorithm and repeat steps 1 and 2. In case of a regression problem, for a new record, each tree in the forest predicts a value for Y (output). The final value can be calculated by taking the average of all the values predicted by all the trees in the forest. Or, in the case of a classification problem, each tree in the forest predicts the category to which the new record belongs. Finally, the new record is assigned to the category that wins the majority vote.  Advantages of Random Forest  Random forest algorithm is unbiased as there are multiple trees and each tree is trained on a subset of data. Random Forest algorithm is very stable. Introducing a new data in the dataset does not affect much as the new data impacts one tree and is pretty hard to impact all the trees. The random forest algorithm works well when you have both categorical and numerical features. With missing values in the dataset, the random forest algorithm performs very well.  Disadvantages of Random Forest  A major disadvantage of random forests lies in their complexity. More computational resources are required and also results in the large number of decision trees joined together. Due to their complexity, training time is more compared to other algorithms.  Reference [1] VanderPlas, J. (n.d.). In-Depth: Decision Trees and Random Forests. In-Depth: Decision Trees and Random Forests | Python Data Science Handbook. https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html.\n[2] Sarkar, P., S, A., \u0026 Shah, P. (2019, October 14). Bagging and Random Forest in Machine Learning: How do they work? Knowledgehut. https://www.knowledgehut.com/blog/data-science/bagging-and-random-forest-in-machine-learning.\n",
  "wordCount" : "1912",
  "inLanguage": "en",
  "datePublished": "2021-05-28T00:00:00Z",
  "dateModified": "2021-05-28T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/machine_learning/11_bagging_and_random_forest/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Bagging and Random Forest
    </h1>
    <div class="post-meta">May 28, 2021&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#ensemble-learning" aria-label="Ensemble Learning">Ensemble Learning</a><ul>
                        
                <li>
                    <a href="#ensemble-algorithm" aria-label="Ensemble Algorithm">Ensemble Algorithm</a></li>
                <li>
                    <a href="#advantages-of-ensemble-algorithm" aria-label="Advantages of Ensemble Algorithm">Advantages of Ensemble Algorithm</a></li>
                <li>
                    <a href="#disadvantages-of-ensemble-algorithm" aria-label="Disadvantages of Ensemble Algorithm">Disadvantages of Ensemble Algorithm</a></li></ul>
                </li>
                <li>
                    <a href="#bootstrap-sampling" aria-label="Bootstrap Sampling">Bootstrap Sampling</a></li>
                <li>
                    <a href="#bagging" aria-label="Bagging">Bagging</a><ul>
                        
                <li>
                    <a href="#baging-for-classification" aria-label="Baging for Classification">Baging for Classification</a></li>
                <li>
                    <a href="#advantages-of-bagging" aria-label="Advantages of Bagging">Advantages of Bagging</a></li>
                <li>
                    <a href="#disadvantages-of-bagging" aria-label="Disadvantages of Bagging">Disadvantages of Bagging</a></li></ul>
                </li>
                <li>
                    <a href="#random-forest" aria-label="Random Forest">Random Forest</a><ul>
                        
                <li>
                    <a href="#advantages-of-random-forest" aria-label="Advantages of Random Forest">Advantages of Random Forest</a></li>
                <li>
                    <a href="#disadvantages-of-random-forest" aria-label="Disadvantages of Random Forest">Disadvantages of Random Forest</a></li></ul>
                </li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h2 id="ensemble-learning">Ensemble Learning<a hidden class="anchor" aria-hidden="true" href="#ensemble-learning">#</a></h2>
<p><strong>Ensemble methods</strong> aim at improving the predictive performance of a given statistical learning or model ﬁtting technique. The general principle of ensemble methods is to construct a linear combination of some model ﬁtting method, instead of using a single ﬁt of the method.</p>
<p>An ensemble is itself a <strong>supervised learning</strong> algorithm, because it can be trained and then used to make predictions. Ensemble methods combine several decision trees classifiers to produce better predictive performance than a single decision tree classifier. The main principle behind the ensemble model is that a group of weak learners come together to form a strong learner, thus increasing the accuracy of the model.When we try to predict the target variable using any machine learning technique, the main causes of difference in actual and predicted values are <strong>noise</strong>, <strong>variance</strong>, and <strong>bias</strong>. Ensemble helps to reduce these factors (except noise, which is irreducible error). The noise-related error is mainly due to noise in the training data and can&rsquo;t be removed. However, the errors due to bias and variance can be reduced. The total error can be expressed as follows:</p>
<p><code>$$ \mathrm{Total \ Error} = \mathrm{Bias} + \mathrm{Variance} + \mathrm{Irreducible \ Error} $$</code></p>
<p>A measure such as <strong>mean square error (MSE)</strong> captures all of these errors for a continuous target variable and can be represented as follows:</p>
<p><code>$$ \begin{align*} \mathrm{MSE} &amp;= E[(Y-\hat{f}(x))^{2}] \\ &amp;= \underbrace{[E[\hat{f}(x)]-f(x)]^{2}}_{\mathrm{Bias}} + \underbrace{E[(\hat{f}(x)-E[\hat{f}(x)])^{2}]}_{\mathrm{Variance}} + \underbrace{\varepsilon}_{\mathrm{Noise}} \\ \end{align*} $$</code></p>
<p>Using techniques like Bagging and Boosting helps to decrease the variance and increase the robustness of the model. Combinations of multiple classifiers decrease variance, especially in the case of unstable classifiers, and may produce a more reliable classification than a single classifier.</p>
<h3 id="ensemble-algorithm">Ensemble Algorithm<a hidden class="anchor" aria-hidden="true" href="#ensemble-algorithm">#</a></h3>
<p>The goal of ensemble algorithms is to combine the predictions of several base estimators built with a given learning algorithm in order to improve generalizability / robustness over a single estimator.</p>
<div align="center">
<img src="/img_ML/11_Ensemble_Algorithm.PNG" width=600px/>
</div>
<p>There are two families of ensemble methods which are usually distinguished:</p>
<ol>
<li><strong>Averaging methods.</strong> The driving principle is to build several estimators independently and then to average their predictions. On average, the combined estimator is usually better than any of the single base estimator because its variance is reduced.</li>
</ol>
<ul>
<li><strong>Examples:</strong> Bagging methods, Forests of randomized trees.</li>
</ul>
<ol start="2">
<li><strong>Boosting methods.</strong> Base estimators are built sequentially and one tries to reduce the bias of the combined estimator. The motivation is to combine several weak models to produce a powerful ensemble.</li>
</ol>
<ul>
<li><strong>Examples:</strong> AdaBoost, Gradient Tree Boosting.</li>
</ul>
<h3 id="advantages-of-ensemble-algorithm">Advantages of Ensemble Algorithm<a hidden class="anchor" aria-hidden="true" href="#advantages-of-ensemble-algorithm">#</a></h3>
<ul>
<li>Ensemble is a proven method for improving the accuracy of the model and works in most of the cases.</li>
<li>Ensemble makes the model more robust and stable thus ensuring decent performance on the test cases in most scenarios.</li>
<li>You can use ensemble to capture linear and simple as well nonlinear complex relationships in the data. This can be done by using two different models and forming an ensemble of two.</li>
</ul>
<h3 id="disadvantages-of-ensemble-algorithm">Disadvantages of Ensemble Algorithm<a hidden class="anchor" aria-hidden="true" href="#disadvantages-of-ensemble-algorithm">#</a></h3>
<ul>
<li>Ensemble reduces the model interpret-ability and makes it very difficult to draw any crucial business insights at the end</li>
<li>It is time-consuming and thus might not be the best idea for real-time applications</li>
<li>The selection of models for creating an ensemble is an art which is really hard to master</li>
</ul>
<br>
<h2 id="bootstrap-sampling">Bootstrap Sampling<a hidden class="anchor" aria-hidden="true" href="#bootstrap-sampling">#</a></h2>
<p>Sampling is the process of selecting a subset of observations from the population with the purpose of estimating some parameters about the whole population. Resampling methods, on the other hand, are used to improve the estimates of the population parameters. In machine learning, the bootstrap method refers to random sampling with replacement. This sample is referred to as a resample. This allows the model or algorithm to get a better understanding of the various biases, variances and features that exist in the resample. Taking a sample of the data allows the resample to contain different characteristics then it might have contained as a whole.</p>
<p>Bootstrapping is also great for <strong>small size</strong> data sets that can have a tendency to overfit. In fact, we recommended this to one company who was concerned because their data sets were far from “Big Data”. Bootstrapping can be a solution in this case because algorithms that utilize bootstrapping can be more robust and handle new data sets depending on the methodology chosen(boosting or bagging). The reason behind using the bootstrap method is because it can test the stability of a solution. By using multiple sample data sets and then testing multiple models, it can increase robustness. Perhaps one sample data set has a larger mean than another, or a different standard deviation. This might break a model that was overfit, and not tested using data sets with different variations.</p>
<p>One of the many reasons bootstrapping has become very common is because of the increase in computing power. This allows for many times more permutations to be done with different resamples than previously. Bootstrapping is used in both Bagging and Boosting.</p>
<div align="center">
<img src="/img_ML/11_bootstrap.PNG" width=600px/>
</div>
<p>Assume we have a sample of <code>$n$</code> values (x) and we’d like to get an estimate of the mean of the sample.</p>
<p><code>$$ \mathrm{mean}(x) = \frac{1}{n} \times \mathrm{sum}(x) \\ $$</code></p>
<p>Consider a sample of 100 values (x) and we’d like to get an estimate of the mean of the sample. We know that our sample is small and that the mean has an error in it. We can improve the estimate of our mean using the bootstrap procedure:</p>
<ol>
<li>Create many (e.g. 1000) random sub-samples of the data set with replacement (meaning we can select the same value multiple times).</li>
<li>Calculate the mean of each sub-sample</li>
<li>Calculate the average of all of our collected means and use that as our estimated mean for the data</li>
</ol>
<br>
<h2 id="bagging">Bagging<a hidden class="anchor" aria-hidden="true" href="#bagging">#</a></h2>
<p><strong>B</strong>ootstrap <strong>AG</strong>gregation, also known as <strong>Bagging</strong>, is a powerful ensemble method that was proposed by Leo Breiman in 1994 to prevent overfitting. The concept behind bagging is to combine the predictions of several base learners to create a more accurate output. Bagging is the application of the Bootstrap procedure to a high-variance machine learning algorithm, typically decision trees.</p>
<div align="center">
<img src="/img_ML/11_Bagging.PNG" width=600px/>
</div>
<ol>
<li>Suppose there are N observations and M features. A sample from observation is selected randomly with replacement (Bootstrapping).</li>
<li>A subset of features are selected to create a model with sample of observations and subset of features.</li>
<li>Feature from the subset is selected which gives the best split on the training data.</li>
<li>This is repeated to create many models and every model is trained in parallel</li>
<li>Prediction is given based on the aggregation of predictions from all the models.</li>
</ol>
<p>This approach can be used with machine learning algorithms that have a <strong>high variance</strong>, such as decision trees. A separate model is trained on each bootstrap sample of data and the average output of those models used to make predictions. This technique is called bootstrap aggregation or bagging for short. Variance means that an algorithm’s performance is sensitive to the training data, with high variance suggesting that the more the training data is changed, the more the performance of the algorithm will vary.</p>
<p>The performance of high variance machine learning algorithms like unpruned decision trees can be improved by training many trees and taking the average of their predictions. Results are often better than a single decision tree. What Bagging does is help <strong>reduce variance</strong> from models that are might be very accurate, but only on the data they were trained on. This is also known as <strong>overfitting</strong>. Bagging gets around this by creating its own variance amongst the data by sampling and replacing data while it tests multiple hypothesis(models). In turn, this reduces the noise by utilizing multiple samples that would most likely be made up of data with various attributes(median, average, etc).</p>
<p>Once each model has developed a hypothesis. The models use voting for classification or averaging for regression. This is where the &ldquo;Aggregating&rdquo; in &ldquo;Bootstrap Aggregating&rdquo; comes into play. Each hypothesis has the <strong>same weight</strong> as all the others. Essentially, all these models run at the same time, and vote on which hypothesis is the most accurate. This helps to decrease variance i.e. reduce the overfit.</p>
<blockquote>
<p><strong>Bagging</strong> 的思路是所有基础模型都一致对待，每个基础模型手里都只有一票。然后使用民主投票的方式得到最终的结果。大部分情况下，经过 bagging 得到的结果方差（variance）更小。</p>
</blockquote>
<h3 id="baging-for-classification">Baging for Classification<a hidden class="anchor" aria-hidden="true" href="#baging-for-classification">#</a></h3>
<p>Consider a classification problem with <code>$K$</code> classes. When averaging over the <code>$B$</code> bootstrap samples, there are two options and conflicting advice</p>
<ul>
<li>
<p><strong>Option 1:</strong> use the majority of votes (consensus estimate)</p>
<ul>
<li>This is the only option mentioned in James et al.</li>
</ul>
</li>
<li>
<p><strong>Option 2:</strong> average the proportions from the individual classifiers and choose class with greatest average proportion</p>
<ul>
<li>The bagged predictor is the class with the largest average proportion:</li>
</ul>
</li>
</ul>
<p><code>$$ \hat{G}_{bag}(x) = \arg\max_{k} \hat{f}_{k}(x) \\ $$</code></p>
<p>Where <code>$\hat{f}_{k}$</code> is the average proportion for class <code>$k$</code></p>
<p><code>$$ \hat{f}_{k} = \frac{1}{B}\sum_{b=1}^{B}\hat{p}_{b}(x) \\ $$</code>
Where <code>$p_{b}$</code> is the proportion in the corresponding terminal leaf for bag <code>$b$</code>.</p>
<h3 id="advantages-of-bagging">Advantages of Bagging<a hidden class="anchor" aria-hidden="true" href="#advantages-of-bagging">#</a></h3>
<ul>
<li>Bagging takes advantage of ensemble learning wherein multiple weak learners outperform a single strong learner.</li>
<li>It helps reduce variance and thus helps us avoid overfitting.</li>
</ul>
<h3 id="disadvantages-of-bagging">Disadvantages of Bagging<a hidden class="anchor" aria-hidden="true" href="#disadvantages-of-bagging">#</a></h3>
<ul>
<li>There is loss of interpretability of the model.</li>
<li>There can possibly be a problem of high bias if not modeled properly.</li>
<li>While bagging gives us more accuracy, it is computationally expensive and may not be desirable depending on the use case.</li>
</ul>
<br>
<h2 id="random-forest">Random Forest<a hidden class="anchor" aria-hidden="true" href="#random-forest">#</a></h2>
<p>Random forest is a supervised learning algorithm. The &ldquo;forest&rdquo; it builds, is an ensemble of decision trees, usually trained with the “bagging” method. The general idea of the bagging method is that a combination of learning models increases the overall result.</p>
<blockquote>
<p><strong>Bagging</strong> + <strong>决策树(Decision Tree)</strong> = <strong>随机森林(Random Forest)</strong></p>
</blockquote>
<p>The random forest is a model made up of many decision trees. Rather than just simply averaging the prediction of trees (which we could call a “forest”), this model uses two key concepts that gives it the name random:</p>
<ol>
<li>Random sampling of training data points when building trees</li>
<li>Random subsets of features considered when splitting nodes</li>
</ol>
<div align="center">
<img src="/img_ML/11_Random_Forest.PNG" width=550px/>
</div>
<p>The basic steps involved in performing the random forest algorithm are mentioned below:</p>
<ol>
<li>Pick N random records from the dataset.</li>
<li>Build a decision tree based on these N records.</li>
<li>Choose the number of trees you want in your algorithm and repeat steps 1 and 2.</li>
<li>In case of a regression problem, for a new record, each tree in the forest predicts a value for Y (output). The final value can be calculated by taking the average of all the values predicted by all the trees in the forest. Or, in the case of a classification problem, each tree in the forest predicts the category to which the new record belongs. Finally, the new record is assigned to the category that wins the majority vote.</li>
</ol>
<h3 id="advantages-of-random-forest">Advantages of Random Forest<a hidden class="anchor" aria-hidden="true" href="#advantages-of-random-forest">#</a></h3>
<ul>
<li>Random forest algorithm is unbiased as there are multiple trees and each tree is trained on a subset of data.</li>
<li>Random Forest algorithm is very stable. Introducing a new data in the dataset does not affect much as the new data impacts one tree and is pretty hard to impact all the trees.</li>
<li>The random forest algorithm works well when you have both categorical and numerical features.</li>
<li>With missing values in the dataset, the random forest algorithm performs very well.</li>
</ul>
<h3 id="disadvantages-of-random-forest">Disadvantages of Random Forest<a hidden class="anchor" aria-hidden="true" href="#disadvantages-of-random-forest">#</a></h3>
<ul>
<li>A major disadvantage of random forests lies in their complexity. More computational resources are required and also results in the large number of decision trees joined together.</li>
<li>Due to their complexity, training time is more compared to other algorithms.</li>
</ul>
<br>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] VanderPlas, J. (n.d.). In-Depth: Decision Trees and Random Forests. In-Depth: Decision Trees and Random Forests | Python Data Science Handbook. <a href="https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html">https://jakevdp.github.io/PythonDataScienceHandbook/05.08-random-forests.html</a>.</p>
<p>[2] Sarkar, P., S, A., &amp; Shah, P. (2019, October 14). Bagging and Random Forest in Machine Learning: How do they work? Knowledgehut. <a href="https://www.knowledgehut.com/blog/data-science/bagging-and-random-forest-in-machine-learning">https://www.knowledgehut.com/blog/data-science/bagging-and-random-forest-in-machine-learning</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/machine-learning/">Machine Learning</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/bagging/">Bagging</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/random-forest/">Random Forest</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/machine_learning/12_boosting/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Boosting</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/machine_learning/10_decision_tree/">
    <span class="title">Next Page »</span>
    <br>
    <span>Decision Trees</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bagging and Random Forest on twitter"
        href="https://twitter.com/intent/tweet/?text=Bagging%20and%20Random%20Forest&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f&amp;hashtags=MachineLearning%2cBagging%2cRandomForest">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bagging and Random Forest on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f&amp;title=Bagging%20and%20Random%20Forest&amp;summary=Bagging%20and%20Random%20Forest&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bagging and Random Forest on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f&title=Bagging%20and%20Random%20Forest">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bagging and Random Forest on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bagging and Random Forest on whatsapp"
        href="https://api.whatsapp.com/send?text=Bagging%20and%20Random%20Forest%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Bagging and Random Forest on telegram"
        href="https://telegram.me/share/url?text=Bagging%20and%20Random%20Forest&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fmachine_learning%2f11_bagging_and_random_forest%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
