<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural Network: Radial Basis Function Neural Networks (RBN) | Followb1ind1y</title>
<meta name="keywords" content="RBN, SURE" />
<meta name="description" content="In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).
 RBNN is composed of input, hidden, and output layer.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Neural Network: Radial Basis Function Neural Networks (RBN)" />
<meta property="og:description" content="In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).
 RBNN is composed of input, hidden, and output layer." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-04T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-07-04T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Neural Network: Radial Basis Function Neural Networks (RBN)"/>
<meta name="twitter:description" content="In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).
 RBNN is composed of input, hidden, and output layer."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Neural Network: Radial Basis Function Neural Networks (RBN)",
      "item": "https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural Network: Radial Basis Function Neural Networks (RBN)",
  "name": "Neural Network: Radial Basis Function Neural Networks (RBN)",
  "description": "In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).\n RBNN is composed of input, hidden, and output layer.",
  "keywords": [
    "RBN", "SURE"
  ],
  "articleBody": "In Single Perceptron / Multi-layer Perceptron(MLP), we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our RBN what it does is, it transforms the input signal into another form, which can be then feed into the network to get linear separability. RBN is structurally same as perceptron(MLP).\n RBNN is composed of input, hidden, and output layer. RBNN is strictly limited to have exactly one hidden layer. We call this hidden layer as feature vector. We apply non-linear transfer function to the feature vector before we go for classification problem. When we increase the dimension of the feature vector, the linear separability of feature vector increases.\nNetwork Structure  1. Input : $$ \\{(x_{i},y_{i})\\}_{i=1}^{n} \\ , \\ \\mathrm{where} \\ x_{i} \\subset \\mathbb{R}^{d} \\\\ $$\n$$ X = \\begin{bmatrix} x_{11} \\ \\cdots \\ x_{1n} \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ x_{d1} \\ \\cdots \\ x_{dn} \\\\ \\end{bmatrix}_{\\ d\\times n} = \\begin{bmatrix} \\vdots \\ \\ \\ \\vdots \\\\ x_{1} \\ \\cdots \\ x_{n} \\\\ \\vdots \\ \\ \\ \\vdots \\\\ \\end{bmatrix}_{\\ d\\times n} \\ , \\ Y = \\begin{bmatrix} y_{11} \\ \\cdots \\ y_{1n} \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ y_{k1} \\ \\cdots \\ y_{kn} \\\\ \\end{bmatrix}_{\\ k\\times n} $$\n2. Radial Basis Function  We define a $\\mathrm{receptor} = t$. We draw confrontal maps around the $\\mathrm{receptor}$. Gaussian Functions are generally used for Radian Basis Function(confrontal mapping). So we define the radial distance $r = \\parallel x- t \\parallel.$ There are many choices for the basis function. The commonly used is:  $$ \\phi_{j}(x_{i}) = e^{-|x_{i}\\ - \\ \\mu_{j}|^{2}} $$\n 3. Output: $$ y_{k}(x)=\\sum_{j=1}^{m}W_{jk}\\phi_{j}(x) $$\n$$ W = \\begin{bmatrix} w_{11} \\ \\cdots \\ w_{1k} \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ w_{m1} \\ \\cdots \\ w_{mk} \\\\ \\end{bmatrix}_{\\ m\\times k} \\ , \\ \\phi = \\begin{bmatrix} \\phi_{1}(x_{1}) \\ \\cdots \\ \\phi_{1}(x_{n}) \\\\ \\vdots \\ \\ddots \\ \\vdots \\\\ \\phi_{m}(x_{1}) \\ \\cdots \\ \\phi_{m}(x_{n}) \\\\ \\end{bmatrix}_{\\ m\\times n} $$\nThe output will be:\n$$ Y = W^{T}\\phi $$\nwhere $Y$ and $\\phi$ are known while $W$ is unknown.\nOptimization $$ \\psi = \\parallel Y - W^{T}\\phi \\ \\parallel^{2} $$\n$W$ can be computed by minimizing our objective function w.r.t $w$.\n$$ \\min_{W}\\parallel Y - W^{T}\\phi \\ \\parallel^{2} $$\nThis optimization problem can be solved in close form:\n$$ \\begin{align*} \\frac{\\partial}{\\partial W}\\parallel Y - W^{T}\\phi \\ \\parallel^{2} \u0026= \\frac{\\partial}{\\partial W}Tr[(Y - W^{T}\\phi)^{T}(Y - W^{T}\\phi)] \\\\ \u0026= \\frac{\\partial}{\\partial W}Tr[Y^{T}Y+\\phi^{T}WW^{T}\\phi-Y^{T}W^{T}\\phi - \\phi^{T}WY] \\\\ \u0026= 0 + 2\\phi\\phi^{T}W - 2\\phi Y^{T} = 0 \\\\ \u0026\\Rightarrow \\phi\\phi^{T}W = \\phi Y^{T} \\\\ \u0026\\Rightarrow W = (\\phi\\phi^{T})^{-1}\\phi Y^{T} \\end{align*} $$\nIn RBF network the estimated function is:\n$$ \\begin{align*} \\hat{Y} \u0026= W^{T}\\phi \\\\ \u0026= ((\\phi\\phi^{T})^{-1}\\phi Y^{T})^{T}\\phi \\\\ \u0026= Y\\phi^{T}((\\phi\\phi^{T})^{-1})^{T}\\phi \\\\ \\Rightarrow \\hat{Y}^{T} \u0026= \\underbrace{\\phi^{T}(\\phi\\phi^{T})^{-1}\\phi}_{H} Y^{T} \\\\ \\Rightarrow \\hat{Y}^{T} \u0026= HY^{T} \\end{align*} $$\nSteinâ€™s unbiased risk estimator (SURE) Assume $ T = \\{(x_{i},y_{i})\\}_{i=1}^{n} $ be the training set.\n $f(\\cdot)$ $\\to$ True model $\\hat{f}(\\cdot)$ $\\to$ Estimated model err $\\to$ Empirical error: $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}$ Err $\\to$ True error: $\\frac{1}{n}\\sum_{i=1}^{n}(\\hat{f}_{i}-f_{i})^{2}$ $y$ $\\to$ Observations  Also assume\n$$ y_{i} = f(x_{i}) + \\varepsilon_{i} \\ , \\ \\ \\ \\ \\mathrm{Where} \\ \\varepsilon_{i} \\sim N(0,\\sigma^{2}) $$\nFor point $(x_{0},y_{0})$ we are interested in\n$$ \\begin{align*} E[(\\hat{y}_{0}-y_{0})^{2}] \u0026= E[(\\hat{f}_{0}-f_{0}-\\varepsilon_{0})^{2}] \\\\ \u0026= E[((\\hat{f}_{0}-f_{0})-\\varepsilon_{0})^{2}] \\\\ \u0026= E[(\\hat{f}_{0}-f_{0})^{2}+\\varepsilon_{0}^{2}-2\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\ \u0026= E[(\\hat{f}_{0}-f_{0})^{2}]+E[\\varepsilon_{0}^{2}]-2E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\ \u0026= E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2}-2E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\\\ \\end{align*} $$\nCase 1 Assume $(x_{0},y_{0})\\notin T$.\nIn this case, since $\\hat{f}$ is estimated only based on points in training set, therefore it is completely independent from $(x_{0},y_{0})$\n$$ E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] = 0 \\\\ \\Rightarrow E[(\\hat{y}_{0}-y_{0})^{2}] = E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2} $$\nIf summing up all $m$ points that are not in $T$.\n$$ \\underbrace{\\sum_{i=1}^{m}(\\hat{y}_{i}-y_{i})^{2}}_{\\mathrm{err}}= \\underbrace{\\sum_{i=1}^{m}(\\hat{f}_{i}-f_{i})^{2}}_{\\mathrm{Err}}+ m\\sigma^{2} \\\\ \\mathrm{err} = \\mathrm{Err} + m\\sigma^{2} \\\\ \\mathrm{Err} = \\mathrm{err} - m\\sigma^{2} $$\nEmpirical error ($\\mathrm{err}$) is a good estimator of true error ($\\mathrm{Err}$) if the point $(x_{0},y_{0})$ is not in the training set.\nCase 2 Assume $(x_{0},y_{0})\\in T$. Then $E[\\varepsilon_{0}(\\hat{f}_{0}-f_{0})] \\neq 0$\nSteinâ€™s Lemma: If $x \\sim N(\\theta,\\sigma^{2})$ and $g(x)$ differentiable. Then,\n$$ E[g(x)(x-\\theta)]=\\sigma^{2}E[\\frac{\\partial g(x)}{\\partial x}] $$\n$$ \\begin{align*} E[\\underbrace{\\varepsilon_{0}}_{(x-\\theta)}\\underbrace{(\\hat{f}_{0}-f_{0})}_{g(x)}] \u0026=\\sigma^{2}E[\\frac{\\partial (\\hat{f}_{0}-f_{0})}{\\partial \\varepsilon_{0}}] \\\\ \u0026= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial \\varepsilon_{0}}-\\underbrace{\\frac{\\partial f_{0}}{\\partial \\varepsilon_{0}}}_{0}] \\\\ \u0026= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial \\varepsilon_{0}}] \\\\ \u0026= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial y_{0}}\\cdot\\underbrace{\\frac{\\partial y_{0}}{\\partial \\varepsilon_{0}}}_{1}] \\\\ \u0026= \\sigma^{2}E[\\frac{\\partial \\hat{f}_{0}}{\\partial y_{0}}] \\\\ \u0026= \\sigma^{2}E[D_{0}] \\end{align*} $$\nThus,\n$$ E[(\\hat{y}_{0}-y_{0})^{2}] = E[(\\hat{f}_{0}-f_{0})^{2}]+\\sigma^{2} - 2\\sigma^{2}E[D_{0}] $$\nSum over all $n$ data points:\n$$ \\underbrace{\\sum_{i=1}^{n}(\\hat{y}_{i}-y_{i})^{2}}_{\\mathrm{err}}= \\underbrace{\\sum_{i=1}^{n}(\\hat{f}_{i}-f_{i})^{2}}_{\\mathrm{Err}}+ n\\sigma^{2} -2\\sigma^{2}\\sum_{i=1}^{n}D_{i} \\\\ \\mathrm{Err} = \\mathrm{err} - n\\sigma^{2}+\\underbrace{2\\sigma^{2}\\sum_{i=1}^{n}D_{i}}_{\\mathrm{Complexity \\ of \\ model}} \\Rightarrow \\mathrm{Steinâ€™s \\ Unbiased \\ Risk \\ Estimator \\ (SURE)} $$\nCoplexity control for RBN Letâ€™s apply SURE to RBF:\n$$ \\left. \\begin{array} \\\\ D_{i} = \\frac{\\partial \\hat{f}_{i}}{\\partial y_{i}} \\\\ \\hat{f}_{i} = \\hat{y}_{i} = H_{i:y} \\end{array} \\right \\}D_{i}=\\frac{\\partial \\hat{f}_{i}}{\\partial y_{i}} = \\frac{\\partial H_{i:y}}{\\partial y_{i}} = H_{ii} $$\nThen SURE will be:\n$$ \\begin{align*} \\mathrm{Err} \u0026= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}\\sum_{i=1}^{n}H_{ii} \\\\ \u0026= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr(H) \\\\ \u0026= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr[\\phi^{T}(\\phi\\phi^{T})^{-1}\\phi] \\\\ \u0026= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr[\\underbrace{\\phi\\phi^{T}(\\phi\\phi^{T})^{-1}}_{I\\to m\\times m}] \\\\ \u0026= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}Tr[I_{m}] \\\\ \u0026= \\mathrm{err} - n\\sigma^{2}+2\\sigma^{2}m \\\\ \\end{align*} $$\nFor computing SURE, we need to know the value of $\\sigma$. But we do not know it. Therefore we need to estimate it.\n$$ \\sigma^{2} = \\frac{\\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i})^{2}}{n-1} $$\nis the function of complexity (more complex, smaller $\\sigma^{2}$), in practice, we do not consider the $\\hat{y}$ to be the function of complexity and instead we consider it to be a low bias and high variance estimation (for example a line). With this assumption the $\\sigma$ will be considered to be constant and independent from the complexity of model.\nReference [1] McCormick, C. (2013, August 15). Radial Basis Function Network (RBFN) Tutorial. Radial Basis Function Network (RBFN) Tutorial Â· Chris McCormick. https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/.\n",
  "wordCount" : "926",
  "inLanguage": "en",
  "datePublished": "2021-07-04T00:00:00Z",
  "dateModified": "2021-07-04T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;Â»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Neural Network: Radial Basis Function Neural Networks (RBN)
    </h1>
    <div class="post-meta">July 4, 2021&nbsp;Â·&nbsp;5 min&nbsp;Â·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#network-structure" aria-label="Network Structure">Network Structure</a><ul>
                        
                <li>
                    <a href="#1-input-" aria-label="1. Input :">1. Input :</a></li>
                <li>
                    <a href="#2-radial-basis-function" aria-label="2. Radial Basis Function">2. Radial Basis Function</a></li>
                <li>
                    <a href="#3-output" aria-label="3. Output:">3. Output:</a></li></ul>
                </li>
                <li>
                    <a href="#optimization" aria-label="Optimization">Optimization</a></li>
                <li>
                    <a href="#steins-unbiased-risk-estimator-sure" aria-label="Steinâ€™s unbiased risk estimator (SURE)">Steinâ€™s unbiased risk estimator (SURE)</a><ul>
                        
                <li>
                    <a href="#case-1" aria-label="Case 1">Case 1</a></li>
                <li>
                    <a href="#case-2" aria-label="Case 2">Case 2</a></li></ul>
                </li>
                <li>
                    <a href="#coplexity-control-for-rbn" aria-label="Coplexity control for RBN">Coplexity control for RBN</a></li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p>In <strong>Single Perceptron / Multi-layer Perceptron(MLP)</strong>, we only have linear separability because they are composed of input and output layers(some hidden layers in MLP). We at least need one hidden layer to derive a non-linearity separation. Our <strong>RBN</strong> what it does is, it transforms the input signal into another form, which can be then feed into the network to get <strong>linear separability</strong>. RBN is structurally same as perceptron(MLP).</p>
<div align="center">
  <img src="/img_DL/02_MLP_RBN.PNG" width=400px/>
</div>
<p>RBNN is composed of <strong>input</strong>, <strong>hidden</strong>, and <strong>output</strong> layer. RBNN is <strong>strictly limited</strong> to have exactly <strong>one</strong> hidden layer. We call this hidden layer as <strong>feature vector</strong>. We apply <strong>non-linear transfer function</strong> to the feature vector before we go for classification problem. When we increase the dimension of the feature vector, the linear separability of feature vector increases.</p>
<h2 id="network-structure">Network Structure<a hidden class="anchor" aria-hidden="true" href="#network-structure">#</a></h2>
<div align="center">
  <img src="/img_DL/02_RBN_Structure.PNG" width=350px/>
</div>
<h3 id="1-input-">1. Input :<a hidden class="anchor" aria-hidden="true" href="#1-input-">#</a></h3>
<p><code>$$ \{(x_{i},y_{i})\}_{i=1}^{n} \ , \ \mathrm{where} \ x_{i} \subset \mathbb{R}^{d} \\ $$</code></p>
<p><code>$$ X = \begin{bmatrix} x_{11} \ \cdots \ x_{1n} \\ \vdots \ \ddots \ \vdots \\ x_{d1} \ \cdots \ x_{dn} \\ \end{bmatrix}_{\ d\times n} = \begin{bmatrix} \vdots \ \ \ \vdots \\ x_{1} \ \cdots \  x_{n} \\ \vdots \ \ \ \vdots \\ \end{bmatrix}_{\ d\times n} \ , \ Y = \begin{bmatrix} y_{11} \ \cdots \ y_{1n} \\ \vdots \ \ddots \ \vdots \\ y_{k1} \ \cdots \ y_{kn} \\ \end{bmatrix}_{\ k\times n} $$</code></p>
<h3 id="2-radial-basis-function">2. Radial Basis Function<a hidden class="anchor" aria-hidden="true" href="#2-radial-basis-function">#</a></h3>
<ul>
<li>We define a <code>$\mathrm{receptor} = t$</code>.</li>
<li>We draw confrontal maps around the <code>$\mathrm{receptor}$</code>.</li>
<li>Gaussian Functions are generally used for Radian Basis Function(confrontal mapping). So we define the radial distance <code>$r = \parallel x- t \parallel.$</code></li>
<li>There are many choices for the basis function. The commonly used is:</li>
</ul>
<p><code>$$ \phi_{j}(x_{i}) = e^{-|x_{i}\ - \ \mu_{j}|^{2}} $$</code></p>
<div align="center">
  <img src="/img_DL/02_Radial_Function.jpeg" width=350px/>
</div>
<h3 id="3-output">3. Output:<a hidden class="anchor" aria-hidden="true" href="#3-output">#</a></h3>
<p><code>$$ y_{k}(x)=\sum_{j=1}^{m}W_{jk}\phi_{j}(x) $$</code></p>
<p><code>$$ W = \begin{bmatrix} w_{11} \ \cdots \ w_{1k} \\ \vdots \ \ddots \ \vdots \\ w_{m1} \ \cdots \ w_{mk} \\ \end{bmatrix}_{\ m\times k} \ , \ \phi = \begin{bmatrix} \phi_{1}(x_{1}) \ \cdots \ \phi_{1}(x_{n}) \\ \vdots \ \ddots \ \vdots \\ \phi_{m}(x_{1}) \ \cdots \ \phi_{m}(x_{n}) \\ \end{bmatrix}_{\ m\times n} $$</code></p>
<p>The output will be:</p>
<p><code>$$ Y = W^{T}\phi $$</code></p>
<p>where <code>$Y$</code> and <code>$\phi$</code> are known while <code>$W$</code> is unknown.</p>
<h2 id="optimization">Optimization<a hidden class="anchor" aria-hidden="true" href="#optimization">#</a></h2>
<p><code>$$ \psi = \parallel Y - W^{T}\phi \ \parallel^{2} $$</code></p>
<p><code>$W$</code> can be computed by minimizing our objective function w.r.t <code>$w$</code>.</p>
<p><code>$$ \min_{W}\parallel Y - W^{T}\phi \ \parallel^{2} $$</code></p>
<p>This optimization problem can be solved in close form:</p>
<p><code>$$ \begin{align*} \frac{\partial}{\partial W}\parallel Y - W^{T}\phi \ \parallel^{2} &amp;= \frac{\partial}{\partial W}Tr[(Y - W^{T}\phi)^{T}(Y - W^{T}\phi)] \\ &amp;= \frac{\partial}{\partial W}Tr[Y^{T}Y+\phi^{T}WW^{T}\phi-Y^{T}W^{T}\phi - \phi^{T}WY] \\ &amp;= 0 + 2\phi\phi^{T}W - 2\phi Y^{T} = 0 \\ &amp;\Rightarrow \phi\phi^{T}W = \phi Y^{T} \\ &amp;\Rightarrow W = (\phi\phi^{T})^{-1}\phi Y^{T} \end{align*} $$</code></p>
<p>In RBF network the estimated function is:</p>
<p><code>$$ \begin{align*} \hat{Y} &amp;= W^{T}\phi \\ &amp;= ((\phi\phi^{T})^{-1}\phi Y^{T})^{T}\phi \\ &amp;= Y\phi^{T}((\phi\phi^{T})^{-1})^{T}\phi \\ \Rightarrow \hat{Y}^{T} &amp;= \underbrace{\phi^{T}(\phi\phi^{T})^{-1}\phi}_{H} Y^{T} \\ \Rightarrow \hat{Y}^{T} &amp;= HY^{T} \end{align*} $$</code></p>
<h2 id="steins-unbiased-risk-estimator-sure">Steinâ€™s unbiased risk estimator (SURE)<a hidden class="anchor" aria-hidden="true" href="#steins-unbiased-risk-estimator-sure">#</a></h2>
<p>Assume <code>$ T = \{(x_{i},y_{i})\}_{i=1}^{n} $</code> be the training set.</p>
<ul>
<li><code>$f(\cdot)$</code> <code>$\to$</code> True model</li>
<li><code>$\hat{f}(\cdot)$</code> <code>$\to$</code> Estimated model</li>
<li>err <code>$\to$</code> Empirical error: <code>$\frac{1}{n}\sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}$</code></li>
<li>Err <code>$\to$</code> True error: <code>$\frac{1}{n}\sum_{i=1}^{n}(\hat{f}_{i}-f_{i})^{2}$</code></li>
<li><code>$y$</code> <code>$\to$</code> Observations</li>
</ul>
<p>Also assume</p>
<p><code>$$ y_{i} = f(x_{i}) + \varepsilon_{i} \ , \ \ \ \ \mathrm{Where} \ \varepsilon_{i} \sim N(0,\sigma^{2}) $$</code></p>
<p>For point <code>$(x_{0},y_{0})$</code> we are interested in</p>
<p><code>$$ \begin{align*} E[(\hat{y}_{0}-y_{0})^{2}] &amp;= E[(\hat{f}_{0}-f_{0}-\varepsilon_{0})^{2}] \\ &amp;= E[((\hat{f}_{0}-f_{0})-\varepsilon_{0})^{2}] \\ &amp;= E[(\hat{f}_{0}-f_{0})^{2}+\varepsilon_{0}^{2}-2\varepsilon_{0}(\hat{f}_{0}-f_{0})] \\ &amp;= E[(\hat{f}_{0}-f_{0})^{2}]+E[\varepsilon_{0}^{2}]-2E[\varepsilon_{0}(\hat{f}_{0}-f_{0})] \\ &amp;= E[(\hat{f}_{0}-f_{0})^{2}]+\sigma^{2}-2E[\varepsilon_{0}(\hat{f}_{0}-f_{0})] \\ \end{align*} $$</code></p>
<h3 id="case-1">Case 1<a hidden class="anchor" aria-hidden="true" href="#case-1">#</a></h3>
<p>Assume <code>$(x_{0},y_{0})\notin T$</code>.</p>
<p>In this case, since <code>$\hat{f}$</code> is estimated only based on points in training set, therefore it is completely independent from <code>$(x_{0},y_{0})$</code></p>
<p><code>$$ E[\varepsilon_{0}(\hat{f}_{0}-f_{0})] = 0 \\ \Rightarrow E[(\hat{y}_{0}-y_{0})^{2}] = E[(\hat{f}_{0}-f_{0})^{2}]+\sigma^{2} $$</code></p>
<p>If summing up all <code>$m$</code> points that are not in <code>$T$</code>.</p>
<p><code>$$ \underbrace{\sum_{i=1}^{m}(\hat{y}_{i}-y_{i})^{2}}_{\mathrm{err}}= \underbrace{\sum_{i=1}^{m}(\hat{f}_{i}-f_{i})^{2}}_{\mathrm{Err}}+ m\sigma^{2} \\ \mathrm{err} = \mathrm{Err} + m\sigma^{2} \\ \mathrm{Err} = \mathrm{err} - m\sigma^{2} $$</code></p>
<p>Empirical error (<code>$\mathrm{err}$</code>) is a good estimator of true error (<code>$\mathrm{Err}$</code>) if the point <code>$(x_{0},y_{0})$</code> is not in the training set.</p>
<h3 id="case-2">Case 2<a hidden class="anchor" aria-hidden="true" href="#case-2">#</a></h3>
<p>Assume <code>$(x_{0},y_{0})\in T$</code>. Then <code>$E[\varepsilon_{0}(\hat{f}_{0}-f_{0})] \neq 0$</code></p>
<p><strong>Stein&rsquo;s Lemma</strong>: If <code>$x \sim N(\theta,\sigma^{2})$</code> and <code>$g(x)$</code> differentiable. Then,</p>
<p><code>$$ E[g(x)(x-\theta)]=\sigma^{2}E[\frac{\partial g(x)}{\partial x}] $$</code></p>
<p><code>$$ \begin{align*} E[\underbrace{\varepsilon_{0}}_{(x-\theta)}\underbrace{(\hat{f}_{0}-f_{0})}_{g(x)}] &amp;=\sigma^{2}E[\frac{\partial (\hat{f}_{0}-f_{0})}{\partial \varepsilon_{0}}] \\ &amp;= \sigma^{2}E[\frac{\partial \hat{f}_{0}}{\partial \varepsilon_{0}}-\underbrace{\frac{\partial f_{0}}{\partial \varepsilon_{0}}}_{0}] \\ &amp;= \sigma^{2}E[\frac{\partial \hat{f}_{0}}{\partial \varepsilon_{0}}] \\ &amp;= \sigma^{2}E[\frac{\partial \hat{f}_{0}}{\partial y_{0}}\cdot\underbrace{\frac{\partial y_{0}}{\partial \varepsilon_{0}}}_{1}] \\ &amp;= \sigma^{2}E[\frac{\partial \hat{f}_{0}}{\partial y_{0}}] \\ &amp;= \sigma^{2}E[D_{0}] \end{align*} $$</code></p>
<p>Thus,</p>
<p><code>$$ E[(\hat{y}_{0}-y_{0})^{2}] = E[(\hat{f}_{0}-f_{0})^{2}]+\sigma^{2} - 2\sigma^{2}E[D_{0}] $$</code></p>
<p>Sum over all <code>$n$</code> data points:</p>
<p><code>$$ \underbrace{\sum_{i=1}^{n}(\hat{y}_{i}-y_{i})^{2}}_{\mathrm{err}}= \underbrace{\sum_{i=1}^{n}(\hat{f}_{i}-f_{i})^{2}}_{\mathrm{Err}}+ n\sigma^{2} -2\sigma^{2}\sum_{i=1}^{n}D_{i} \\ \mathrm{Err} = \mathrm{err} - n\sigma^{2}+\underbrace{2\sigma^{2}\sum_{i=1}^{n}D_{i}}_{\mathrm{Complexity \ of \ model}} \Rightarrow \mathrm{Steinâ€™s \ Unbiased \ Risk \ Estimator \ (SURE)} $$</code></p>
<h2 id="coplexity-control-for-rbn">Coplexity control for RBN<a hidden class="anchor" aria-hidden="true" href="#coplexity-control-for-rbn">#</a></h2>
<p>Let&rsquo;s apply <strong>SURE</strong> to <strong>RBF</strong>:</p>
<p><code>$$ \left. \begin{array} \\ D_{i} = \frac{\partial \hat{f}_{i}}{\partial y_{i}} \\ \hat{f}_{i} = \hat{y}_{i} = H_{i:y} \end{array} \right \}D_{i}=\frac{\partial \hat{f}_{i}}{\partial y_{i}} = \frac{\partial H_{i:y}}{\partial y_{i}} = H_{ii} $$</code></p>
<p>Then <strong>SURE</strong> will be:</p>
<p><code>$$ \begin{align*} \mathrm{Err} &amp;= \mathrm{err} - n\sigma^{2}+2\sigma^{2}\sum_{i=1}^{n}H_{ii} \\ &amp;= \mathrm{err} - n\sigma^{2}+2\sigma^{2}Tr(H) \\ &amp;= \mathrm{err} - n\sigma^{2}+2\sigma^{2}Tr[\phi^{T}(\phi\phi^{T})^{-1}\phi] \\ &amp;= \mathrm{err} - n\sigma^{2}+2\sigma^{2}Tr[\underbrace{\phi\phi^{T}(\phi\phi^{T})^{-1}}_{I\to m\times m}] \\ &amp;= \mathrm{err} - n\sigma^{2}+2\sigma^{2}Tr[I_{m}] \\ &amp;= \mathrm{err} - n\sigma^{2}+2\sigma^{2}m \\ \end{align*} $$</code></p>
<p>For computing SURE, we need to know the value of <code>$\sigma$</code>. But we do not know it. Therefore we need to estimate it.</p>
<p><code>$$ \sigma^{2} = \frac{\sum_{i=1}^{n}(y_{i}-\hat{y}_{i})^{2}}{n-1} $$</code></p>
<p>is the function of complexity (more complex, smaller <code>$\sigma^{2}$</code>), in practice, we do not consider the <code>$\hat{y}$</code> to be the function of complexity and instead we consider it to be a low bias and high variance estimation (for example a line). With this assumption the <code>$\sigma$</code> will be considered to be constant and independent from the complexity of model.</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] McCormick, C. (2013, August 15). Radial Basis Function Network (RBFN) Tutorial. Radial Basis Function Network (RBFN) Tutorial Â· Chris McCormick. <a href="https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/">https://mccormickml.com/2013/08/15/radial-basis-function-network-rbfn-tutorial/</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/rbn/">RBN</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/sure/">SURE</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/deep_learning/03_regularization_for_deep_learning/">
    <span class="title">Â« Prev Page</span>
    <br>
    <span>Regularization for Deep Learning</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/">
    <span class="title">Next Page Â»</span>
    <br>
    <span>Neural Network: Perceptron and Backpropagation</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Radial Basis Function Neural Networks (RBN) on twitter"
        href="https://twitter.com/intent/tweet/?text=Neural%20Network%3a%20Radial%20Basis%20Function%20Neural%20Networks%20%28RBN%29&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f&amp;hashtags=RBN%2cSURE">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Radial Basis Function Neural Networks (RBN) on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f&amp;title=Neural%20Network%3a%20Radial%20Basis%20Function%20Neural%20Networks%20%28RBN%29&amp;summary=Neural%20Network%3a%20Radial%20Basis%20Function%20Neural%20Networks%20%28RBN%29&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Radial Basis Function Neural Networks (RBN) on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f&title=Neural%20Network%3a%20Radial%20Basis%20Function%20Neural%20Networks%20%28RBN%29">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Radial Basis Function Neural Networks (RBN) on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Radial Basis Function Neural Networks (RBN) on whatsapp"
        href="https://api.whatsapp.com/send?text=Neural%20Network%3a%20Radial%20Basis%20Function%20Neural%20Networks%20%28RBN%29%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Radial Basis Function Neural Networks (RBN) on telegram"
        href="https://telegram.me/share/url?text=Neural%20Network%3a%20Radial%20Basis%20Function%20Neural%20Networks%20%28RBN%29&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f02_radial_basis_function_neural_networks%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
