<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Neural Network: Perceptron and Backpropagation | Followb1ind1y</title>
<meta name="keywords" content="Perceptron, Feedforward Deep Networks, Backpropagation" />
<meta name="description" content="Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.">
<meta name="author" content="Followb1ind1y">
<link rel="canonical" href="https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.e70e973962a3e34880adaea2030c2ba5d772c1d55b6db8842bf38c6db6dae5fd.css" integrity="sha256-5w6XOWKj40iAra6iAwwrpddywdVbbbiEK/OMbbba5f0=" rel="preload stylesheet" as="style">
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js" integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5&#43;kdJvBz5iKbt6B5PJI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://followb1ind1y.github.io/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://followb1ind1y.github.io/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://followb1ind1y.github.io/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://followb1ind1y.github.io/apple-touch-icon.png">
<link rel="mask-icon" href="https://followb1ind1y.github.io/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<meta name="generator" content="Hugo 0.83.1" />
<meta property="og:title" content="Neural Network: Perceptron and Backpropagation" />
<meta property="og:description" content="Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/" /><meta property="og:image" content="https://followb1ind1y.github.io/papermod-cover.png"/><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2021-07-03T00:00:00&#43;00:00" />
<meta property="article:modified_time" content="2021-07-03T00:00:00&#43;00:00" />

<meta name="twitter:card" content="summary_large_image"/>
<meta name="twitter:image" content="https://followb1ind1y.github.io/papermod-cover.png"/>

<meta name="twitter:title" content="Neural Network: Perceptron and Backpropagation"/>
<meta name="twitter:description" content="Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Posts",
      "item": "https://followb1ind1y.github.io/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Neural Network: Perceptron and Backpropagation",
      "item": "https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Neural Network: Perceptron and Backpropagation",
  "name": "Neural Network: Perceptron and Backpropagation",
  "description": "Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.",
  "keywords": [
    "Perceptron", "Feedforward Deep Networks", "Backpropagation"
  ],
  "articleBody": "Neural Networks form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.\n The first thing that comes to our mind when we think of “neural networks” is biology, and indeed, neural nets are inspired by our brains. In machine learning, the neurons' dendrites refer to as input, and the nucleus process the data and forward the calculated output through the axon. In a biological neural network, the width (thickness) of dendrites defines the weight associated with it.\n Perceptron A perceptron is a neural network without any hidden layer. A perceptron only has an input layer and an output layer. Perceptrons can be represented graphically as:\n Where $x_{i}$ is the $i$-th feature of a sample and $\\beta_{i}$ is the $i$-th weight. $\\beta_{0}$ is defined as the bias. The bias alters the position of the decision boundary between the two classes. From a geometrical point of view, Perceptron assigns label “1” to elements on one side of $\\beta^{T}x+\\beta_{0}$ and label “-1” to elements on the other side. Define a cost function, $\\phi(\\beta,\\beta_{0})$, as a summation of the distance between all misclassified points and the hyperplane, or the decision boundary. To minimize the cost function, we need to estimate $\\beta$, $\\beta_{0}$.\n$$ \\min_{\\beta,\\beta_{0}}\\phi(\\beta,\\beta_{0})=\\mathrm{distance \\ of \\ all \\ misclassified \\ points} \\\\ $$\n 1. A hyperplane $L$ can be defined as:\n$$ L=\\{x:f(x)= \\beta^{T}x+\\beta_{0}=0\\} \\\\ $$\nFor any arbitrary points $x_{1}$ and $x_{2}$ on $L$, we have\n$$ \\beta^{T}x_{1}+\\beta_{0}=0 \\\\ \\beta^{T}x_{2}+\\beta_{0}=0 \\\\ \\mathrm{Such \\ that \\ } \\beta^{T}(x_{1}-x_{2})=0 \\\\ $$\n2. For any $x_{0}$ on the hyperplane,\n$$ \\beta^{T}x_{0}+\\beta_{0}=0 \\Rightarrow \\beta^{T}x_{0} = -\\beta_{0} \\\\ $$\n3. We set $\\beta^{*}=\\frac{\\beta}{\\parallel\\beta\\parallel}$ as the unit normal vector of the hyperplane $L$. For simplicity we can call $\\beta^{*}$ norm vector. The distance of point $x$ to $L$ is given by\n$$ \\beta^{*T}(x-x_{0}) = \\beta^{*T}x - \\beta^{*T}x_{0}= \\frac{\\beta^{T}x}{\\parallel\\beta\\parallel}+\\frac{\\beta_{0}}{\\parallel\\beta\\parallel} = \\frac{(\\beta^{T}x+\\beta_{0})}{\\parallel\\beta\\parallel} \\\\ $$\nWhere $x_{0}$ is any point on $L$. Hence, $\\beta^{T}x+\\beta_{0}$ is proportional to the distance of the point $x$ to the hyperplane $L$.\n4. The distance from a misclassified data point $x_{i}$ to the hyperplane $L$ is\n$$ d_{i}=-y_{i}(\\beta^{T}x_{i}+\\beta_{0}) \\\\ $$\nWhere $y_{i}$ is a target value, such that $y_{i}=1$ if $\\beta^{T}x_{i}+\\beta_{0}, $y_{i}=-1$ if $\\beta^{T}x_{i}+\\beta_{0}0$\nSince we need to find the distance from the hyperplane to the misclassified data points, we need to add a negative sign in front. When the data point is misclassified, $\\beta^{T}x_{i}+\\beta_{0}$ will produce an opposite sign of $y_{i}$. Since we need a positive sign for distance, we add a negative sign.\nPerceptron Learning using Gradient Descent The gradient descent is an optimization method that finds the minimum of an objective function by incrementally updating its parameters in the negative direction of the derivative of this function. In our case, the objective function to be minimized is classification error and the parameters of this function are the weights associated with the inputs $\\beta$. The gradient descent algorithm updates the weights as follows:\n$$ \\beta^{new} \\leftarrow \\beta^{old} - \\rho \\frac{\\partial Err}{\\partial \\beta} \\\\ $$\nWhere $\\rho$ is called the learning rate.\nThe classification error can be defined as the distance of misclassified observations to the decision boundary,\n$$ D(\\beta) = -\\sum_{i\\in M}y_{i}\\beta^{T}x_{i} \\\\ $$\nWhere $M$ is the set of misclassified points. The quantity $y_{i}\\beta^{T}x_{i}$ will be negative if $x_{i}$ is misclassified. By taking the derivative of $D(\\beta)$ with respect to $\\beta$\n$$ \\begin{align*} \\frac{\\partial D}{\\partial \\beta} \u0026= - \\sum_{i\\in M}y_{i}x_{i} \\\\ \\frac{\\partial D}{\\partial \\beta_{0}} \u0026= - \\sum_{i\\in M}y_{i} \\\\ \\end{align*} $$\nThe update formula becomes\n$$ \\beta^{new} \\leftarrow \\beta^{old} + \\rho \\sum_{i\\in M}y_{i}x_{i} \\\\ $$\nWhich is equivalent to incrementally updating $\\beta$ for each misclassified point $x_{i}$\n$$ \\beta^{new} \\leftarrow \\beta^{old} + \\rho y_{i}x_{i} \\\\ $$\nThe intuition behind this update is that for misclassified point $x_{i}$, $\\beta$ should be changed in the direction that makes $x_{i}$ as close as possible to the right side. Figure 2 shows how $\\beta$ is updated.\n Separability and Convergence The training set $D$ is said to be linearly separable if there exits a positive constant $\\gamma$ and a weight vector $\\beta$ such that $(\\beta^{T}x_{i}+\\beta_{0})y_{i}\\gamma$ for all $1. That is, if we say that $\\beta$ is the weight vector of Perceptron and $y_{i}$ is the true label of $x_{i}$, then the signd distance of the $x_{i}$ from $\\beta$ is greater than a positive constant $\\gamma$ for any $(x_{i},y_{i})\\in D$.\nIf data is linearly-separable, the solution is theoretically guranteed to converge to a separating hyperplane in a finite numver of iterations. In this situation the number of iterations depends on the learning rate and the margin. However, if the data is not linearly separable there is no guarantee that the algorithm converges.\nFeatures   A Perceptron can only discriminate between two classes at a time.\n  When data is (linearly) separable, there are an infinite number of solutions depending on the starting point.\n  Even though convergence to a solution is guaranteed if the solution exists, the finite number of steps until convergence can be very large.\n  The smaller the gap between the two classes, the longer the time of convergence.\n  When the data is not separable, the algorithm will not converge (it should be stopped after N steps).\n  A learning rate that is too high will make the perceptron periodically oscillate around the solution unless additional steps are taken.\n  The L.S. compute a linear combination of feature of input and return the sign.\n  Learning rate affects the accuracy of the solution and the number of iterations directly.\n  Feedforward Deep Networks Feedforward neural networks are artificial neural networks where the connections between units do not form a cycle. Feedforward neural networks were the first type of artificial neural network invented and are simpler than their counterpart, recurrent neural networks. They are called feedforward because information only travels forward in the network (no loops), first through the input nodes, then through the hidden nodes (if present), and finally through the output nodes.\nFeedforward neural network is a multistage regression or classification model typically represented by a graphical diagram. Regression usually produces one output unit $Y_{1}$ while for $k$ - classification there are $k$ output units $Y_{1...k}$ with each $Y_{k}$ coded as 0 − 1 to represent the $k^{th}$ class.\n where $a_{i} = u \\cdot x$ and $z_{i} = \\phi(a_{i})$ which is a non-linear function with an example being $\\phi(a) = \\frac{1}{1+e^{−a}}$. The function $\\phi$ is called the activation function and is used in classification not regression.\nFeedforward deep networks, a.k.a. multilayer perceptrons (MLPs), are parametric function composed of several parametric function. Each layer of the network defines one of these sub-functions. Each layer (sub-function) has multiple inputs and multiple outputs. Each layer composed of many units (scalar output of the layer). We sometimes refer to each unit as a feature. Each unit is usually a simple transformation of its input. Also, the entire network can be very complex.\n 深度前馈网络(deep feedforward network)，也叫作 前馈神经网络(feedforward neural network) 或者 多层感知机(multilayer perceptron, MLP) ，是典型的深度学习模型。前馈网络的目标是近似某个函数 $f^{*}$。\n Backpropagation Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.\nBackpropagation is a short form for “backward propagation of errors.” It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network.\n $$ Error = |\\hat{Y}-Y|^{2}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ a_{i} = \\sum_{l}z_{l}u_{il}, \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ z_{i} = \\sigma(a_{i}), \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ \\sigma(a) = \\frac{1}{1+e^{-a}} $$\nTake the derivative with respect to weight $u_{il}$\n$$ \\frac{\\partial Error}{\\partial u_{il}} = \\underbrace{\\frac{\\partial Error}{\\partial a_{i}}}_{\\delta_{i}\\ (Unknown)} \\cdot \\underbrace{\\frac{\\partial a_{i}}{\\partial u_{il}}}_{z_{l} \\ (Known)} \\\\ $$\n$$ \\begin{align*} \\delta_{i} = \\frac{\\partial Error}{\\partial a_{i}} \u0026= \\sum_{j}\\underbrace{\\frac{\\partial Error}{\\partial a_{j}}}_{\\delta_{j}} \\cdot \\frac{\\partial a_{j}}{\\partial a_{i}} \\to (\\frac{\\partial a_{j}}{\\partial z_{i}}\\cdot \\frac{\\partial z_{i}}{\\partial a_{i}}) \\\\ \u0026= \\sum_{j}\\delta_{j} \\cdot u_{ji} \\cdot \\sigma'(a_{i}) = \\sigma'(a_{i})\\sum_{j}\\delta_{j} \\cdot u_{ji} \\end{align*} $$\nNote that if $\\sigma(x)$ is the sigmoid function, then\n$$ \\sigma'(x) = \\sigma(x)(1-\\sigma(x)) $$\nNow considering $\\delta_{k}$ for the output layer:\n$$ \\delta_{k} = \\frac{\\partial Error}{\\partial a_{k}} = \\frac{\\partial (y-\\hat{y})^{2}}{\\partial a_{k}} = -2(y-\\hat{y}) \\ \\ \\ \\ \\ \\ \\ \\ \\ \\ where \\ a_{k} = \\hat{y} $$\nThe network weights are updated using the backpropagation algorithm when each training data point $x$ is fed into the feed forward neural network (FFNN).\n$$ u_{il}^{new} \\leftarrow u_{il}^{old} - \\rho \\cdot \\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}} $$\nBackpropagation procedure   First arbitrarily choose some random weights (preferably close to zero) for your network.\n  Apply $x$ to the FFNN’s input layer, and calculate the outputs of all input neurons.\n  Propagate the outputs of each hidden layer forward, one hidden layer at a time, and calculate the outputs of all hidden neurons.\n  Once $x$ reaches the output layer, calculate the output(s) of all output neuron(s) given the outputs of the previous hidden layer.\n  At the output layer, compute $\\delta_{k} = −2(y_{k} − \\hat{y}_{k} )$ for each output neuron(s).\n  Compute each $\\delta_{i}$, starting from $i = k − 1$ all the way to the first hidden layer, where $\\delta_{i}=\\sigma'(a_{i})\\sum_{j}\\delta_{j} \\cdot u_{ji}$\n  Compute $\\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}} = \\delta_{i}z_{l}$ for all weights $u_{il}$.\n  Then update $u_{il}^{new} \\leftarrow u_{il}^{old} - \\rho\\cdot \\frac{\\partial (y-\\hat{y})^{2}}{\\partial u_{il}}$ for all weights $u_{il}$.\n  Continue for next data points and iterate on the training set until weights converge.\n  It is common to cycle through the all of the data points multiple times in order to reach convergence. An epoch represents one cycle in which you feed all of your datapoints through the neural network. It is good practice to randomized the order you feed the points to the neural network within each epoch; this can prevent your weights changing in cycles. The number of epochs required for convergence depends greatly on the learning rate \u0026 convergence requirements used.\nReference [1] Odegua, R. (2021, April 8). Building a Neural Network From Scratch Using Python (Part 1). Medium. https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432.\n[2] Shukla, P., \u0026 Iriondo, R. (2021, April 2). Neural Networks from Scratch with Python Code and Math in Detail- I. Medium. https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#3a44.\n",
  "wordCount" : "1756",
  "inLanguage": "en",
  "datePublished": "2021-07-03T00:00:00Z",
  "dateModified": "2021-07-03T00:00:00Z",
  "author":{
    "@type": "Person",
    "name": "Followb1ind1y"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://followb1ind1y.github.io/posts/deep_learning/01_perceptron_and_backpropagation/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Followb1ind1y",
    "logo": {
      "@type": "ImageObject",
      "url": "https://followb1ind1y.github.io/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>
<noscript>
    <style type="text/css">
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: #1d1e20;
                --entry: #2e2e33;
                --primary: rgba(255, 255, 255, 0.84);
                --secondary: rgba(255, 255, 255, 0.56);
                --tertiary: rgba(255, 255, 255, 0.16);
                --content: rgba(255, 255, 255, 0.74);
                --hljs-bg: #2e2e33;
                --code-bg: #37383e;
                --border: #333;
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://followb1ind1y.github.io/" accesskey="h" title="Followb1ind1y (Alt + H)">Followb1ind1y</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://followb1ind1y.github.io/archives" title="Archive">
                    <span>Archive</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="https://followb1ind1y.github.io/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://followb1ind1y.github.io/">Home</a>&nbsp;»&nbsp;<a href="https://followb1ind1y.github.io/posts/">Posts</a></div>
    <h1 class="post-title">
      Neural Network: Perceptron and Backpropagation
    </h1>
    <div class="post-meta">July 3, 2021&nbsp;·&nbsp;9 min&nbsp;·&nbsp;Followb1ind1y
</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <div class="details">Table of Contents</div>
        </summary>
        <div class="inner"><ul>
                <li>
                    <a href="#perceptron" aria-label="Perceptron">Perceptron</a><ul>
                        
                <li>
                    <a href="#perceptron-learning-using-gradient-descent" aria-label="Perceptron Learning using Gradient Descent">Perceptron Learning using Gradient Descent</a></li>
                <li>
                    <a href="#separability-and-convergence" aria-label="Separability and Convergence">Separability and Convergence</a></li>
                <li>
                    <a href="#features" aria-label="Features">Features</a></li></ul>
                </li>
                <li>
                    <a href="#feedforward-deep-networks" aria-label="Feedforward Deep Networks">Feedforward Deep Networks</a></li>
                <li>
                    <a href="#backpropagation" aria-label="Backpropagation">Backpropagation</a><ul>
                        
                <li>
                    <a href="#backpropagation-procedure" aria-label="Backpropagation procedure">Backpropagation procedure</a></li></ul>
                </li>
                <li>
                    <a href="#reference" aria-label="Reference">Reference</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><p><strong>Neural Networks</strong> form the base of deep learning, which is a subfield of Machine Learning, where the structure of the human brain inspires the algorithms. Neural networks take input data, train themselves to recognize patterns found in the data, and then predict the output for a new set of similar data. Therefore, a neural network can be thought of as the functional unit of deep learning, which mimics the behavior of the human brain to solve complex data-driven problems.</p>
<div align="center">
  <img src="/img_DL/01_AI_ML_DL.PNG" width=400px/>
</div>
<p>The first thing that comes to our mind when we think of &ldquo;neural networks&rdquo; is biology, and indeed, neural nets are inspired by our brains. In machine learning, the neurons' <strong>dendrites</strong> refer to as input, and the nucleus process the data and forward the calculated output through the <strong>axon</strong>. In a biological neural network, the width (thickness) of dendrites defines the weight associated with it.</p>
<div align="center">
  <img src="/img_DL/01_biological_neuron.jpeg" width=700px/>
</div>
<h2 id="perceptron">Perceptron<a hidden class="anchor" aria-hidden="true" href="#perceptron">#</a></h2>
<p>A <strong>perceptron</strong> is a neural network without any hidden layer. A perceptron only has an input layer and an output layer. Perceptrons can be represented graphically as:</p>
<div align="center">
  <img src="/img_DL/01_Perceptron.PNG" width=600px/>
</div>
<p>Where <code>$x_{i}$</code> is the <code>$i$</code>-th feature of a sample and <code>$\beta_{i}$</code> is the <code>$i$</code>-th weight. <code>$\beta_{0}$</code> is defined as the bias. The bias alters the position of the decision boundary between the two classes. From a geometrical point of view, Perceptron assigns label &ldquo;1&rdquo; to elements on one side of <code>$\beta^{T}x+\beta_{0}$</code> and label &ldquo;-1&rdquo; to elements on the other side. Define a cost function, <code>$\phi(\beta,\beta_{0})$</code>, as a summation of the distance between all misclassified points and the hyperplane, or the decision boundary. To minimize the cost function, we need to estimate <code>$\beta$</code>, <code>$\beta_{0}$</code>.</p>
<p><code>$$ \min_{\beta,\beta_{0}}\phi(\beta,\beta_{0})=\mathrm{distance \ of \ all \ misclassified \ points} \\ $$</code></p>
<div align="center">
  <img src="/img_DL/01_decision_boundary.PNG" width=250px/>
</div>
<p><strong>1.</strong> A hyperplane <code>$L$</code> can be defined as:</p>
<p><code>$$ L=\{x:f(x)= \beta^{T}x+\beta_{0}=0\} \\ $$</code></p>
<p>For any arbitrary points <code>$x_{1}$</code> and <code>$x_{2}$</code> on <code>$L$</code>, we have</p>
<p><code>$$ \beta^{T}x_{1}+\beta_{0}=0 \\ \beta^{T}x_{2}+\beta_{0}=0 \\ \mathrm{Such \ that \ } \beta^{T}(x_{1}-x_{2})=0 \\ $$</code></p>
<p><strong>2.</strong> For any <code>$x_{0}$</code> on the hyperplane,</p>
<p><code>$$ \beta^{T}x_{0}+\beta_{0}=0 \Rightarrow \beta^{T}x_{0} = -\beta_{0} \\ $$</code></p>
<p><strong>3.</strong> We set <code>$\beta^{*}=\frac{\beta}{\parallel\beta\parallel}$</code> as the unit normal vector of the hyperplane <code>$L$</code>. For simplicity we can call <code>$\beta^{*}$</code> norm vector. The distance of point <code>$x$</code> to <code>$L$</code> is given by</p>
<p><code>$$ \beta^{*T}(x-x_{0}) = \beta^{*T}x - \beta^{*T}x_{0}= \frac{\beta^{T}x}{\parallel\beta\parallel}+\frac{\beta_{0}}{\parallel\beta\parallel} = \frac{(\beta^{T}x+\beta_{0})}{\parallel\beta\parallel} \\ $$</code></p>
<p>Where <code>$x_{0}$</code> is any point on <code>$L$</code>. Hence, <code>$\beta^{T}x+\beta_{0}$</code> is proportional to the distance of the point <code>$x$</code> to the hyperplane <code>$L$</code>.</p>
<p><strong>4.</strong> The distance from a misclassified data point <code>$x_{i}$</code> to the hyperplane <code>$L$</code> is</p>
<p><code>$$ d_{i}=-y_{i}(\beta^{T}x_{i}+\beta_{0}) \\ $$</code></p>
<p>Where <code>$y_{i}$</code> is a target value, such that <code>$y_{i}=1$</code> if <code>$\beta^{T}x_{i}+\beta_{0}&lt;0$</code>, <code>$y_{i}=-1$</code> if <code>$\beta^{T}x_{i}+\beta_{0}&gt;0$</code></p>
<p>Since we need to find the distance from the hyperplane to the misclassified data points, we need to add a negative sign in front. When the data point is misclassified, <code>$\beta^{T}x_{i}+\beta_{0}$</code> will produce an opposite sign of <code>$y_{i}$</code>. Since we need a positive sign for distance, we add a negative sign.</p>
<h3 id="perceptron-learning-using-gradient-descent">Perceptron Learning using Gradient Descent<a hidden class="anchor" aria-hidden="true" href="#perceptron-learning-using-gradient-descent">#</a></h3>
<p>The gradient descent is an optimization method that finds the minimum of an objective function by incrementally updating its parameters in the negative direction of the derivative of this function. In our case, the objective function to be minimized is classification error and the parameters of this function are the weights associated with the inputs <code>$\beta$</code>. The gradient descent algorithm updates the weights as follows:</p>
<p><code>$$ \beta^{new} \leftarrow \beta^{old} - \rho \frac{\partial Err}{\partial \beta} \\ $$</code></p>
<p>Where <code>$\rho$</code> is called the learning rate.</p>
<p>The classification error can be defined as the distance of misclassified observations to the decision boundary,</p>
<p><code>$$ D(\beta) = -\sum_{i\in M}y_{i}\beta^{T}x_{i} \\ $$</code></p>
<p>Where <code>$M$</code> is the set of misclassified points. The quantity <code>$y_{i}\beta^{T}x_{i}$</code> will be negative if <code>$x_{i}$</code>
is misclassified. By taking the derivative of <code>$D(\beta)$</code> with respect to <code>$\beta$</code></p>
<p><code>$$ \begin{align*} \frac{\partial D}{\partial \beta} &amp;= - \sum_{i\in M}y_{i}x_{i} \\ \frac{\partial D}{\partial \beta_{0}} &amp;= - \sum_{i\in M}y_{i} \\ \end{align*} $$</code></p>
<p>The update formula becomes</p>
<p><code>$$ \beta^{new} \leftarrow \beta^{old} + \rho \sum_{i\in M}y_{i}x_{i} \\ $$</code></p>
<p>Which is equivalent to incrementally updating <code>$\beta$</code> for each misclassified point <code>$x_{i}$</code></p>
<p><code>$$ \beta^{new} \leftarrow \beta^{old} + \rho y_{i}x_{i} \\ $$</code></p>
<p>The intuition behind this update is that for misclassified point <code>$x_{i}$</code>, <code>$\beta$</code> should be changed in the direction that makes <code>$x_{i}$</code> as close as possible to the right side. Figure 2 shows how <code>$\beta$</code> is updated.</p>
<div align="center">
  <img src="/img_DL/01_GD.PNG" width=350px/>
</div>
<h3 id="separability-and-convergence">Separability and Convergence<a hidden class="anchor" aria-hidden="true" href="#separability-and-convergence">#</a></h3>
<p>The training set <code>$D$</code> is said to be linearly separable if there exits a positive constant <code>$\gamma$</code> and a weight vector <code>$\beta$</code> such that <code>$(\beta^{T}x_{i}+\beta_{0})y_{i}&gt;\gamma$</code> for all <code>$1&lt;i&lt;n$</code>. That is, if we say that <code>$\beta$</code> is the weight vector of Perceptron and <code>$y_{i}$</code> is the true label of <code>$x_{i}$</code>, then the signd distance of the <code>$x_{i}$</code> from <code>$\beta$</code> is greater than a positive constant <code>$\gamma$</code> for any <code>$(x_{i},y_{i})\in D$</code>.</p>
<p>If data is linearly-separable, the solution is theoretically guranteed to converge to a separating hyperplane in a finite numver of iterations. In this situation the number of iterations depends on the learning rate and the margin. However, if the data is not linearly separable there is no guarantee that the algorithm converges.</p>
<h3 id="features">Features<a hidden class="anchor" aria-hidden="true" href="#features">#</a></h3>
<ul>
<li>
<p>A Perceptron can only discriminate between two classes at a time.</p>
</li>
<li>
<p>When data is (linearly) separable, there are an infinite number of solutions depending on the starting point.</p>
</li>
<li>
<p>Even though convergence to a solution is guaranteed if the solution exists, the finite number of steps until convergence can be very large.</p>
</li>
<li>
<p>The smaller the gap between the two classes, the longer the time of convergence.</p>
</li>
<li>
<p>When the data is not separable, the algorithm will not converge (it should be stopped after N steps).</p>
</li>
<li>
<p>A learning rate that is too high will make the perceptron periodically oscillate around the solution unless additional steps are taken.</p>
</li>
<li>
<p>The L.S. compute a linear combination of feature of input and return the sign.</p>
</li>
<li>
<p>Learning rate affects the accuracy of the solution and the number of iterations directly.</p>
</li>
</ul>
<h2 id="feedforward-deep-networks">Feedforward Deep Networks<a hidden class="anchor" aria-hidden="true" href="#feedforward-deep-networks">#</a></h2>
<p>Feedforward neural networks are artificial neural networks where the connections between units do not form a cycle. Feedforward neural networks were the first type of artificial neural network invented and are simpler than their counterpart, recurrent neural networks. They are called feedforward because information only travels forward in the network (no loops), first through the input nodes, then through the hidden nodes (if present), and finally through the output nodes.</p>
<p>Feedforward neural network is a multistage regression or classification model typically represented by a graphical diagram. <strong>Regression</strong> usually produces <strong>one</strong> output unit <code>$Y_{1}$</code> while for <code>$k$</code> - classification there are <code>$k$</code> output units <code>$Y_{1...k}$</code> with each <code>$Y_{k}$</code> coded as 0 − 1 to represent the <code>$k^{th}$</code> class.</p>
<div align="center">
  <img src="/img_DL/01_FNN.PNG" width=600px/>
</div>
<p>where <code>$a_{i} = u \cdot x$</code> and <code>$z_{i} = \phi(a_{i})$</code> which is a non-linear function with an example being <code>$\phi(a) = \frac{1}{1+e^{−a}}$</code>. The function <code>$\phi$</code> is called the activation function and is used in classification not regression.</p>
<p>Feedforward deep networks, a.k.a. multilayer perceptrons (MLPs), are parametric function composed of several parametric function. Each layer of the network defines one of these sub-functions. Each layer (sub-function) has multiple inputs and multiple outputs. Each layer composed of many units (scalar output of the layer). We sometimes refer to each unit as a feature. Each unit is usually a simple transformation of its input. Also, the entire network can be very complex.</p>
<blockquote>
<p><strong>深度前馈网络(deep feedforward network)</strong>，也叫作 <strong>前馈神经网络(feedforward neural network)</strong> 或者 <strong>多层感知机(multilayer perceptron, MLP)</strong> ，是典型的深度学习模型。前馈网络的目标是近似某个函数 <code>$f^{*}$</code>。</p>
</blockquote>
<h2 id="backpropagation">Backpropagation<a hidden class="anchor" aria-hidden="true" href="#backpropagation">#</a></h2>
<p>Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.</p>
<p>Backpropagation is a short form for &ldquo;backward propagation of errors.&rdquo; It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network.</p>
<div align="center">
  <img src="/img_DL/01_backprob.PNG" width=650px/>
</div>
<p><code>$$ Error = |\hat{Y}-Y|^{2}, \ \ \ \ \ \ \ \ \ \ a_{i} = \sum_{l}z_{l}u_{il}, \ \ \ \ \ \ \ \ \ \ z_{i} = \sigma(a_{i}), \ \ \ \ \ \ \ \ \ \ \sigma(a) = \frac{1}{1+e^{-a}} $$</code></p>
<p>Take the derivative with respect to weight <code>$u_{il}$</code></p>
<p><code>$$ \frac{\partial Error}{\partial u_{il}} = \underbrace{\frac{\partial Error}{\partial a_{i}}}_{\delta_{i}\ (Unknown)} \cdot \underbrace{\frac{\partial a_{i}}{\partial u_{il}}}_{z_{l} \ (Known)} \\ $$</code></p>
<p><code>$$ \begin{align*} \delta_{i} = \frac{\partial Error}{\partial a_{i}} &amp;= \sum_{j}\underbrace{\frac{\partial Error}{\partial a_{j}}}_{\delta_{j}} \cdot \frac{\partial a_{j}}{\partial a_{i}} \to (\frac{\partial a_{j}}{\partial z_{i}}\cdot \frac{\partial z_{i}}{\partial a_{i}}) \\ &amp;= \sum_{j}\delta_{j} \cdot u_{ji} \cdot \sigma'(a_{i}) = \sigma'(a_{i})\sum_{j}\delta_{j} \cdot u_{ji} \end{align*} $$</code></p>
<p>Note that if <code>$\sigma(x)$</code> is the sigmoid function, then</p>
<p><code>$$ \sigma'(x) = \sigma(x)(1-\sigma(x)) $$</code></p>
<p>Now considering <code>$\delta_{k}$</code> for the output layer:</p>
<p><code>$$ \delta_{k} = \frac{\partial Error}{\partial a_{k}} = \frac{\partial (y-\hat{y})^{2}}{\partial a_{k}} = -2(y-\hat{y})  \ \ \ \ \ \ \ \ \ \ where \ a_{k} = \hat{y} $$</code></p>
<p>The network weights are updated using the backpropagation algorithm when each training data point <code>$x$</code> is fed into the feed forward neural network (FFNN).</p>
<p><code>$$ u_{il}^{new} \leftarrow u_{il}^{old} - \rho \cdot \frac{\partial (y-\hat{y})^{2}}{\partial u_{il}} $$</code></p>
<h3 id="backpropagation-procedure">Backpropagation procedure<a hidden class="anchor" aria-hidden="true" href="#backpropagation-procedure">#</a></h3>
<ol>
<li>
<p>First arbitrarily choose some random weights (preferably close to zero) for your network.</p>
</li>
<li>
<p>Apply <code>$x$</code> to the FFNN&rsquo;s input layer, and calculate the outputs of all input neurons.</p>
</li>
<li>
<p>Propagate the outputs of each hidden layer forward, one hidden layer at a time, and calculate the outputs of all hidden neurons.</p>
</li>
<li>
<p>Once <code>$x$</code> reaches the output layer, calculate the output(s) of all output neuron(s) given the outputs of the previous hidden layer.</p>
</li>
<li>
<p>At the output layer, compute <code>$\delta_{k} = −2(y_{k} − \hat{y}_{k} )$</code> for each output neuron(s).</p>
</li>
<li>
<p>Compute each <code>$\delta_{i}$</code>, starting from <code>$i = k − 1$</code> all the way to the first hidden layer, where <code>$\delta_{i}=\sigma'(a_{i})\sum_{j}\delta_{j} \cdot u_{ji}$</code></p>
</li>
<li>
<p>Compute <code>$\frac{\partial (y-\hat{y})^{2}}{\partial u_{il}} = \delta_{i}z_{l}$</code> for all weights <code>$u_{il}$</code>.</p>
</li>
<li>
<p>Then update <code>$u_{il}^{new} \leftarrow u_{il}^{old} - \rho\cdot \frac{\partial (y-\hat{y})^{2}}{\partial u_{il}}$</code> for all weights <code>$u_{il}$</code>.</p>
</li>
<li>
<p>Continue for next data points and iterate on the training set until weights converge.</p>
</li>
</ol>
<p>It is common to cycle through the all of the data points multiple times in order to reach convergence. An epoch represents one cycle in which you feed all of your datapoints through the neural network. It is good practice to randomized the order you feed the points to the neural network within each epoch; this can prevent your weights changing in cycles. The number of epochs required for convergence depends greatly on the learning rate &amp; convergence requirements used.</p>
<h2 id="reference">Reference<a hidden class="anchor" aria-hidden="true" href="#reference">#</a></h2>
<p>[1] Odegua, R. (2021, April 8). Building a Neural Network From Scratch Using Python (Part 1). Medium. <a href="https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432">https://heartbeat.fritz.ai/building-a-neural-network-from-scratch-using-python-part-1-6d399df8d432</a>.</p>
<p>[2]  Shukla, P., &amp; Iriondo, R. (2021, April 2). Neural Networks from Scratch with Python Code and Math in Detail- I. Medium. <a href="https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#3a44">https://pub.towardsai.net/building-neural-networks-from-scratch-with-python-code-and-math-in-detail-i-536fae5d7bbf#3a44</a>.</p>

</div>
  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://followb1ind1y.github.io/tags/perceptron/">Perceptron</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/feedforward-deep-networks/">Feedforward Deep Networks</a></li>
      <li><a href="https://followb1ind1y.github.io/tags/backpropagation/">Backpropagation</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://followb1ind1y.github.io/posts/deep_learning/02_radial_basis_function_neural_networks/">
    <span class="title">« Prev Page</span>
    <br>
    <span>Neural Network: Radial Basis Function Neural Networks (RBN)</span>
  </a>
  <a class="next" href="https://followb1ind1y.github.io/posts/applied_math_and_ml_basics/04_machine_learning_basics_for_ml/">
    <span class="title">Next Page »</span>
    <br>
    <span>[ML Basics] Machine Learning Basics</span>
  </a>
</nav>

<div class="share-buttons">
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Perceptron and Backpropagation on twitter"
        href="https://twitter.com/intent/tweet/?text=Neural%20Network%3a%20Perceptron%20and%20Backpropagation&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f&amp;hashtags=Perceptron%2cFeedforwardDeepNetworks%2cBackpropagation">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-253.927,424.544c135.939,0 210.268,-112.643 210.268,-210.268c0,-3.218 0,-6.437 -0.153,-9.502c14.406,-10.421 26.973,-23.448 36.935,-38.314c-13.18,5.824 -27.433,9.809 -42.452,11.648c15.326,-9.196 26.973,-23.602 32.49,-40.92c-14.252,8.429 -30.038,14.56 -46.896,17.931c-13.487,-14.406 -32.644,-23.295 -53.946,-23.295c-40.767,0 -73.87,33.104 -73.87,73.87c0,5.824 0.613,11.494 1.992,16.858c-61.456,-3.065 -115.862,-32.49 -152.337,-77.241c-6.284,10.881 -9.962,23.601 -9.962,37.088c0,25.594 13.027,48.276 32.95,61.456c-12.107,-0.307 -23.448,-3.678 -33.41,-9.196l0,0.92c0,35.862 25.441,65.594 59.311,72.49c-6.13,1.686 -12.72,2.606 -19.464,2.606c-4.751,0 -9.348,-0.46 -13.946,-1.38c9.349,29.426 36.628,50.728 68.965,51.341c-25.287,19.771 -57.164,31.571 -91.8,31.571c-5.977,0 -11.801,-0.306 -17.625,-1.073c32.337,21.15 71.264,33.41 112.95,33.41Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Perceptron and Backpropagation on linkedin"
        href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f&amp;title=Neural%20Network%3a%20Perceptron%20and%20Backpropagation&amp;summary=Neural%20Network%3a%20Perceptron%20and%20Backpropagation&amp;source=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-288.985,423.278l0,-225.717l-75.04,0l0,225.717l75.04,0Zm270.539,0l0,-129.439c0,-69.333 -37.018,-101.586 -86.381,-101.586c-39.804,0 -57.634,21.891 -67.617,37.266l0,-31.958l-75.021,0c0.995,21.181 0,225.717 0,225.717l75.02,0l0,-126.056c0,-6.748 0.486,-13.492 2.474,-18.315c5.414,-13.475 17.767,-27.434 38.494,-27.434c27.135,0 38.007,20.707 38.007,51.037l0,120.768l75.024,0Zm-307.552,-334.556c-25.674,0 -42.448,16.879 -42.448,39.002c0,21.658 16.264,39.002 41.455,39.002l0.484,0c26.165,0 42.452,-17.344 42.452,-39.002c-0.485,-22.092 -16.241,-38.954 -41.943,-39.002Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Perceptron and Backpropagation on reddit"
        href="https://reddit.com/submit?url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f&title=Neural%20Network%3a%20Perceptron%20and%20Backpropagation">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-3.446,265.638c0,-22.964 -18.616,-41.58 -41.58,-41.58c-11.211,0 -21.361,4.457 -28.841,11.666c-28.424,-20.508 -67.586,-33.757 -111.204,-35.278l18.941,-89.121l61.884,13.157c0.756,15.734 13.642,28.29 29.56,28.29c16.407,0 29.706,-13.299 29.706,-29.701c0,-16.403 -13.299,-29.702 -29.706,-29.702c-11.666,0 -21.657,6.792 -26.515,16.578l-69.105,-14.69c-1.922,-0.418 -3.939,-0.042 -5.585,1.036c-1.658,1.073 -2.811,2.761 -3.224,4.686l-21.152,99.438c-44.258,1.228 -84.046,14.494 -112.837,35.232c-7.468,-7.164 -17.589,-11.591 -28.757,-11.591c-22.965,0 -41.585,18.616 -41.585,41.58c0,16.896 10.095,31.41 24.568,37.918c-0.639,4.135 -0.99,8.328 -0.99,12.576c0,63.977 74.469,115.836 166.33,115.836c91.861,0 166.334,-51.859 166.334,-115.836c0,-4.218 -0.347,-8.387 -0.977,-12.493c14.564,-6.47 24.735,-21.034 24.735,-38.001Zm-119.474,108.193c-20.27,20.241 -59.115,21.816 -70.534,21.816c-11.428,0 -50.277,-1.575 -70.522,-21.82c-3.007,-3.008 -3.007,-7.882 0,-10.889c3.003,-2.999 7.882,-3.003 10.885,0c12.777,12.781 40.11,17.317 59.637,17.317c19.522,0 46.86,-4.536 59.657,-17.321c3.016,-2.999 7.886,-2.995 10.885,0.008c3.008,3.011 3.003,7.882 -0.008,10.889Zm-5.23,-48.781c-16.373,0 -29.701,-13.324 -29.701,-29.698c0,-16.381 13.328,-29.714 29.701,-29.714c16.378,0 29.706,13.333 29.706,29.714c0,16.374 -13.328,29.698 -29.706,29.698Zm-160.386,-29.702c0,-16.381 13.328,-29.71 29.714,-29.71c16.369,0 29.689,13.329 29.689,29.71c0,16.373 -13.32,29.693 -29.689,29.693c-16.386,0 -29.714,-13.32 -29.714,-29.693Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Perceptron and Backpropagation on facebook"
        href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-106.468,0l0,-192.915l66.6,0l12.672,-82.621l-79.272,0l0,-53.617c0,-22.603 11.073,-44.636 46.58,-44.636l36.042,0l0,-70.34c0,0 -32.71,-5.582 -63.982,-5.582c-65.288,0 -107.96,39.569 -107.96,111.204l0,62.971l-72.573,0l0,82.621l72.573,0l0,192.915l-191.104,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Perceptron and Backpropagation on whatsapp"
        href="https://api.whatsapp.com/send?text=Neural%20Network%3a%20Perceptron%20and%20Backpropagation%20-%20https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f">
        <svg version="1.1" viewBox="0 0 512 512" xml:space="preserve">
            <path
                d="M449.446,0c34.525,0 62.554,28.03 62.554,62.554l0,386.892c0,34.524 -28.03,62.554 -62.554,62.554l-386.892,0c-34.524,0 -62.554,-28.03 -62.554,-62.554l0,-386.892c0,-34.524 28.029,-62.554 62.554,-62.554l386.892,0Zm-58.673,127.703c-33.842,-33.881 -78.847,-52.548 -126.798,-52.568c-98.799,0 -179.21,80.405 -179.249,179.234c-0.013,31.593 8.241,62.428 23.927,89.612l-25.429,92.884l95.021,-24.925c26.181,14.28 55.659,21.807 85.658,21.816l0.074,0c98.789,0 179.206,-80.413 179.247,-179.243c0.018,-47.895 -18.61,-92.93 -52.451,-126.81Zm-126.797,275.782l-0.06,0c-26.734,-0.01 -52.954,-7.193 -75.828,-20.767l-5.441,-3.229l-56.386,14.792l15.05,-54.977l-3.542,-5.637c-14.913,-23.72 -22.791,-51.136 -22.779,-79.287c0.033,-82.142 66.867,-148.971 149.046,-148.971c39.793,0.014 77.199,15.531 105.329,43.692c28.128,28.16 43.609,65.592 43.594,105.4c-0.034,82.149 -66.866,148.983 -148.983,148.984Zm81.721,-111.581c-4.479,-2.242 -26.499,-13.075 -30.604,-14.571c-4.105,-1.495 -7.091,-2.241 -10.077,2.241c-2.986,4.483 -11.569,14.572 -14.182,17.562c-2.612,2.988 -5.225,3.364 -9.703,1.12c-4.479,-2.241 -18.91,-6.97 -36.017,-22.23c-13.314,-11.876 -22.304,-26.542 -24.916,-31.026c-2.612,-4.484 -0.279,-6.908 1.963,-9.14c2.016,-2.007 4.48,-5.232 6.719,-7.847c2.24,-2.615 2.986,-4.484 4.479,-7.472c1.493,-2.99 0.747,-5.604 -0.374,-7.846c-1.119,-2.241 -10.077,-24.288 -13.809,-33.256c-3.635,-8.733 -7.327,-7.55 -10.077,-7.688c-2.609,-0.13 -5.598,-0.158 -8.583,-0.158c-2.986,0 -7.839,1.121 -11.944,5.604c-4.105,4.484 -15.675,15.32 -15.675,37.364c0,22.046 16.048,43.342 18.287,46.332c2.24,2.99 31.582,48.227 76.511,67.627c10.685,4.615 19.028,7.371 25.533,9.434c10.728,3.41 20.492,2.929 28.209,1.775c8.605,-1.285 26.499,-10.833 30.231,-21.295c3.732,-10.464 3.732,-19.431 2.612,-21.298c-1.119,-1.869 -4.105,-2.99 -8.583,-5.232Z" />
        </svg>
    </a>
    <a target="_blank" rel="noopener noreferrer" aria-label="share Neural Network: Perceptron and Backpropagation on telegram"
        href="https://telegram.me/share/url?text=Neural%20Network%3a%20Perceptron%20and%20Backpropagation&amp;url=https%3a%2f%2ffollowb1ind1y.github.io%2fposts%2fdeep_learning%2f01_perceptron_and_backpropagation%2f">
        <svg version="1.1" xml:space="preserve" viewBox="2 2 28 28">
            <path
                d="M26.49,29.86H5.5a3.37,3.37,0,0,1-2.47-1,3.35,3.35,0,0,1-1-2.47V5.48A3.36,3.36,0,0,1,3,3,3.37,3.37,0,0,1,5.5,2h21A3.38,3.38,0,0,1,29,3a3.36,3.36,0,0,1,1,2.46V26.37a3.35,3.35,0,0,1-1,2.47A3.38,3.38,0,0,1,26.49,29.86Zm-5.38-6.71a.79.79,0,0,0,.85-.66L24.73,9.24a.55.55,0,0,0-.18-.46.62.62,0,0,0-.41-.17q-.08,0-16.53,6.11a.59.59,0,0,0-.41.59.57.57,0,0,0,.43.52l4,1.24,1.61,4.83a.62.62,0,0,0,.63.43.56.56,0,0,0,.4-.17L16.54,20l4.09,3A.9.9,0,0,0,21.11,23.15ZM13.8,20.71l-1.21-4q8.72-5.55,8.78-5.55c.15,0,.23,0,.23.16a.18.18,0,0,1,0,.06s-2.51,2.3-7.52,6.8Z" />
        </svg>
    </a>
</div>

  </footer>
</article>
    </main>
    <footer class="footer">
    <span>&copy; 2021 <a href="https://followb1ind1y.github.io/">Followb1ind1y</a></span>
    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://git.io/hugopapermod" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)">
    <button class="top-link" id="top-link" type="button" accesskey="g">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
            <path d="M12 6H0l6-6z" />
        </svg>
    </button>
</a>

<script>
    let menu = document.getElementById('menu')
    menu.scrollLeft = localStorage.getItem("menu-scroll-position");
    menu.onscroll = function () {
        localStorage.setItem("menu-scroll-position", menu.scrollLeft);
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = 'copy';

        function copyingDone() {
            copybutton.innerText = 'copied!';
            setTimeout(() => {
                copybutton.innerText = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>

<script src="//yihui.org/js/math-code.js"></script>


<script type="text/javascript"
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</body>

</html>
